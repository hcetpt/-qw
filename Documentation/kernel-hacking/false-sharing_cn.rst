SPDX 许可证标识符: GPL-2.0

=============
错误共享
=============

什么是错误共享
=====================
错误共享与缓存机制有关，该机制用于维护存储在多个 CPU 缓存中的同一缓存行的数据一致性；其学术定义见 [1]_。考虑一个包含引用计数（refcount）和字符串的结构体：

```c
struct foo {
    refcount_t refcount;
    ...
    char name[16];
} ____cacheline_internodealigned_in_smp;
```

成员 'refcount' (A) 和 'name' (B) 共享同一个缓存行如下所示：

```
                +-----------+                     +-----------+
                |   CPU 0   |                     |   CPU 1   |
                +-----------+                     +-----------+
               /                                        |
              /                                         |
             V                                          V
         +----------------------+             +----------------------+
         | A      B             | Cache 0     | A       B            | Cache 1
         +----------------------+             +----------------------+
                             |                  |
  ---------------------------+------------------+-----------------------------
                             |                  |
                           +----------------------+
                           |                      |
                           +----------------------+
              主内存  | A       B            |
                           +----------------------+
```

'refcount' 经常被修改，而 'name' 在对象创建时设置一次之后就不再修改。当许多 CPU 同时访问 'foo' 时，假设只有某个 CPU 频繁增加 'refcount' 的值，而其他 CPU 只读取 'name' 的值，由于这种“共享”，所有读取 'name' 的 CPU 都必须不断重新加载整个缓存行，即使 'name' 从未改变。

错误共享导致性能退化的情况有很多实例。其中一个是 `mm_struct` 结构体内部的读写信号量 `mmap_lock`，其缓存行布局的变化触发了性能退化，并且 Linus 在 [2]_ 中进行了分析。

有害的错误共享有两个关键因素：

* 被多个 CPU 访问（共享）的全局数据。
* 在并发访问数据时，至少有一个写操作：写/写或写/读的情况。
这些共享可能来自完全不相关的内核组件，或者来自同一内核组件的不同代码路径。

错误共享的陷阱
======================
在过去，当一个平台只有一个或少数几个 CPU 时，可以有意将热点数据成员放在同一个缓存行中，以使其成为热点缓存并节省缓存行/TLB，例如锁及其保护的数据。但对于最近拥有数百个 CPU 的大型系统来说，当锁竞争激烈时，这种方法可能不起作用，因为持有锁的 CPU 可能会写入数据，而其他 CPU 则忙于自旋锁。

回顾过去的案例，有几个频繁出现的错误共享模式：

* 锁（自旋锁/互斥锁/信号量）及其保护的数据有意放在同一个缓存行中。
* 多个全局数据放在一起形成一个缓存行。某些内核子系统有许多小尺寸（4 字节）的全局参数，这些参数很容易被组合在一起放入一个缓存行。
* 大型数据结构中的数据成员随机地放在一起而不被注意到（通常缓存行大小为 64 字节或更多），例如 'mem_cgroup' 结构体。
接下来的“缓解”部分提供了实际案例。
虚假共享很容易发生，除非有意地进行检查，并且对于性能关键的工作负载运行特定工具来检测影响性能的虚假共享情况并相应优化是非常有价值的。

如何检测和分析虚假共享
=====================
`perf record`、`perf report` 和 `perf stat` 广泛用于性能调优，一旦检测到热点区域，可以进一步使用诸如 `perf-c2c` 和 `pahole` 等工具来检测和定位可能的虚假共享数据结构。`addr2line` 在有多层内联函数时也非常擅长解码指令指针。
`perf-c2c` 可以捕获虚假共享命中次数最多的缓存行，访问该缓存行的函数（文件行号），以及数据的内联偏移量。简单的命令如下：

  ```
  $ perf c2c record -ag sleep 3
  $ perf c2c report --call-graph none -k vmlinux
  ```

在测试 `will-it-scale` 的 `tlb_flush1` 情况时，`perf` 报告如下：

  ```
  Total records                     :    1658231
  Locked Load/Store Operations      :      89439
  Load Operations                   :     623219
  Load Local HITM                   :      92117
  Load Remote HITM                  :        139
  
  #----------------------------------------------------------------------
      4        0     2374        0        0        0  0xff1100088366d880
  #----------------------------------------------------------------------
    0.00%   42.29%    0.00%    0.00%    0.00%    0x8     1       1  0xffffffff81373b7b         0       231       129     5312        64  [k] __mod_lruvec_page_state    [kernel.vmlinux]  memcontrol.h:752   1
    0.00%   13.10%    0.00%    0.00%    0.00%    0x8     1       1  0xffffffff81374718         0       226        97     3551        64  [k] folio_lruvec_lock_irqsave  [kernel.vmlinux]  memcontrol.h:752   1
    0.00%   11.20%    0.00%    0.00%    0.00%    0x8     1       1  0xffffffff812c29bf         0       170       136      555        64  [k] lru_add_fn                 [kernel.vmlinux]  mm_inline.h:41     1
    0.00%    7.62%    0.00%    0.00%    0.00%    0x8     1       1  0xffffffff812c3ec5         0       175       108      632        64  [k] release_pages              [kernel.vmlinux]  mm_inline.h:41     1
    0.00%   23.29%    0.00%    0.00%    0.00%   0x10     1       1  0xffffffff81372d0a         0       234       279     1051        64  [k] __mod_memcg_lruvec_state   [kernel.vmlinux]  memcontrol.c:736   1
  ```

关于 `perf-c2c` 的一个很好的介绍是[3]_。
`pahole` 解码以缓存行粒度分隔的数据结构布局。用户可以将 `perf-c2c` 输出中的偏移量与 `pahole` 的解码匹配，以定位确切的数据成员。对于全局数据，用户可以在 `System.map` 中搜索数据地址。

可能的缓解措施
===============
虚假共享并不总是需要缓解。虚假共享的缓解措施应平衡性能提升与复杂性和空间消耗。有时，较低的性能是可以接受的，没有必要对每个很少使用的数据结构或冷数据路径进行超优化。随着核心数量的增加，虚假共享影响性能的情况变得更加频繁。由于这些负面影响，许多补丁已经在各种子系统（如网络和内存管理）中被提出并合并。一些常见的缓解措施（及示例）包括：

* 将热点全局数据单独放在其专用的缓存行中，即使它只是一个 `short` 类型。缺点是会消耗更多的内存、缓存行和 TLB 条目。
- 提交 91b6d3256356（"net: cache align tcp_memory_allocated, tcp_sockets_allocated"）

* 重组数据结构，将干扰成员分离到不同的缓存行中。一个缺点是可能会引入其他成员的新虚假共享。
- 提交 802f1d522d5f（"mm: page_counter: re-layout structure to reduce false sharing"）

* 在可能的情况下用读代替写，特别是在循环中。例如，对于某个全局变量，使用比较（读）-然后写而不是无条件写。例如，使用：
  
  ```if (!test_bit(XXX)) set_bit(XXX);```
  
  而不是直接 `"set_bit(XXX);"`，类似地，对于 `atomic_t` 数据：
  
  ```if (atomic_read(XXX) == AAA) atomic_set(XXX, BBB);```
  
  - 提交 7b1002f7cfe5（"bcache: fixup bcache_dev_sectors_dirty_add() multithreaded CPU false sharing"）
  - 提交 292648ac5cf1（"mm: gup: allow FOLL_PIN to scale in SMP"）

* 尽可能将热点全局数据转换为“每核数据+全局数据”，或者合理增加同步每核数据到全局数据的阈值，以减少或推迟对该全局数据的写入。
- 提交 520f897a3554（"ext4: use percpu_counters for extent_status cache hits/misses"）
  - 提交 56f3547bfa4d（"mm: adjust vm_committed_as_batch according to vm overcommit policy"）

当然，所有缓解措施都应仔细验证以确保不会引发副作用。为了避免在编码时引入虚假共享，最好：

* 注意缓存行边界
* 将主要只读字段分组在一起
* 将同时写入的事项分组在一起
* 将经常读取和经常写入的字段分隔在不同的缓存行上
并且最好添加一条注释说明虚假共享的考虑。
需要注意的是，有时候即使检测到严重的虚假共享并解决了该问题，性能可能仍然没有明显提升，因为热点会转移到新的位置。

杂项
======
一个未解决的问题是内核有一个可选的数据结构随机化机制，这也会随机化数据成员之间的缓存行共享情况。

.. [1] https://en.wikipedia.org/wiki/False_sharing
.. [2] https://lore.kernel.org/lkml/CAHk-=whoqV=cX5VC80mmR9rr+Z+yQ6fiQZm36Fb-izsanHg23w@mail.gmail.com/
.. [3] https://joemario.github.io/blog/2016/09/01/c2c-blog/
