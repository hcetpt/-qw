始于1999年11月，由Kanoj Sarcar <kanoj@sgi.com>发起

=============
什么是NUMA？
=============

这个问题可以从两个角度来看：硬件视角和Linux软件视角。
从硬件角度来看，NUMA系统是一种计算机平台，它由多个组件或装配体组成，每个组件可能包含0个或多个CPU、本地内存和/或I/O总线。为了简洁并区分这些物理组件/装配体的硬件视图与软件抽象，我们将这些组件/装配体称为“节点”。
每个“节点”都可以视为系统的SMP（对称多处理器）子集——尽管某些用于独立SMP系统的必要组件可能在给定的节点上没有配置。NUMA系统的各个节点通过某种系统互连连接在一起——例如，交叉开关或点对点链接是常见的NUMA系统互连类型。这两种类型的互连都可以聚合起来创建具有不同距离节点的NUMA平台。

对于Linux来说，主要感兴趣的NUMA平台是所谓的缓存一致NUMA（ccNUMA）系统。在ccNUMA系统中，所有内存对任何连接到任何节点的CPU都是可见且可访问的，并且缓存一致性由处理器缓存和/或系统互连在硬件层面处理。内存访问时间和有效带宽取决于执行内存访问的CPU或I/O总线所在的节点与目标内存所在节点之间的距离。例如，连接在同一节点上的CPU访问内存将比访问其他远程节点上的内存具有更快的访问时间和更高的带宽。NUMA平台可以有多个距离不同的节点。

平台供应商并不是为了让软件开发人员的生活变得有趣而构建NUMA系统。相反，这种架构是为了提供可扩展的内存带宽。然而，要实现可扩展的内存带宽，系统和应用程序软件必须安排大部分内存引用（缓存未命中）指向“本地”内存——如果有，即同一节点上的内存，或者最接近的具有内存的节点。

这引出了Linux对NUMA系统的软件视图：

Linux将系统的硬件资源划分为多个称为“节点”的软件抽象。Linux将节点映射到硬件平台的物理节点上，对某些架构抽象掉一些细节。与物理节点一样，软件节点可能包含0个或多个CPU、内存和/或I/O总线。同样，对“更近”节点（映射到更近的物理节点）的内存访问通常会比对更远节点的访问具有更快的访问时间和更高的有效带宽。

对于某些架构，如x86，Linux会“隐藏”任何代表没有连接内存的物理节点，并将连接到该节点的任何CPU重新分配到代表有内存的节点。因此，在这些架构上，不能假设Linux关联给定节点的所有CPU都会看到相同的本地内存访问时间和带宽。

此外，对于某些架构，例如x86，Linux支持额外节点的仿真。对于NUMA仿真，Linux会将现有的节点——或非NUMA平台的系统内存——划分为多个节点。每个仿真的节点将管理底层节点的一部分物理内存。NUMA仿真有助于在非NUMA平台上测试NUMA内核和应用程序特性，并作为与cpuset结合使用时的一种内存资源管理机制（参见Documentation/admin-guide/cgroup-v1/cpusets.rst）。

对于每个带有内存的节点，Linux构建了一个独立的内存管理系统，包括自己的空闲页列表、使用中的页列表、使用统计以及用于协调访问的锁。此外，Linux为每个内存区域（一个或多个DMA、DMA32、NORMAL、HIGH_MEMORY、MOVABLE）构建一个有序的“zonelist”。当所选区域/节点无法满足分配请求时，zonelist指定了应访问的区域/节点。这种情况称为“溢出”或“回退”。
因为某些节点包含多个包含不同类型内存的区域，Linux 必须决定是否按照使分配回退到同一类型但不同节点的区域，还是回退到同一节点上不同类型的区域。这是一个重要的考虑因素，因为有些区域（如DMA或DMA32）代表相对稀缺的资源。Linux选择默认的按节点排序的区域列表（Node ordered zonelist）。这意味着它会尝试先从同一节点上的其他区域进行回退，然后再使用根据NUMA距离排序的远程节点。

默认情况下，Linux将尝试从执行请求的CPU所分配的节点来满足内存分配请求。具体来说，Linux将尝试从请求发起节点的适当区域列表中的第一个节点进行分配。这称为“本地分配”。

如果“本地”节点无法满足请求，内核将检查选定区域列表中其他节点的区域，寻找列表中第一个可以满足请求的区域。

本地分配倾向于使随后对已分配内存的访问保持在底层物理资源上，并远离系统互连——只要为该任务分配的内存后来没有迁移到其他地方。Linux调度器了解平台的NUMA拓扑结构——体现在“调度域”数据结构中[见Documentation/scheduler/sched-domains.rst]——并且调度器试图尽量减少任务向远端调度域的迁移。然而，调度器不会直接考虑任务的NUMA占用情况。

因此，在足够的不平衡情况下，任务可以在节点之间迁移，远离它们初始所在的节点和内核数据结构。

系统管理员和应用程序设计者可以使用各种CPU亲和力命令行接口（如taskset(1)和numactl(1)）以及程序接口（如sched_setaffinity(2)）来限制任务的迁移，以改善NUMA局部性。此外，还可以使用Linux NUMA内存策略来修改内核的默认本地分配行为[见Documentation/admin-guide/mm/numa_memory_policy.rst]。

系统管理员可以使用控制组和CPU集来限制非特权用户在调度或NUMA命令和函数中指定的CPU和节点内存。[见Documentation/admin-guide/cgroup-v1/cpusets.rst]

对于不隐藏无内存节点的架构，Linux只会将包含内存的区域（节点）包括在区域列表中。这意味着对于一个无内存的节点，“本地内存节点”——即CPU节点区域列表中的第一个节点——将不是该节点本身，而是内核在构建区域列表时选择的最近的具有内存的节点。

因此，默认的本地分配将成功地提供最近可用的内存。这是允许此类分配在内存溢出时回退到其他附近节点的相同机制的结果。

一些内核分配不希望或不能容忍这种分配回退行为。相反，它们希望确保从指定节点获取内存，或者被告知该节点没有空闲内存。通常在子系统为每个CPU分配内存资源时就是这种情况。

为此类分配制定的一个典型模型是使用内核的numa_node_id()或CPU_to_node()函数之一获取当前CPU所连接的节点ID，然后仅从返回的节点ID请求内存。当此类分配失败时，请求子系统可能会回退到自己的回退路径。slab内核内存分配器就是一个例子。或者，子系统可能会选择在分配失败时不启用或禁用自身。内核分析子系统就是一个例子。

如果架构支持——不隐藏——无内存节点，则连接到无内存节点的CPU将始终产生回退路径开销，或者某些子系统在尝试仅从无内存节点分配内存时会初始化失败。为了透明地支持此类架构，内核子系统可以使用numa_mem_id()或cpu_to_mem()函数来查找调用或指定CPU的“本地内存节点”。同样，这也是默认本地页面分配将尝试从中进行尝试的节点。
当然，请提供你需要翻译的文本。
