### 缓冲区共享与同步 (dma-buf)

dma-buf 子系统提供了一个框架，用于在多个设备驱动程序和子系统之间共享用于硬件（直接内存访问，DMA）访问的缓冲区，并同步异步硬件访问。例如，它被DRM子系统广泛使用，以在进程间、同一进程内的上下文及库API间以及与其他子系统（如 V4L2）间交换缓冲区。

本文件描述了内核子系统如何使用和与dma-buf提供的三个主要原语进行交互：

- **dma-buf**：表示一个sg_table，并作为一个文件描述符暴露给用户空间，以便在进程、子系统、设备等之间传递；
- **dma-fence**：提供一种机制来指示异步硬件操作何时完成；
- **dma-resv**：为特定的dma-buf管理一组dma-fence，允许隐式（内核排序）的工作同步，以保持一致性的访问假象。

### 用户空间API原则与使用

有关如何设计您的子系统的dma-buf使用的API的更多细节，请参见 `Documentation/userspace-api/dma-buf-alloc-exchange.rst`。

### 共享DMA缓冲区

本文档旨在指导设备驱动程序编写者了解dma-buf缓冲区共享API是什么，以及如何使用它来导出和使用共享缓冲区。

任何希望成为DMA缓冲区共享一部分的设备驱动程序都可以作为“导出者”或“使用者”（或称为“导入者”）缓冲区。
假设驱动程序A想要使用由驱动程序B创建的缓冲区，则称B为导出者，而A为缓冲区使用者/导入者。

#### 导出者

- 实现并管理 :c:type:`struct dma_buf_ops <dma_buf_ops>` 中的缓冲区操作，
- 通过使用dma_buf共享API允许其他用户共享该缓冲区，
- 管理缓冲区分配的详细信息，封装在一个 :c:type:`struct dma_buf <dma_buf>` 中，
- 决定实际的后端存储位置，
- 并负责所有（共享）用户的散列列表迁移。

#### 缓冲区使用者

- 是该缓冲区的其中一个（可能很多个）共享用户，
- 不需要关心缓冲区是如何分配的或分配在何处，
- 需要一种机制来获取构成此缓冲区的散列列表的访问权限，该列表映射到其自己的地址空间中，以便可以访问相同的内存区域。这个接口由 :c:type:`struct dma_buf_attachment <dma_buf_attachment>` 提供。
任何dma-buf缓冲共享框架的导出者或使用者必须在它们各自的Kconfig中包含一个 '选择 DMA_SHARED_BUFFER'
用户空间接口说明
~~~~~~~~~~~~~~~~~~~~~~~~~

通常，DMA缓冲文件描述符对用户空间来说是一个不透明的对象，因此公开的一般接口非常简单。但有几个要点需要考虑：

- 自从内核版本3.12开始，dma-buf文件描述符支持llseek系统调用，但是只支持offset=0和whence=SEEK_END|SEEK_SET的情况。支持SEEK_SET是为了允许常规的大小发现模式：size = SEEK_END(0); SEEK_SET(0)。所有其他llseek操作都会报告-EINVAL。
如果dma-buf文件描述符上的llseek不被支持，内核会对所有情况报告-ESPIPE。用户空间可以使用这种方式来检测是否支持通过llseek来发现dma-buf的大小。
- 为了避免exec时的文件描述符泄露问题，文件描述符必须设置FD_CLOEXEC标志。这不仅仅是一个资源泄露问题，也是一个潜在的安全漏洞。它可能会使新执行的应用程序通过泄露的文件描述符访问那些本不应该被它访问的缓冲区。
通过单独的fcntl()调用来设置此标志与在创建文件描述符时原子性地设置相比存在固有竞态条件问题，特别是在多线程应用中[3]。当是库代码打开/创建文件描述符时问题更加严重，因为应用程序可能根本不知道这些文件描述符的存在。
为了规避这一问题，用户空间必须有一种方式请求在创建dma-buf文件描述符时设置O_CLOEXEC标志。所以任何由导出驱动程序提供的用于创建dmabuf文件描述符的API都必须提供一种方法，让用户空间能够控制设置传递给dma_buf_fd()的O_CLOEXEC标志。
- 支持内存映射DMA缓冲的内容。有关详细信息，请参阅下面的“CPU访问DMA缓冲对象”部分。
- DMA缓冲文件描述符也是可轮询的，详情请参阅下面的“隐式围栏轮询支持”部分。
- DMA缓冲文件描述符还支持一些dma-buf特定的ioctl，详情请参阅下面的“DMA缓冲ioctl”部分。

基本操作与设备DMA访问
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. 内核文档:: drivers/dma-buf/dma-buf.c
   :doc: dma buf 设备访问

CPU访问DMA缓冲对象
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. 内核文档:: drivers/dma-buf/dma-buf.c
   :doc: CPU访问

隐式围栏轮询支持
~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. 内核文档:: drivers/dma-buf/dma-buf.c
   :doc: 隐式围栏轮询

DMA-BUF统计信息
~~~~~~~~~~~~~~~~~~
.. 内核文档:: drivers/dma-buf/dma-buf-sysfs-stats.c
   :doc: 概览

DMA缓冲ioctl
~~~~~~~~~~~~~~~~~

.. 内核文档:: include/uapi/linux/dma-buf.h

DMA-BUF锁定约定
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. 内核文档:: drivers/dma-buf/dma-buf.c
   :doc: 锁定约定

内核函数和结构体引用
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. 内核文档:: drivers/dma-buf/dma-buf.c
   :export:

.. 内核文档:: include/linux/dma-buf.h
   :internal:

保留对象
-------------------

.. 内核文档:: drivers/dma-buf/dma-resv.c
   :doc: 保留对象概览

.. 内核文档:: drivers/dma-buf/dma-resv.c
   :export:

.. 内核文档:: include/linux/dma-resv.h
   :internal:

DMA围栏
----------

.. 内核文档:: drivers/dma-buf/dma-fence.c
   :doc: DMA围栏概览

DMA围栏跨驱动程序合同
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. 内核文档:: drivers/dma-buf/dma-fence.c
   :doc: 围栏跨驱动程序合同

DMA围栏信号注释
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. 内核文档:: drivers/dma-buf/dma-fence.c
   :doc: 围栏信号注释

DMA围栏截止时间提示
~~~~~~~~~~~~~~~~~~~~~~~~

.. 内核文档:: drivers/dma-buf/dma-fence.c
   :doc: 截止时间提示

DMA围栏函数参考
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. 内核文档:: drivers/dma-buf/dma-fence.c
   :export:

.. 内核文档:: include/linux/dma-fence.h
   :internal:

DMA围栏数组
~~~~~~~~~~~~~~~

.. 内核文档:: drivers/dma-buf/dma-fence-array.c
   :export:

.. 内核文档:: include/linux/dma-fence-array.h
   :internal:

DMA围栏链
~~~~~~~~~~~~~~~

.. 内核文档:: drivers/dma-buf/dma-fence-chain.c
   :export:

.. 内核文档:: include/linux/dma-fence-chain.h
   :internal:

DMA围栏解包
~~~~~~~~~~~~~~~~

.. 内核文档:: include/linux/dma-fence-unwrap.h
   :internal:

DMA围栏同步文件
~~~~~~~~~~~~~~~

.. 内核文档:: drivers/dma-buf/sync_file.c
   :export:

.. 内核文档:: include/linux/sync_file.h
   :internal:

DMA围栏同步文件uABI
~~~~~~~~~~~~~~~~~~~~~~~~

.. 内核文档:: include/uapi/linux/sync_file.h
   :internal:

无限期DMA围栏
~~~~~~~~~~~~~~~~~~~~~

在不同的时候，曾提议使用具有不确定时间直到dma_fence_wait()完成的struct dma_fence。示例包括：

* 未来围栏，用于HWC1中指示当缓冲区不再被显示使用时的信号，并且是在使该缓冲区可见的屏幕更新中创建的。
这段英文描述的技术细节可以翻译为：

完成这个围栏的时间完全由用户空间控制。

* 代理围栏：被提议用于处理`&drm_syncobj`，在这种情况下围栏尚未被设置。用于异步延迟命令提交。
* 用户空间围栏或GPU futexes：在命令缓冲区内的细粒度锁定，用户空间使用其进行跨引擎或与CPU的同步，然后将其作为DMA围栏导入，以融入现有的winsys协议。
* 长时间运行的计算命令缓冲区：虽然仍然使用传统的批处理结束DMA围栏进行内存管理，而不是上下文抢占DMA围栏，后者在重新调度计算任务时会被重新连接。

所有这些方案的共同点是用户空间控制这些围栏的依赖关系，并且控制它们何时触发。将无限期围栏与常规内核DMA围栏混合使用是不可行的，即使包括了回退超时来防止恶意用户空间行为也是如此：

* 只有内核了解所有DMA围栏依赖关系，用户空间并不知道由于内存管理和调度决策而注入的依赖关系。
* 只有用户空间了解无限期围栏的所有依赖关系以及它们确切完成的时间，内核无法看到这些信息。
此外，内核必须能够为了内存管理需求而阻止用户空间命令的提交，这意味着我们必须支持无限期围栏依赖于DMA围栏。如果内核也像DMA围栏一样支持无限期围栏，就像上述任何提案那样，可能会导致死锁的情况发生。

.. kernel-render:: DOT
   :alt: 无限期围栏依赖循环图
   :caption: 无限期围栏依赖循环图

   digraph "Fencing Cycle" {
      node [shape=box bgcolor=grey style=filled]
      kernel [label="内核DMA围栏"]
      userspace [label="用户空间控制的围栏"]
      kernel -> userspace [label="内存管理"]
      userspace -> kernel [label="未来围栏、围栏代理、..."]

      { rank=same; kernel userspace }
   }

这意味着内核可能意外地通过用户空间不知情的内存管理依赖关系造成死锁，从而随机挂起工作负载直到超时机制启动。从用户空间的角度来看，这些工作负载中并不存在死锁。在这样混合的围栏架构中，没有单一实体能够了解所有的依赖关系。因此，在内核内部避免此类死锁是不可能的。

避免依赖循环的唯一解决方案是不允许内核中的无限期围栏。这意味着：

* 不允许未来围栏、代理围栏或用户空间围栏作为DMA围栏导入，无论是否包含超时机制。
* 不允许存在任何直接内存访问 (DMA) 栅栏来标记命令提交的批处理缓冲区结束，其中用户空间被允许使用用户空间栅栏或长时间运行的计算任务。这也意味着在这种情况下，共享缓冲区不能有隐式的栅栏。

可恢复硬件页错误的影响
~~~~~~~~~~~~~~~~~~~~~~~~~~

现代硬件支持可恢复的页错误，这对 DMA 栅栏有很多影响。
首先，一个待处理的页错误显然会阻塞在加速器上运行的工作，并且通常需要内存分配来解决这个错误。
但是，不允许内存分配阻塞 DMA 栅栏的完成，这意味着任何使用可恢复页错误的工作负载都不能利用 DMA 栅栏进行同步。相反，必须使用由用户空间控制的同步栅栏。

在图形处理器单元 (GPU) 上，这带来了一个问题，因为当前 Linux 上的桌面合成协议依赖于 DMA 栅栏，这意味着如果没有完全基于用户空间栅栏的新用户空间堆栈，它们就无法从可恢复的页错误中受益。具体来说，这意味着不可能实现隐式同步。
例外情况是当页错误仅用作迁移提示而从未用于按需填充内存请求时。目前这意味着 GPU 上的可恢复页错误仅限于纯计算工作负载。

此外，GPU 通常在三维渲染和计算方面共享资源，如计算单元或命令提交引擎。如果一个带有 DMA 栅栏的三维任务和一个使用可恢复页错误的计算任务都在等待处理，可能会导致死锁：

- 三维工作负载可能需要等待计算任务完成并释放硬件资源。
- 计算工作负载可能因陷入页错误而停滞不前，因为内存分配正等待三维工作负载的 DMA 栅栏完成。

有几个选项可以防止这个问题，其中一个方案是驱动程序需要确保：

- 即使在页错误发生但尚未修复的情况下，计算任务也总是可以被抢占。并非所有硬件都支持这一点。
- 使用 DMA 栅栏的工作负载与需要页错误处理的工作负载具有独立的硬件资源以保证向前进展。这可以通过例如专用引擎和对使用 DMA 栅栏的工作负载最小计算单元预留的方式来实现。
- 预留方法可以通过仅在DMA围栏工作负载处于执行状态时预留硬件资源来进一步优化。这必须覆盖从DMA围栏对其他线程可见到通过dma_fence_signal()完成的时间段。
- 作为最后的手段，如果硬件不提供有用的预留机制，则在切换需要DMA围栏或需要处理页错误的工作时，必须将所有工作负载从GPU中刷新。这意味着，在带有页错误处理的计算任务被插入调度队列之前，所有DMA围栏必须完成。反之亦然，在DMA围栏可以在系统中的任何地方变得可见之前，所有的计算工作负载必须被抢占以确保所有待处理的GPU页错误都被刷新。
- 只有一个相当理论化的选项是在分配内存以修复硬件页错误时解开这些依赖关系，这可以通过独立的内存块或运行时跟踪所有DMA围栏的完整依赖图来实现。这会对内核产生非常广泛的影响，因为仅在CPU端解决页错误本身就可能涉及页错误。相比之下，限制处理硬件页错误的影响范围至特定驱动程序要更加可行和稳健。
  
  应注意的是，在独立硬件（如复制引擎或其他GPU）上运行的工作负载没有任何影响。这使我们能够在内核内部继续使用DMA围栏，即使是在解决硬件页错误的情况下，例如通过使用复制引擎清除或复制所需内存来解决页错误。

  在某种程度上，这个页错误问题是“无限DMA围栏”讨论的一个特殊情况：允许来自计算工作负载的无限围栏依赖于DMA围栏，但不允许反向依赖。甚至页错误问题本身也不是新的，因为在用户空间中的其他CPU线程可能会遇到页错误，从而导致用户空间的围栏被阻塞——在GPU上支持页错误并没有带来根本性的新问题。
