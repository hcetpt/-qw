工作队列  
=========

:日期: 2010年9月
:作者: Tejun Heo <tj@kernel.org>
:作者: Florian Mickler <florian@mickler.org>

简介  
============

在许多情况下，需要一个异步进程执行上下文，而工作队列（wq）API是最常用的机制之一。当需要这种异步执行上下文时，描述要执行的函数的工作项会被放入队列中。独立线程充当异步执行上下文。该队列称为工作队列，该线程称为工作线程。只要工作队列上有工作项，工作线程就会依次执行与这些工作项关联的函数。当工作队列上没有剩余的工作项时，工作线程将处于空闲状态。当新的工作项被加入队列时，工作线程又开始执行。

为什么需要并发管理的工作队列？
==================================

在原始的wq实现中，一个多线程（MT）wq为每个CPU有一个工作线程，而单线程（ST）wq则在整个系统中有一个工作线程。单一的MT wq需要保留与CPU数量相同数量的工作线程。多年来内核发展了很多MT wq用户，并且随着CPU核心数的持续增加，一些系统仅在启动时就耗尽了默认的32k PID空间。
尽管MT wq浪费了大量的资源，但提供的并发级别并不令人满意。这一限制对ST和MT wq都是普遍存在的，尽管在MT上不那么严重。每个wq都维护着自己的独立工作线程池。MT wq每CPU只能提供一个执行上下文，而ST wq则为整个系统提供一个执行上下文。工作项必须争夺这些极其有限的执行上下文，导致包括容易产生围绕单一执行上下文的死锁在内的各种问题。
提供的并发级别与资源使用之间的紧张关系也迫使用户做出不必要的权衡，例如libata选择使用ST wq进行PIO轮询，并接受两个PIO不能同时进行的不必要的限制。由于MT wq并不能提供更好的并发性，因此像async或fscache这样需要更高并发级别的用户不得不自己实现线程池。
并发管理的工作队列（cmwq）是wq的一种重新实现，其重点在于以下目标：
* 保持与原始工作队列API的兼容性
* 使用每个CPU统一的工作线程池，由所有wq共享，以按需提供灵活的并发级别，而不浪费大量资源
* 自动调节工作线程池和并发级别，以便API用户无需担心这些细节。

设计
=====

为了简化函数的异步执行，引入了一个新的抽象概念：工作项。
一个工作项是一个简单的结构体，它包含一个指向要异步执行的函数的指针。每当驱动程序或子系统想要异步执行某个函数时，就需要设置一个指向该函数的工作项，并将该工作项排队到工作队列上。
工作项可以在线程或BH（软中断）上下文中执行。
对于线程化的工作队列，专门的线程（称为[k]worker）从队列中依次执行函数。如果没有任务排队，则worker线程会处于空闲状态。这些worker线程由worker池进行管理。
cmwq设计区分了面向用户的、子系统和驱动程序排队工作项的工作队列以及后台机制，后者管理worker池并处理排队的工作项。
有两个worker池，一个用于普通工作项，另一个用于高优先级工作项，每个可能的CPU都有一个这样的worker池，还有一些额外的worker池来服务于未绑定工作队列上的工作项——这些后端池的数量是动态的。
BH工作队列使用相同的框架。但是，由于只能有一个并发执行上下文，因此无需担心并发问题。
每个CPU的BH worker池只包含一个伪worker，代表BH执行上下文。可以认为BH工作队列是对软中断的一个便利接口。
子系统和驱动程序可以通过特殊的工作队列API函数根据需要创建和排队工作项。通过在他们放置工作项的工作队列上设置标志，他们可以影响工作项执行的一些方面。这些标志包括诸如CPU局部性、并发限制、优先级等。详细信息请参阅下面的`alloc_workqueue()` API描述。
当一个工作项被加入到一个工作队列时，根据队列参数和工作队列属性来确定目标工作线程池，并将该工作项附加到该线程池的共享工作列表上。例如，除非特别覆盖，一个绑定工作队列的工作项将会被加入到与发出者所在CPU相关联的普通或高优先级工作线程池的工作列表中。
对于任何线程池实现来说，管理并发级别（即有多少执行上下文处于活动状态）是一个重要的问题。cmwq尝试保持最低但足够的并发级别：最低以节省资源，足够则确保系统得到充分利用。
每个绑定到实际CPU的工作线程池通过与调度程序挂钩来实现并发管理。每当有活动的工作线程唤醒或睡眠时，都会通知工作线程池，并跟踪当前可运行的工作线程的数量。通常，工作项不会占用CPU并消耗大量周期。这意味着维持适量的并发以防止工作处理停滞应该是最优的。只要在CPU上有至少一个或多个可运行的工作线程，工作线程池就不会启动新的工作项执行，但是当最后一个正在运行的工作线程进入睡眠状态时，它会立即安排一个新的工作线程，以防止CPU在有待处理的工作项时空闲。这允许使用最少数量的工作线程而不损失执行带宽。
保留闲置的工作线程除了占用kthreads的内存空间外没有其他成本，因此cmwq会在杀死它们之前保留一段时间的闲置工作线程。
对于未绑定的工作队列，支持的线程池数量是动态的。未绑定的工作队列可以使用`apply_workqueue_attrs()`函数分配自定义属性，而工作队列会自动创建符合这些属性的后台工作线程池。调节并发级别的责任在于用户。还有一面标志可以标记绑定的wq忽略并发管理。请参阅API部分获取详细信息。
向前进度保证依赖于在需要更多执行上下文时能够创建工作线程，而这通过使用救援工作线程来保证。所有可能在处理内存回收代码路径上使用的工作项都必须被加入到预留了救援工作线程用于内存压力下执行的工作队列(wq)中。否则可能会导致工作线程池因等待执行上下文释放而死锁。
应用编程接口（API）

`alloc_workqueue()`函数分配了一个wq。原来的`create_*workqueue()`函数已被废弃并计划移除。`alloc_workqueue()`函数接受三个参数：`@name`、`@flags`和`@max_active`。`@name`是wq的名字，同时也是救援线程的名字（如果有）。wq不再直接管理执行资源，而是作为一个提供向前进度保证、刷新和工作项属性的领域。`@flags`和`@max_active`控制着工作项如何被分配执行资源、调度和执行。
### 标志

`WQ_BH`
  - BH 工作队列可以视为对软中断（softirq）的一个便捷接口。BH 工作队列总是每个CPU一个，并且所有BH工作项都在队列所在CPU的软中断上下文中按照队列顺序执行。
  - 所有BH工作队列必须设置 `max_active` 为0，并且只允许使用 `WQ_HIGHPRI` 这一额外标志。
  - BH工作项不允许睡眠。除此之外的所有特性，如延迟队列、刷新和取消等都是支持的。

`WQ_UNBOUND`
  - 排队到未绑定工作队列（unbound wq）的工作项将由特殊的工作者池处理，这些工作者池中包含不绑定到任何特定CPU的工作者。这使得该工作队列表现为一个简单的执行上下文提供者，而不涉及并发管理。
  - 未绑定的工作者池试图尽快开始执行工作项。未绑定的工作队列牺牲了局部性，但在以下情况下很有用：
    - 预期并发级别需求会有大幅度波动，并且使用绑定的工作队列可能会导致在不同CPU上创建大量大部分时间未使用的工作者，因为发起者会跳转到不同的CPU上。
    - 长时间运行的CPU密集型工作负载，这些工作负载可以通过系统调度程序更好地管理。

`WQ_FREEZABLE`
  - 可冻结的工作队列参与系统的挂起操作中的冻结阶段。工作队列上的工作项会被排空，并且直到解冻前不会开始执行新的工作项。

`WQ_MEM_RECLAIM`
  - 所有可能在内存回收路径中使用的所有工作队列 **必须** 设置此标志。无论内存压力如何，该工作队列都保证至少有一个执行上下文。

`WQ_HIGHPRI`
  - 高优先级工作队列的工作项被排队到目标CPU的高优先级工作者池。高优先级工作者池由具有较高nice值的工作者线程提供服务。
  
  注意：普通工作者池和高优先级工作者池之间不相互作用。每个都有自己的工作者池，并在其工作者之间实现并发管理。
```WQ_CPU_INTENSIVE```
CPU 密集型的工作队列中的工作项不会计入并发级别。换句话说，可运行的 CPU 密集型工作项并不会阻止同一工作线程池中的其他工作项开始执行。这对于那些预期会大量占用 CPU 周期的限定工作项很有用，因为它们的执行将由系统调度程序来调节。
尽管 CPU 密集型工作项不计入并发级别，但其执行的启动仍然受到并发管理的调节，并且可运行的非 CPU 密集型工作项可能会延迟 CPU 密集型工作项的执行。
对于未限定的工作队列来说，此标志没有意义。
``max_active``
----------------

``@max_active`` 确定了每个 CPU 上可以分配给一个工作队列中工作项的最大执行上下文数量。例如，如果 ``@max_active`` 设置为 16，则每个 CPU 上最多可以同时执行该工作队列中的 16 个工作项。这始终是一个每 CPU 的属性，即使对于未限定的工作队列也是如此。
``@max_active`` 的最大限制是 512，而当指定 0 时使用的默认值是 256。这些值被设置得足够高，以确保它们不是限制因素，同时在失控情况下提供保护。
一个工作队列中的活动工作项数量通常由该工作队列的用户进行调节，更具体地说，由用户在同一时间可能排队的工作项数量决定。除非有明确的需求来限制活动工作项的数量，否则建议指定 '0'。
一些用户依赖于严格的执行顺序，即任何时刻只有一个工作项正在处理，工作项按照排队顺序进行处理。虽然以前可以通过将 ``@max_active`` 设置为 1 并使用 ``WQ_UNBOUND`` 来实现这种行为，但现在不再如此。相反，请使用 ``alloc_ordered_queue()``。
示例执行场景
==================

以下示例执行场景试图说明在不同配置下 cmwq 的行为。
工作项 w0、w1 和 w2 在同一 CPU 上被排队到限定工作队列 q0 中。
- w0 运行 5 毫秒，然后休眠 10 毫秒，再运行 5 毫秒后完成。
- w1 和 w2 各自运行 5 毫秒，然后休眠 10 毫秒。
忽略所有其他任务、工作和处理开销，并假设采用简单的FIFO调度，以下是使用原始wq时可能发生的一系列事件的一种高度简化的版本：

 时间（毫秒）	事件
 0		w0开始运行并占用CPU
 5		w0进入睡眠状态
 15		w0醒来并占用CPU
 20		w0完成
 20		w1开始运行并占用CPU
 25		w1进入睡眠状态
 35		w1醒来并完成
 35		w2开始运行并占用CPU
 40		w2进入睡眠状态
 50		w2醒来并完成

而对于具有`@max_active` >= 3的cmwq，则是这样的：

 时间（毫秒）	事件
 0		w0开始运行并占用CPU
 5		w0进入睡眠状态
 5		w1开始运行并占用CPU
 10		w1进入睡眠状态
 10		w2开始运行并占用CPU
 15		w2进入睡眠状态
 15		w0醒来并占用CPU
 20		w0完成
 20		w1醒来并完成
 25		w2醒来并完成

如果`@max_active` == 2，则如下：

 时间（毫秒）	事件
 0		w0开始运行并占用CPU
 5		w0进入睡眠状态
 5		w1开始运行并占用CPU
 10		w1进入睡眠状态
 15		w0醒来并占用CPU
 20		w0完成
 20		w1醒来并完成
 20		w2开始运行并占用CPU
 25		w2进入睡眠状态
 35		w2醒来并完成

现在，假设w1和w2被排队到另一个设置有`WQ_CPU_INTENSIVE`标志的工作队列q1中，情况如下：

 时间（毫秒）	事件
 0		w0开始运行并占用CPU
 5		w0进入睡眠状态
 5		w1和w2开始运行并占用CPU
 10		w1进入睡眠状态
 15		w2进入睡眠状态
 15		w0醒来并占用CPU
 20		w0完成
 20		w1醒来并完成
 25		w2醒来并完成

指导原则
=========

* 如果一个工作队列可能处理在内存回收过程中使用的作业项，请不要忘记使用`WQ_MEM_RECLAIM`。每个设置了`WQ_MEM_RECLAIM`标志的工作队列都有为其保留的执行上下文。如果有多个作业项在内存回收期间相互依赖，那么它们应该分别排队到各自带有`WQ_MEM_RECLAIM`标志的工作队列中。
* 如果不需要严格的顺序，则无需使用单线程工作队列。
* 除非有特殊需求，推荐将`@max_active`设为0。在大多数情况下，并发级别通常远低于默认限制。
* 工作队列作为提供向前进展保证的领域（例如`WQ_MEM_RECLAIM`、刷新以及作业项属性）。不参与内存回收、不需要作为一组作业项的一部分进行刷新、且不需要任何特殊属性的作业项可以使用系统工作队列之一。使用专用工作队列与系统工作队列之间在执行特性上没有区别。
* 除非预期作业项会消耗大量的CPU周期，否则通常使用有界工作队列是有益的，因为这会提高在工作队列操作和作业项执行中的局部性。

亲和力范围
===========

未绑定的工作队列根据其亲和力范围对CPU进行分组以改进缓存局部性。例如，如果工作队列使用默认的“cache”亲和力范围，则它会按照最后一级缓存边界来分组CPU。在该工作队列上排队的一个作业项会被分配给与发起CPU共享同一最后一级缓存的某CPU上的一个工作者。一旦启动，根据亲和力范围的`affinity_strict`设置，工作者可能会或不会被允许移动出该范围。
目前工作队列支持以下亲和力范围：
``default``
  使用模块参数`workqueue.default_affinity_scope`中定义的范围，该参数始终设置为下面列出的范围之一。
``cpu``
  不对CPU进行分组。在一个CPU上发出的作业项将由该CPU上的工作者处理。这使得未绑定的工作队列的行为类似于每CPU的工作队列，但不进行并发管理。
``smt``
处理器根据SMT边界进行分组。这通常意味着每个物理CPU核心的逻辑线程会被归为一组。

``cache``
处理器根据缓存边界进行分组。具体使用哪个缓存边界由架构代码决定。在很多情况下会使用L3缓存。这是默认的亲和力范围。

``numa``
处理器根据NUMA边界进行分组。

``system``
所有处理器都被放在同一组中。工作队列不会努力将工作项安排在接近发出工作的处理器上执行。

默认的亲和力范围可以通过模块参数`workqueue.default_affinity_scope`进行更改，而特定工作队列的亲和力范围则可以通过`apply_workqueue_attrs()`函数来更改。

如果设置了`WQ_SYSFS`，则工作队列在其`/sys/devices/virtual/workqueue/WQ_NAME/`目录下会有以下与亲和力范围相关的接口文件：

``affinity_scope``
读取该文件以查看当前的亲和力范围。写入该文件以更改亲和力范围。
当默认范围是当前范围时，读取此文件还会显示当前有效的亲和力范围（括号内表示），例如，“default (cache)”（默认(cache)）。

``affinity_strict``
默认值为0，表示亲和力范围不是严格的。当一个工作项开始执行时，工作队列会尽力确保工作者位于其亲和力范围内，这一过程被称为归还(repatriation)。一旦开始执行，调度器可以自由地将工作者移动到系统中的任何位置，只要它认为合适即可。这使得可以在必要和可用的情况下从范围局部性获益，同时也能利用其他处理器。

如果设置为1，则保证该范围内的所有工作者始终处于该范围内。当跨越亲和力范围有其他含义时，这可能是有用的，例如，在功耗或工作负载隔离方面。严格的NUMA范围也可以用于匹配较旧内核的工作队列行为。
亲和性范围与性能
===============================

如果未绑定的工作队列在无需进一步调整的情况下，对大多数使用场景都能达到最优表现，那将是最理想的。不幸的是，在当前的内核中，局部性和利用率之间存在显著的权衡关系，当工作队列被大量使用时，需要进行明确的配置。更高的局部性会导致更高的效率，即对于消耗的相同数量的CPU周期可以完成更多的工作。然而，如果工作项没有被发布者足够地分散到各个亲和性范围内，更高的局部性也可能导致整体系统利用率降低。下面通过dm-crypt进行的性能测试清楚地展示了这种权衡。

测试运行在一个拥有12个核心/24个线程且分布在四个L3缓存上的CPU（AMD Ryzen 9 3900x）上。为了保持一致性，关闭了CPU的时钟加速功能。
``/dev/dm-0`` 是一个创建于NVME SSD（Samsung 990 PRO）上的dm-crypt设备，并使用 ``cryptsetup`` 以默认设置打开。
场景1：足够的发布者且工作均匀分布于整个机器
-------------------------------------------------------------

使用的命令如下： ::

  $ fio --filename=/dev/dm-0 --direct=1 --rw=randrw --bs=32k --ioengine=libaio \
    --iodepth=64 --runtime=60 --numjobs=24 --time_based --group_reporting \
    --name=iops-test-job --verify=sha512

有24个发布者，每个发布者并发发出64个I/O操作。 ``--verify=sha512`` 使得 ``fio`` 每次生成并读回内容，这使得执行局部性在发布者与 ``kcryptd`` 之间变得重要。以下是根据 ``kcryptd`` 上不同的亲和性范围设置测量的五次运行中的读带宽和CPU利用率。带宽单位为 MiBps，CPU利用率为百分比。
.. list-table::
   :widths: 16 20 20
   :header-rows: 1

   * - 亲和性
     - 带宽 (MiBps)
     - CPU 利用率 (%)

   * - 系统
     - 1159.40 ±1.34
     - 99.31 ±0.02

   * - 缓存
     - 1166.40 ±0.89
     - 99.34 ±0.01

   * - 缓存 (严格)
     - 1166.00 ±0.71
     - 99.35 ±0.01

当有足够的发布者分布在整个系统中时，“缓存”或“严格”缓存亲和性都没有任何缺点。所有三种配置都能使整个机器饱和，但基于缓存亲和性的配置由于改进的局部性而提高了0.6%的性能。
场景2：较少的发布者，但仍有足够工作使系统饱和
-----------------------------------------------------

使用的命令如下： ::

  $ fio --filename=/dev/dm-0 --direct=1 --rw=randrw --bs=32k \
    --ioengine=libaio --iodepth=64 --runtime=60 --numjobs=8 \
    --time_based --group_reporting --name=iops-test-job --verify=sha512

与前一场景唯一的区别在于 ``--numjobs=8``。发布者的数量减少了三分之一，但仍有足够的总工作量来使系统饱和。
.. list-table::
   :widths: 16 20 20
   :header-rows: 1

   * - 亲和性
     - 带宽 (MiBps)
     - CPU 利用率 (%)

   * - 系统
     - 1155.40 ±0.89
     - 97.41 ±0.05

   * - 缓存
     - 1154.40 ±1.14
     - 96.15 ±0.09

   * - 缓存 (严格)
     - 1112.00 ±4.64
     - 93.26 ±0.35

这是足够多的工作量来使系统饱和。无论是“系统”还是“缓存”亲和性几乎都使机器饱和，但并未完全饱和。“缓存”亲和性使用了更少的CPU资源，但更好的效率使其带宽与“系统”亲和性相当。
八个发布者在四个L3缓存范围内移动仍然允许“缓存(严格)”亲和性在很大程度上使机器饱和，但工作保护的损失现在开始造成3.7%的带宽损失。
场景3：更少的发布者，不足以使系统饱和的工作量
-----------------------------------------------------------

使用的命令如下： ::

  $ fio --filename=/dev/dm-0 --direct=1 --rw=randrw --bs=32k \
    --ioengine=libaio --iodepth=64 --runtime=60 --numjobs=4 \
    --time_based --group_reporting --name=iops-test-job --verify=sha512

再次，唯一的不同是 ``--numjobs=4``。发布者数量减少到四个后，现在没有足够多的工作量来使整个系统饱和，带宽开始依赖于完成延迟。
列表表格如下：
   
   | 属性 | 带宽（MiB/s）| CPU 利用率（%）|
   | ---- | ---- | ---- |
   | Affinity | 993.60 ±1.82 | 75.49 ±0.06 |
   | cache | 973.40 ±1.52 | 74.90 ±0.07 |
   | cache (strict) | 828.20 ±4.49 | 66.84 ±0.29 |

现在，局部性和利用率之间的权衡更加清晰。“cache”相比“system”显示出2%的带宽损失，而“cache (strict)”则有惊人的20%损失。

### 结论与建议

在上述实验中，“cache”亲和域相对于“system”的效率优势虽然一致且明显，但很小。然而，其影响取决于不同亲和域间的距离，在更复杂的处理器拓扑结构中可能会更为显著。
虽然某些情况下工作保守性（work-conservation）的丢失是不利的，但这比“cache (strict)”要好得多，并且最大化工作队列利用率的情况不太常见。因此，“cache”作为未绑定池的默认亲和域。

* 由于没有一个选项适用于大多数情况，对于可能消耗大量CPU的工作队列使用，建议使用`apply_workqueue_attrs()`进行配置和/或启用`WQ_SYSFS`。
* 严格采用“cpu”亲和域的未绑定工作队列的行为与每个CPU的`WQ_CPU_INTENSIVE`工作队列相同。后者实际上并没有真正优势，而未绑定工作队列提供了更多的灵活性。
* 亲和域是在Linux v6.5中引入的。为了模拟之前的行为，可以使用严格的“numa”亲和域。
* 非严格亲和域中的工作保守性丢失可能源于调度器。理论上，内核应该能够在大多数情况下做出正确的选择并保持工作保守性。因此，未来调度器改进可能会使这些可调参数变得不必要。

### 考察配置

使用`tools/workqueue/wq_dump.py`来检查未绑定CPU亲和性配置、工作者池以及工作队列如何映射到这些池： 

```
$ tools/workqueue/wq_dump.py
...
```

### 监控

使用`tools/workqueue/wq_monitor.py`来监控工作队列操作：

```
$ tools/workqueue/wq_monitor.py events
...
```
查看命令的帮助信息以获取更多信息。
调试
==========

由于工作函数是由通用的工作线程执行的，因此需要一些技巧来找出行为异常的工作队列用户。
工作线程在进程列表中显示为：:: 

  root      5671  0.0  0.0      0     0 ?        S    12:07   0:00 [kworker/0:1]  
  root      5672  0.0  0.0      0     0 ?        S    12:07   0:00 [kworker/1:2]  
  root      5673  0.0  0.0      0     0 ?        S    12:12   0:00 [kworker/0:0]  
  root      5674  0.0  0.0      0     0 ?        S    12:13   0:00 [kworker/1:0]

如果kworker出现异常（使用过多的CPU），可能存在两种类型的问题：

1. 某些任务频繁地被调度
2. 单个工作项消耗大量的CPU周期

第一种情况可以通过追踪来跟踪：:: 

	$ echo workqueue:workqueue_queue_work > /sys/kernel/tracing/set_event  
	$ cat /sys/kernel/tracing/trace_pipe > out.txt  
	(等待几秒钟)  
	^C  

如果有任务在工作队列上忙碌循环，它将主导输出结果，并且可以通过工作项函数确定问题所在
对于第二种类型的问题，只需检查问题工作线程的堆栈跟踪即可。:: 

	$ cat /proc/THE_OFFENDING_KWORKER/stack  

工作项的函数应该很容易在堆栈跟踪中看到
非重入条件
==================

如果满足以下条件，在一个工作项被排队后，工作队列可以保证该工作项不会发生重入：

1. 工作函数没有被改变
2. 没有人将工作项排到另一个工作队列中
3. 工作项没有被重新启动
换句话说，如果满足上述条件，则可以保证任何给定时间全局范围内最多只有一个工作系统在执行该工作项
请注意，在自身函数中将工作项重新排队（排到相同的队列）不会破坏这些条件，因此这样做是安全的。否则，在工作函数内部打破这些条件时需要谨慎
内核内联文档参考
=====================

.. kernel-doc:: include/linux/workqueue.h

.. kernel-doc:: kernel/workqueue.c
