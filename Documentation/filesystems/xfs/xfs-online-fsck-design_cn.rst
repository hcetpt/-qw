.. SPDX 许可证标识符: GPL-2.0
.. _xfs_online_fsck_design:

.
本文件中标题样式的映射：
        标题 1 使用 "====" 在上方和下方
        标题 2 使用 "===="
        标题 3 使用 "----"
        标题 4 使用 "````"
        标题 5 使用 "^^^^"
        标题 6 使用 "~~~~"
        标题 7 使用 "...."

        各部分手动编号，因为内核代码中似乎都是这样做的
======================
XFS 在线文件系统检查设计
======================

本文档记录了 XFS 文件系统在线检查功能的设计。
本文档的目的有三个：

- 帮助内核分发商确切地了解 XFS 在线文件系统检查功能是什么，以及他们应该注意的问题
- 帮助阅读代码的人在开始深入研究代码之前熟悉相关概念和设计要点
- 通过记录支持高层决策的理由来帮助维护系统的开发者
随着在线文件系统检查代码的合并，本文档中的主题分支链接将被替换为代码链接
本文档遵循 GNU 公共许可证 v2 的条款
主要作者是 Darrick J. Wong
此设计文档分为七个部分
### 目录
1. 什么是文件系统检查？

#### 1. 什么是文件系统检查？

一个Unix文件系统有四个主要职责：

- 提供一个命名层次结构，应用程序可以通过这个层次结构将任意的数据块与任何长度的时间相关联，

- 将物理存储介质虚拟化到这些名称上，

- 在任何时候检索这些命名的数据块，
- 检查资源使用情况。

直接支持这些功能的元数据（例如文件、目录、空间映射）有时被称为主要元数据。次要元数据（例如反向映射和目录父指针）支持文件系统内部的操作，如内部一致性检查和重组。正如其名称所示，汇总元数据是为了性能原因而将主要元数据中的信息进行浓缩。 

---

### 第一部分：定义了fsck工具是什么以及编写新工具的动机
第二部分和第三部分提供了在线fsck过程的高级概述，并介绍了如何对其进行测试以确保功能正确。
第四部分讨论了新程序的用户界面及其预期的使用模式。
第五部分和第六部分展示了高层组件及其组合方式，并通过案例研究展示了每个修复功能的实际工作原理。
第七部分总结了迄今为止讨论的内容，并推测了在在线fsck之上还可以构建什么其他功能。

.. contents:: 目录
   :local:

1. 什么是文件系统检查？
==============================

一个Unix文件系统有四个主要职责：

- 提供一个命名层次结构，应用程序可以通过这个层次结构将任意的数据块与任何长度的时间相关联，

- 将物理存储介质虚拟化到这些名称上，

- 在任何时候检索这些命名的数据块，
- 检查资源使用情况。

直接支持这些功能的元数据（例如文件、目录、空间映射）有时被称为主要元数据。次要元数据（例如反向映射和目录父指针）支持文件系统内部的操作，如内部一致性检查和重组。正如其名称所示，汇总元数据是为了性能原因而将主要元数据中的信息进行浓缩。
文件系统检查（fsck）工具会检查文件系统中的所有元数据，以查找错误。
除了检查明显的元数据损坏外，fsck还会通过相互参照不同类型的元数据记录来查找不一致性。
人们不喜欢丢失数据，因此大多数fsck工具还包含一些修复所发现问题的能力。
作为提醒——大多数Linux fsck工具的主要目标是将文件系统的元数据恢复到一致状态，而不是最大化数据恢复。
这一原则在这里不会被挑战。
20世纪的文件系统通常在磁盘格式中缺乏任何冗余，这意味着fsck只能通过删除文件直到不再检测到错误的方式来应对错误。
更近的文件系统设计在其元数据中包含了足够的冗余，使得当发生非灾难性错误时，现在可以重新生成数据结构；这种能力有助于两种策略。

+--------------------------------------------------------------------------+
| **注意**：                                                                |
+--------------------------------------------------------------------------+
| 系统管理员通过增加独立存储系统的数量来避免数据丢失，这通常是通过创建备份实现的；他们还通过增加每个存储系统的冗余来避免停机时间，通常是通过创建RAID阵列。 |
| fsck工具只解决第一个问题。                                               |
+--------------------------------------------------------------------------+

TLDR; 显示代码！
-----------------------

代码发布到kernel.org的git树如下：
`内核更改 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=repair-symlink>`_，
`用户空间更改 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfsprogs-dev.git/log/?h=scrub-media-scan-service>`_，
以及 `质量测试更改 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfstests-dev.git/log/?h=repair-dirs>`_
每个添加在线修复功能的内核补丁集将在内核、xfsprogs和fstests的git仓库中使用相同的分支名称。
现有工具
--------------

这里描述的在线fsck工具将是XFS（在Linux上）历史上第三个用于检查和修复文件系统的工具。
两个程序先于它存在：

第一个程序是“xfs_check”，它是作为XFS调试器（“xfs_db”）的一部分创建的，只能用于未挂载的文件系统。
它会遍历文件系统中的所有元数据，查找元数据中的不一致性，但不具备修复所发现问题的能力。
由于其高内存需求和无法修复问题的特点，该程序现已弃用，不再进一步讨论。
第二个程序是“xfs_repair”，它的创建目的是比第一个程序更快且更健壮。
与前辈类似，它也只能用于未挂载的文件系统。
它使用基于范围的内存数据结构来减少内存消耗，并尝试适当地调度预读I/O操作，以减少在扫描整个文件系统的元数据时的I/O等待时间。
该工具最重要的特性是能够通过删除必要的内容来应对文件元数据和目录树中的不一致性，从而消除问题。
空间使用元数据会根据观察到的文件元数据进行重建。

### 问题陈述

当前的XFS工具留下了一些未解决的问题：

1. **用户程序**在意外关机导致元数据出现静默损坏的情况下，**突然失去**对文件系统的访问权限。
这些情况发生得**不可预测**，并且通常没有警告。
2. **用户**在发生**意外关机**后的恢复期间会经历**完全服务中断**。
3. 如果文件系统被下线以**主动查找问题**，**用户**会经历**完全服务中断**。
4. **数据所有者**无法在不读取所有数据的情况下**检查数据的完整性**。这可能导致他们在存储系统管理员执行线性介质扫描就足够的情况下面临大量的账单费用。
5. 如果缺乏在线评估文件系统健康状况的手段，**系统管理员**将无法**安排**一个维护窗口来处理损坏问题。
6. **机群监控工具**无法在需要**手动干预**和停机的情况下**自动化定期检查**文件系统的健康状况。
7. **用户**可能会因为恶意行为者利用Unicode的某些特性在目录中放置误导性的名称而被诱骗执行他们不希望的操作。

鉴于上述需要解决的问题及其受益者，提出的解决方案是一个针对运行中的文件系统的第三个fsck工具。
这个新的第三个程序包含三个部分：内核中的元数据检查设施、内核中的元数据修复设施以及一个用户空间驱动程序，用于在活动文件系统上驱动fsck操作。
``xfs_scrub`` 是该驱动程序的名字。
本文档的其余部分介绍了新fsck工具的目标和用例，描述了其主要设计要点与这些目标之间的联系，并讨论了它与现有工具的相似性和差异性。

+--------------------------------------------------------------------------+
| **注意**：                                                               |
+--------------------------------------------------------------------------+
| 在本文档中，现有的离线fsck工具也可以称为"``xfs_repair``"。               |
| 新的在线fsck工具的用户空间驱动程序可以称为"``xfs_scrub``"。              |
| 在线fsck的内核部分中，验证元数据的部分称为"在线擦除"（online scrub）， |
| 而修复元数据的部分称为"在线修复"（online repair）。                      |
+--------------------------------------------------------------------------+

命名层次结构由称为目录和文件的对象组成，物理空间被划分为称为分配组的部分。
分片技术在高度并行系统上提高了性能，并有助于在发生损坏时限制损害范围。
文件系统的划分为主对象（分配组和inode）意味着有许多机会对文件系统的子集进行有针对性的检查和修复。
在此过程中，其他部分继续处理I/O请求。
即使某个文件系统元数据只能通过扫描整个系统来重新生成，该扫描也可以在后台进行，同时其他文件操作继续执行。
总之，在线fsck利用资源分片和冗余元数据，在系统运行时实现有针对性的检查和修复操作。
这种能力将与自动系统管理相结合，使XFS的自主自愈最大化服务可用性。

2. 操作理论
======================

由于在线fsck需要锁定和扫描实时元数据对象，在线fsck由三个独立的代码组件组成：
第一个是用户空间驱动程序``xfs_scrub``，负责识别单个元数据项、为它们安排工作项、适当应对结果并向系统管理员报告结果。
第二项和第三项在内核中实现，具有检查和修复各种在线 fsck 工作项的功能。

+------------------------------------------------------------------+
| **注释**:                                                        |
+------------------------------------------------------------------+
| 为了简洁起见，本文档将“在线 fsck 工作项”缩短为“擦除项”。          |
+------------------------------------------------------------------+

擦除项类型按照与 Unix 设计理念一致的方式进行划分，也就是说每个项应处理一个元数据结构的一个方面，并且要处理得当。
范围
-----

原则上，在线 fsck 应该能够检查并修复离线 fsck 程序可以处理的所有内容。然而，在线 fsck 无法一直运行，这意味着潜在错误可能在一次擦除完成后出现。如果这些错误导致下次挂载失败，则只能使用离线 fsck。这一限制意味着离线 fsck 工具的维护将继续进行。

在线 fsck 的第二个限制是它必须遵循与常规文件系统相同的资源共享和锁获取规则。这意味着擦除操作不能采取任何捷径来节省时间，因为这样做可能会导致并发问题。换句话说，在线 fsck 并不是离线 fsck 的完全替代品，而且一次完整的在线 fsck 运行可能比离线 fsck 花费更长时间。然而，这两个限制是可以接受的权衡，以满足在线 fsck 的不同动机：**最小化系统停机时间** 和 **增加操作的可预测性**。
工作阶段
--------

用户空间驱动程序 `xfs_scrub` 将检查和修复整个文件系统的任务分为七个阶段。每个阶段专注于检查特定类型的清理项，并依赖于所有前一阶段的成功完成。这七个阶段如下：

1. 收集已挂载文件系统和计算机的几何信息，发现内核的在线文件系统检查（fsck）功能，并打开底层存储设备。
2. 检查分配组元数据、所有实时卷元数据和所有配额文件。每个元数据结构都被安排为一个单独的清理项。如果在inode头或inode B树中发现损坏，并且允许 `xfs_scrub` 进行修复，则这些清理项将被修复以准备进入第三阶段。修复是通过使用清理项中的信息重新提交带有修复标志的内核清理调用实现的；这一点将在下一节中讨论。优化和其他所有修复推迟到第四阶段。
3. 检查文件系统中每个文件的所有元数据。每个元数据结构也被安排为一个单独的清理项。
如果需要进行修复，并且允许 `xfs_scrub` 执行修复，并且在第二阶段没有检测到问题，则立即修复这些检查项。

优化、延迟修复和未成功的修复将推迟到第四阶段。

4. 如果调用者允许，此阶段将执行所有剩余的修复和计划的优化。
在开始修复之前，会检查汇总计数器并执行必要的修复，以确保后续修复不会因为汇总计数器严重错误而导致资源预留步骤失败。
只要文件系统中的某个地方的修复有进展，未成功的修复会被重新排队。
如果文件系统是干净的，则在第四阶段结束时修剪文件系统中的空闲空间。

5. 到本阶段开始时，所有主要和次要文件系统元数据必须正确。
检查并纠正汇总计数器，如空闲空间计数和配额资源计数。
检查目录条目名称和扩展属性名称，查找可疑条目，如名称中出现控制字符或令人困惑的 Unicode 序列。

6. 如果调用者要求进行介质扫描，则读取文件系统中所有分配和写入的数据文件扩展。
使用硬件辅助的数据文件完整性检查功能是在线文件系统检查（online fsck）中的新特性；之前的工具都没有这一功能。
如果发生介质错误，它们将被映射到所属的文件并进行报告。
7. 重新检查汇总计数器，并向调用者提供空间使用和文件数量的汇总。
此职责分配将在本文档后面的 :ref:`scrubcheck` 部分中重新审视。

每个清理项的步骤
-------------------------

内核清理代码采用三步策略来检查和修复由清理项表示的元数据对象的一个方面：

1. 检查感兴趣的清理项是否存在损坏、优化机会以及是否包含由系统管理员直接控制但看起来可疑的值。
   如果该项没有损坏且不需要优化，则释放资源并将正面扫描结果返回给用户空间。
   如果该项存在损坏或可以优化但调用者不允许这样做，则释放资源并将负面扫描结果返回给用户空间。
   否则，内核进入第二步。
2. 调用修复函数重建数据结构。
   修复函数通常选择从其他元数据重建结构，而不是尝试挽救现有的结构。
如果修复失败，第一步的扫描结果将返回给用户空间。
否则，内核会进入第三步。
3. 在第三步中，内核对新的元数据项进行相同的检查以评估修复的效果。
重新评估的结果将返回给用户空间。

元数据分类
-----------

每种元数据对象（因此也是每种清理项）按如下分类：

主要元数据
```````````````

属于这一类别的元数据结构通常是文件系统用户最熟悉的，因为它们要么直接由用户创建，要么索引用户创建的对象。大多数文件系统对象都属于此类：

- 空闲空间和引用计数信息

- 索引节点记录和索引

- 文件数据的存储映射信息

- 目录

- 扩展属性

- 符号链接

- 配额限制

清理遵循与常规文件系统访问相同的资源和锁获取规则。主要元数据对象是清理处理中最简单的类型。被清理的项目所归属的主要文件系统对象（分配组或索引节点）会被锁定，以防并发更新。检查函数会检查与该类型相关的所有记录是否存在明显的错误，并将健康记录与其他元数据进行交叉验证以查找不一致之处。对于这类清理项的修复较为简单，因为修复函数在执行时已经持有了上一步骤中获取的所有资源。修复函数会根据需要扫描可用的元数据，记录完成结构所需的所有观察结果。
接下来，它将观察结果暂存到一个新的磁盘结构中，并以原子方式提交以完成修复。最后，小心地回收旧数据结构中的存储空间。

由于“xfs_scrub”在整个修复过程中锁定一个主要对象，这实际上是在文件系统的一个子集上执行的离线修复操作。这最小化了修复代码的复杂性，因为无需处理来自其他线程的并发更新，也无需访问文件系统的其他部分。因此，索引结构可以非常快速地重建，试图访问损坏结构的程序将在修复完成前被阻塞。

修复代码所需的唯一基础设施是用于暂存观察结果的区域以及一种将新结构写入磁盘的方法。

尽管存在这些限制，在线修复的优势显而易见：针对文件系统各个部分进行修复避免了服务的完全中断。

这一机制在 V. Srinivasan 和 M. J. Carey 的论文《在线索引构建算法的性能》（"Performance of On-Line Index Construction Algorithms"）第 2.1 节（“离线算法”）中有详细描述，该论文发表于《扩展数据库技术》（*Extending Database Technology*），1992 年，第 293-309 页。

大多数主要元数据修复函数在格式化新的磁盘结构之前，会将中间结果暂存在内存数组中，这与 Srinivasan 在第 2.3 节（“基于列表的算法”）中讨论的基于列表的算法非常相似。然而，任何在整个修复过程中保持资源锁的数据结构构建器始终是一种离线算法。
.. _secondary_metadata:

次要元数据
```````````````

此类别的元数据结构反映了在主要元数据中找到的记录，但仅用于在线文件系统检查（fsck）或文件系统的重组。次要元数据包括：

- 反向映射信息

- 目录父指针

这类元数据对于 scrub 处理来说较为困难，因为 scrub 会附加到次要对象上，但需要检查主要元数据，这与通常的资源获取顺序相反。通常这意味着需要对整个文件系统进行扫描以重建元数据。检查函数可以通过限制范围来减少运行时间。然而，修复操作需要对主要元数据进行全面扫描，这可能需要很长时间才能完成。在这种情况下，`xfs_scrub` 无法在整个修复过程中锁定资源。相反，修复函数会在内存中设置一个暂存结构来存储观察结果。根据特定修复函数的要求，暂存索引将具有与磁盘上的结构相同的格式，或者具有针对该修复函数专门设计的格式。下一步是释放所有锁并开始文件系统扫描。当修复扫描器需要记录一个观察结果时，会锁定暂存数据足够长的时间以应用更新。
当文件系统扫描正在进行时，修复功能会挂钩文件系统，以便可以将待处理的文件系统更新应用到暂存信息中。
一旦扫描完成，拥有对象会被重新锁定，使用实时数据写入一个新的磁盘结构，并且修复操作会以原子方式提交。
挂钩会被禁用，并且暂存区域会被释放。
最后，旧数据结构的存储空间会被仔细回收。
引入并发有助于在线修复避免各种锁定问题，但会大大增加代码复杂性。
实时文件系统代码必须被挂钩，以便修复功能可以观察正在进行的更新。
暂存区域必须成为一个完全功能的并行结构，以便可以从挂钩中合并更新。
最后，挂钩、文件系统扫描和inode锁定模型必须足够地集成，使得挂钩事件能够决定某个更新是否应应用于暂存结构。
理论上，清理实现可以对主要元数据应用相同的技术，但这样做会使其实现变得极其复杂且性能降低。
尝试访问损坏结构的程序不会被阻止运行，这可能会导致应用程序失败或意外的文件系统关闭。
次级元数据修复策略的灵感来源于Srinivasan的第2.4节，以及C. Mohan在《无需停止更新即可为非常大的表创建索引的算法》(1992年)中的第2节("NSF: 不使用辅助文件构建索引")和第3.1.1节("重复键插入问题")。

上述提到的辅助索引与Srinivasan和Mohan中提到的辅助文件方法有些相似。
他们的方法包括一个索引构建器，该构建器提取相关记录数据以尽可能快地构建新结构；以及一个辅助结构，用于捕获所有其他线程在新索引上线时会提交的所有更新。

在索引构建扫描完成后，记录在辅助文件中的更新将应用于新索引。
为了避免索引构建器与其他写入线程之间的冲突，构建器维护一个公开可见的指针，跟踪扫描在整个记录空间中的进度。
为了避免辅助文件与索引构建器之间的工作重复，当更新的记录ID大于记录ID空间中的指针位置时，辅助文件中的更新将被忽略。

为了尽量减少对代码库其余部分的更改，XFS在线修复在新索引完全准备好之前将其隐藏。
换句话说，在修复过程中不会尝试暴露新索引的键空间。
这种做法的复杂性非常高，或许更适合构建新的索引。

**未来工作的问题**：用于促进修复的全扫描和实时更新代码是否也可以用来实现全面检查？

*答案*：理论上可以。如果每个清理函数都使用这些实时扫描来构建元数据的影子副本，并将其与磁盘上的记录进行比较，那么检查将会更加强大。
然而，这样做比当前的检查函数需要更多的工作量。
在线扫描和钩子是在后来才开发出来的。
这反过来增加了那些清理函数的运行时间。
摘要信息
```````````````````

最后一类元数据结构汇总了主要元数据记录的内容。
这些通常用于加速资源使用查询，并且通常比它们所代表的主要元数据小得多。
摘要信息的例子包括：

- 空闲空间和inode的汇总计数

- 目录中的文件链接计数

- 配额资源使用计数

检查和修复需要完整的文件系统扫描，但资源和锁的获取遵循与常规文件系统访问相同的路径。
超级块汇总计数器由于其内核计数器的底层实现有特殊要求，将单独处理。
其他类型的摘要计数器（配额资源计数和文件链接计数）的检查和修复使用了上述相同的技术进行文件系统扫描和钩子操作，但由于底层数据是一组整数计数器，因此暂存数据不需要是磁盘结构的完全功能镜像。
配额和文件链接计数修复策略的灵感来自G. Graefe的论文《Concurrent Queries and Updates in Summary Views and Their Indexes》中2.12节（“在线索引操作”）至2.14节（“增量视图维护”）的部分内容，该论文发表于2011年。
由于配额是非负整数形式的资源使用计数，在线配额检查可以使用2.14节描述的增量视图差异来跟踪每个事务中待处理的块和inode使用计数的变化，并在事务提交时将这些变化提交到一个dquot辅助文件中。 
<http://www.odbms.org/wp-content/uploads/2014/06/Increment-locks.pdf>
### Delta Tracking

对于 dquot（磁盘配额）而言，delta 跟踪是必要的，因为索引构建器会扫描 inode，而正在重建的数据结构是一个 dquot 索引。链接计数检查将视图差异和提交步骤合二为一，因为它设置的是被扫描对象的属性，而不是将它们写入一个单独的数据结构。

每个在线 fsck 函数将在本文档后面的案例研究中进行讨论。

### 风险管理
--------------

在开发在线 fsck 的过程中，识别出了几个可能使该功能不适合某些分发商和用户的风险因素。虽然可以采取措施来减轻或消除这些风险，但这样做会牺牲一些功能。

- **性能下降**：向文件系统添加元数据索引会增加将更改持久化到磁盘的时间成本，反向空间映射和目录父指针也不例外。要求最高性能的系统管理员可以在格式化时禁用反向映射特性，但这一选择会极大地降低在线 fsck 发现并修复不一致的能力。
- **错误修复**：与所有软件一样，可能会存在导致错误修复被写入文件系统的缺陷。作者通过系统性的模糊测试（下一节详细说明）来尽早发现漏洞，但这种方法可能无法发现所有问题。内核构建系统提供了 Kconfig 选项（`CONFIG_XFS_ONLINE_SCRUB` 和 `CONFIG_XFS_ONLINE_REPAIR`），使得分发商可以选择不接受这种风险。
- **xfsprogs 构建系统**：xfsprogs 的构建系统有一个配置选项（`--enable-scrub=no`），该选项可以禁用 `xfs_scrub` 二进制文件的构建。然而，如果内核功能仍然启用，这并不能降低风险。

- **无法修复**：有时，文件系统损坏得太严重以至于无法修复。如果多个元数据索引的关键空间在某种程度上重叠，但无法从收集到的记录中形成一致的叙述，则修复会失败。为了减少带有脏事务的修复失败导致文件系统不可用的可能性，在线修复功能被设计为在提交新结构之前先阶段化和验证所有新记录。

- **异常行为**：在线 fsck 需要许多特权——例如对块设备进行原始 I/O 操作、通过句柄打开文件、忽略 Unix 自主访问控制以及执行管理更改的能力。自动在后台运行此操作会让人感到不安，因此 systemd 后台服务被配置为仅使用所需的特权运行。显然，这不能解决某些问题，比如内核崩溃或死锁，但它应该足以防止擦除过程逃逸并重新配置系统。定时任务没有这种保护。

- **模糊测试爱好者**：现在有很多人似乎认为运行自动化模糊测试来查找恶意行为，并将利用代码喷洒到公共邮件列表以实现即时零日披露是一种社会福利。在作者看来，只有当模糊测试者帮助修复这些缺陷时，才能实现这一好处，但这一观点显然在所谓的安全“研究人员”中并不广泛共享。
XFS维护者持续管理这些事件的能力对开发过程的稳定性构成了持续的风险。
自动化测试应在特性仍处于实验阶段时提前承担部分风险。
许多这些风险是软件编程固有的。
尽管如此，希望这一新功能能够有助于减少意外停机时间。

### 测试计划

如前所述，fsck 工具有三个主要目标：

1. 检测元数据中的不一致性；
2. 消除这些不一致性；
3. 最小化进一步的数据丢失。

正确操作的演示对于建立用户对软件按预期行为的信心是必要的。
不幸的是，在低成本高IOPS存储的虚拟机引入之前，定期对fsck工具的各个方面进行全面测试实际上是不可行的。
考虑到充足的硬件资源，针对在线fsck项目的测试策略包括与现有fsck工具进行差异分析，并系统地测试每种元数据对象的每个属性。
测试可以分为以下四个主要类别，具体如下：

#### 集成测试与fstests

任何自由软件质量保证工作的主要目标是使测试尽可能便宜和广泛，以最大化社区的规模优势。
换句话说，测试应最大化覆盖文件系统配置场景和硬件设置的广度。这通过使在线 fsck 的作者能够及早发现并修复错误来提高代码质量，并帮助新功能的开发者在开发过程中更早地发现集成问题。

Linux 文件系统社区共享一个通用的质量保证测试套件 `fstests <https://git.kernel.org/pub/scm/fs/xfs/xfstests-dev.git/>`_，用于功能测试和回归测试。

甚至在在线 fsck 的开发工作开始之前，fstests（当运行于 XFS 时）就会在每个测试之间对测试文件系统和临时文件系统运行 `xfs_check` 和 `xfs_repair -n` 命令。这提供了一定程度的保证，即内核和 fsck 工具对于一致的元数据保持同步。

在开发在线检查代码期间，修改了 fstests，使其在每个测试之间运行 `xfs_scrub -n`，以确保新的检查代码产生的结果与现有的两个 fsck 工具相同。

为了开始在线修复的开发，修改了 fstests 以运行 `xfs_repair`，以便在测试之间重建文件系统的元数据索引。这确保了离线修复不会导致崩溃、修复后留下损坏的文件系统或触发在线检查的警告。这也为哪些可以离线修复、哪些不可以离线修复建立了基准。

为了完成在线修复的第一阶段开发，修改了 fstests 以能够在“强制重建”模式下运行 `xfs_scrub`。
这使得我们能够比较在线修复与现有离线修复工具的有效性。

通用的元数据块模糊测试
------------------------

XFS 从拥有一个非常强大的调试工具 `xfs_db` 中受益匪浅。在开发在线 fsck 之前，已经创建了一套 fstests 来测试一种较为常见的故障：整个元数据块被损坏。这需要创建一个 fstests 库代码，该代码能够创建包含所有可能类型的元数据对象的文件系统。接下来，创建了单独的测试用例来创建一个测试文件系统，识别特定类型的元数据对象的一个块，使用现有的 `xfs_db` 中的 `blocktrash` 命令破坏它，并测试特定元数据验证策略的反应。这个早期的测试套件使 XFS 开发者能够测试内核验证函数的能力以及离线 fsck 工具检测和消除不一致元数据的能力。测试套件的这一部分被扩展以完全相同的方式覆盖在线 fsck。换句话说，对于给定的 fstests 文件系统配置：

* 对于文件系统中存在的每个元数据对象：

  * 向其写入垃圾数据

  * 测试以下几种反应：

    1. 内核验证器阻止明显错误的元数据
    2. 离线修复 (`xfs_repair`) 检测并修复
    3. 在线修复 (`xfs_scrub`) 检测并修复

针对元数据记录的模糊测试
--------------------------

在线 fsck 的测试计划包括扩展现有的文件系统测试基础设施，以提供更强大的功能：对文件系统中每个元数据对象的每个元数据字段进行针对性的模糊测试。`xfs_db` 可以修改文件系统中每个块中的每个元数据结构的每个字段，以模拟内存损坏和软件漏洞的影响。由于 fstests 已经包含了创建包含所有已知元数据格式的文件系统的能力，因此可以使用 `xfs_db` 进行详尽的模糊测试！

对于给定的 fstests 文件系统配置：

* 对于文件系统中存在的每个元数据对象：
* 对该元数据对象中的每个记录...
* 对该记录中的每个字段...
* 对可以应用于位字段的每种可想象的转换...
  1. 清除所有位
  2. 设置所有位
  3. 切换最高有效位
  4. 切换中间位
  5. 切换最低有效位
  6. 增加少量值
  7. 减少少量值
  8. 随机化内容
  
  * ...测试以下反应：

    1. 内核验证器阻止明显的坏元数据
    2. 离线检查（`xfs_repair -n`）
    3. 离线修复（`xfs_repair`）
    4. 在线检查（`xfs_scrub -n`）
    5. 在线修复（`xfs_scrub`）
    6. 两种修复工具（先使用`xfs_scrub`，如果在线修复不成功，则使用`xfs_repair`）

这是一个相当复杂的组合爆炸！

幸运的是，拥有如此全面的测试覆盖范围使得XFS开发者能够轻松地检查XFS的fsck工具的响应。自从引入模糊测试框架以来，这些测试被用来发现`xfs_repair`中错误的修复代码和整个类别的元数据对象中缺失的功能。增强的测试被用于最终确定`xfs_check`的弃用，通过确认`xfs_repair`至少能检测到与旧工具一样多的损坏情况。这些测试对`xfs_scrub`同样非常有价值——它们允许在线fsck开发者将在线fsck与离线fsck进行比较，并使XFS开发者能够找到代码库中的不足之处。

提出的补丁集包括：
- 通用模糊测试改进：[链接](https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfstests-dev.git/log/?h=fuzzer-improvements)
- 模糊测试基线：[链接](https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfstests-dev.git/log/?h=fuzz-baseline)
- 提高模糊测试的全面性：[链接](https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfstests-dev.git/log/?h=more-fuzz-testing)

压力测试
--------

在线fsck的一个独特要求是能够在常规工作负载的同时操作文件系统。虽然当然不可能在系统运行时完全没有影响地运行`xfs_scrub`，但在线修复代码绝不应向文件系统元数据中引入不一致性，且常规工作负载不应注意到资源饥饿问题。
为了验证这些条件是否得到满足，fstests 已经在以下方面进行了增强：

* 对于每种 scrub 项类型，创建一个测试，在运行 ``fsstress`` 时检查该类型的项目
* 对于每种 scrub 项类型，创建一个测试，在运行 ``fsstress`` 时修复该类型的项目
* 与 ``fsstress`` 并行运行 ``xfs_scrub -n``，以确保检查整个文件系统不会引起问题
* 在强制重建模式下与 ``fsstress`` 并行运行 ``xfs_scrub``，以确保强制修复整个文件系统不会引起问题
* 在冻结和解冻文件系统时，与 ``fsstress`` 并行运行 ``xfs_scrub`` 的检查和强制修复模式
* 在只读和读写重新挂载文件系统时，与 ``fsstress`` 并行运行 ``xfs_scrub`` 的检查和强制修复模式
* 同上，但使用 ``fsx`` 而不是 ``fsstress`` 进行测试。（尚未完成？）

成功的定义是能够在不观察到由于元数据损坏、内核挂起检查警告或其他任何形式的恶意行为导致的意外文件系统关闭的情况下运行所有这些测试。

提出的补丁集包括 `通用压力测试 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfstests-dev.git/log/?h=race-scrub-and-mount-state-changes>`_ 和 `现有按功能压力测试的演变 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfstests-dev.git/log/?h=refactor-scrub-stress>`_

### 4. 用户界面

在线 fsck 的主要用户是系统管理员，就像离线修复一样。
在线 fsck 向管理员提供了两种操作模式：
一种是在需要时进行在线 fsck 的前台命令行进程，另一种是在后台执行自主检查和修复的服务。
按需检查
------------------

对于希望获取文件系统元数据最新信息的管理员来说，可以在命令行上以前台进程的方式运行 ``xfs_scrub``。
该程序会在管理员等待结果报告时检查文件系统中的每一条元数据，类似于现有的 ``xfs_repair`` 工具。
这两个工具都共享一个 ``-n`` 选项来执行只读扫描，以及一个 ``-v`` 选项来增加报告信息的详细程度。
``xfs_scrub`` 的一个新功能是 ``-x`` 选项，它利用硬件的错误校正能力来检查数据文件内容。
默认情况下不启用介质扫描，因为这可能会显著增加程序运行时间，并在较旧的存储硬件上消耗大量带宽。
前台调用的输出会被记录在系统日志中。
``xfs_scrub_all`` 程序会遍历已挂载的文件系统列表，并为每个文件系统并行启动 ``xfs_scrub``。
对于解析到同一顶级内核块设备的任何文件系统，它会串行化扫描以防止资源过度消耗。

后台服务
------------------

为了减轻系统管理员的工作负担，``xfs_scrub`` 包提供了一套 `systemd <https://systemd.io/>`_ 定时器和服务，默认情况下在周末自动在线运行 fsck。
后台服务配置了 ``xfs_scrub`` 以尽可能低的权限运行，最低的 CPU 和 I/O 优先级，并且在单线程模式下受到 CPU 限制。
这可以根据客户工作负载的延迟和吞吐量需求，由systemd管理员随时进行调整。
后台服务的输出也会记录在系统日志中。
如果需要，可以通过设置以下服务文件中的`EMAIL_ADDR`环境变量，自动发送失败报告（无论是由于不一致还是运行时错误）：

* ``xfs_scrub_fail@.service``
* ``xfs_scrub_media_fail@.service``
* ``xfs_scrub_all_fail.service``

是否启用后台扫描的决定留给系统管理员。
这可以通过启用以下任一服务来完成：

* 在systemd系统上启用``xfs_scrub_all.timer``
* 在非systemd系统上启用``xfs_scrub_all.cron``

此自动每周扫描默认配置为每月执行一次额外的媒体扫描，以检查所有文件数据。
这种方法不如存储文件数据块校验和那样可靠，但如果应用程序软件提供了自己的完整性检查，或者存储设备的完整性保证被认为足够的话，这种方法的性能要好得多。
systemd单元文件定义已经经过了安全审计（截至systemd 249版本），以确保xfs_scrub进程对系统的其他部分的访问尽可能少。
这是通过`systemd-analyze security`完成的，在此之后，权限被限制到最小必要范围，沙箱被设置到最大可能程度，并且对文件系统树的访问也被限制到启动程序和访问被扫描文件系统所需的最小范围。
服务定义文件将CPU使用限制为一个CPU核心的80%，并尽可能地应用了友好的I/O和CPU调度优先级。
采取这一措施是为了尽量减少文件系统的其余部分的延迟。
对于cron作业，尚未进行此类加固。
提议的补丁集：
`启用 xfs_scrub 后台服务 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfsprogs-dev.git/log/?h=scrub-media-scan-service>`_
健康报告
--------

XFS 在内存中缓存每个文件系统的健康状态摘要。
每当运行 `xfs_scrub` 或在常规操作期间检测到文件系统元数据中的不一致时，该信息都会得到更新。
系统管理员应使用 `xfs_spaceman` 的 `health` 命令将此信息下载为人类可读的格式。
如果发现问题，管理员可以安排一个减少的服务窗口来运行在线修复工具以解决问题。
如果不成功，管理员可以决定安排一个维护窗口来运行传统的离线修复工具以解决问题。

**未来工作问题**：健康报告是否应该与新的 inotify 文件系统错误通知系统集成？
对于系统管理员来说，有一个守护进程监听损坏通知并启动修复是否有帮助？

*答案*：这些问题仍然没有答案，但应该成为与早期采用者和潜在下游用户讨论的一部分。
提议的补丁集包括 `将健康报告与修复结果关联起来 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=corruption-health-reports>`_ 和 `在内存回收期间保留疾病信息 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=indirect-health-reporting>`_。

5. 内核算法和数据结构
========================

本节讨论提供在系统运行时检查和修复元数据能力的关键内核算法和数据结构。
本节的前几章揭示了提供元数据检查基础的部分。
本节的其余部分介绍了XFS如何通过某些机制进行自我恢复。

自描述元数据
-------------------------

从2012年的XFS版本5开始，XFS更新了几乎每一个磁盘块头的格式，记录了一个魔法数字、校验和、一个全局“唯一”标识符（UUID）、所有者代码、该块在磁盘上的地址以及日志序列号。
当从磁盘加载块缓冲区时，魔法数字、UUID、所有者代码和磁盘地址确认所检索的块与当前文件系统的特定所有者匹配，并且块中的信息应该位于磁盘地址处。
前三个组件使得检查工具能够忽略不属于该文件系统的所谓元数据，第四个组件使得文件系统能够检测丢失的写入。
每当文件系统操作修改一个块时，该更改会作为事务的一部分提交到日志中。
日志随后处理这些事务，在它们安全地持久化到存储后标记为完成。
日志代码维护最后一个事务性更新的校验和和日志序列号。
校验和有助于检测撕裂写和其他可能在计算机及其存储设备之间引入的不一致问题。
序列号跟踪使日志恢复能够避免将过时的日志更新应用到文件系统上。
这两个特性通过提供一种手段来检测读取元数据块时的明显损坏，从而提高了整体运行时的弹性，但这些缓冲验证器无法提供元数据结构之间的任何一致性检查。
更多信息，请参阅文档：
Documentation/filesystems/xfs/xfs-self-describing-metadata.rst

反向映射
--------------

XFS 的原始设计（大约在 1993 年）是对 20 世纪 80 年代 Unix 文件系统设计的改进。
在那个时代，存储密度昂贵，CPU 时间稀缺，过高的寻道时间会严重影响性能。
出于性能原因，文件系统的设计者不愿意在文件系统中增加冗余，即使这会以数据完整性为代价。
21 世纪初的文件系统设计者选择不同的策略来增加内部冗余——要么存储几乎相同的元数据副本，要么使用更高效的空间编码技术。
对于 XFS，选择了一种不同的冗余策略来现代化设计：一个次级空间使用索引，将分配的磁盘区段映射回其所有者。
通过添加一个新的索引，文件系统保留了其大部分处理大量数据集时对高度多线程工作负载的良好扩展能力，因为主要的文件元数据（目录树、文件块映射和分配组）保持不变。
像任何提高冗余性的系统一样，反向映射功能增加了空间映射活动的开销成本。
然而，它有两个关键优势：首先，反向索引是实现在线 fsck 和其他请求功能（如空闲空间碎片整理、更好的介质故障报告和文件系统缩小）的关键；其次，反向映射 B 树的不同磁盘存储格式能够阻止设备级别的去重，因为文件系统需要真实的冗余。

+--------------------------------------------------------------------------+
| **侧栏**：                                                               |
+--------------------------------------------------------------------------+
| 对于添加次级索引的一个批评是，它并没有提高用户数据存储本身的鲁棒性。  |
| 这是一个有效的观点，但为文件数据块校验和添加新的索引会增加写入放大，  |
| 因为数据重写变成了复制写入，从而提前老化文件系统。                     |
| 按照过去三十年的先例，希望保证文件数据完整性的用户可以提供他们所需的   |
| 强大解决方案。至于元数据，添加新的次级空间使用索引的复杂性远小于在 XFS  |
| 中添加卷管理和存储设备镜像。                                            |
| RAID 和卷管理的完善最好留给内核现有的层来完成。                          |
+--------------------------------------------------------------------------+

反向空间映射记录中捕获的信息如下：

.. code-block:: c

    struct xfs_rmap_irec {
        xfs_agblock_t    rm_startblock;   /* 区段起始块 */
        xfs_extlen_t     rm_blockcount;   /* 区段长度 */
        uint64_t         rm_owner;        /* 区段所有者 */
        uint64_t         rm_offset;       /* 在所有者内的偏移量 */
        unsigned int     rm_flags;        /* 状态标志 */
    };

前两个字段捕获了物理空间的位置和大小，单位为文件系统的块。
所有者字段告诉scrub哪些元数据结构或文件inode被分配了这个空间。
对于分配给文件的空间，偏移量字段告诉scrub这个空间在文件叉中的映射位置。
最后，标志字段提供了关于空间使用情况的额外信息——这是属性叉的扩展？文件映射b树的扩展？还是未写入的数据扩展？

在线文件系统检查通过将每个主要元数据记录的信息与其他所有空间索引进行比较来判断其一致性。
反向映射索引在一致性检查过程中起着关键作用，因为它包含所有空间分配信息的一个集中副本。
程序运行时间和资源获取的难易程度是在线检查所能参考的唯一真正限制。
例如，可以将文件数据扩展映射与以下内容进行检查：

* 在空闲空间信息中没有条目
* 在inode索引中没有条目
* 如果文件未标记为具有共享扩展，则在引用计数数据中没有条目
* 反向映射信息中条目的对应性

关于反向映射索引有几个观察点：

1. 如果上述任何主要元数据存在疑问，反向映射可以提供正确性的正面确认。
### 检查和交叉引用

#### 检查代码对于大多数基本元数据遵循的路径与上述路径类似。

2. 证明次级元数据与初级元数据的一致性非常困难，因为这需要对所有初级空间元数据进行全面扫描，而这非常耗时。
例如，检查文件扩展映射B树块的反向映射记录需要锁定文件并搜索整个B树以确认该块。

相反，清理（scrub）依赖于在检查初级空间映射结构期间进行严格的交叉引用。

3. 如果所需的锁定顺序与常规文件系统操作使用的顺序不同，则一致性扫描必须使用非阻塞锁获取原语。
例如，如果文件系统通常在获取AGF缓冲锁之前先获取文件ILOCK，但清理希望在持有AGF缓冲锁的同时获取文件ILOCK，那么清理不能在这第二次获取上阻塞。

这意味着，在系统负载较重的情况下，无法保证在检查反向映射数据的这一部分过程中向前进展。

总之，反向映射在重建初级元数据中起着关键作用。
这些记录如何分阶段、写入磁盘以及提交到文件系统的详细信息将在后续章节中介绍。

#### 检查和交叉引用

检查元数据结构的第一步是检查结构内包含的所有记录及其与系统其他部分的关系。
XFS 包含多层检查，旨在防止不一致的元数据对系统造成破坏。
每一层都提供有助于内核做出以下三个决策的信息：

- 这个结构的一部分是否明显损坏（`XFS_SCRUB_OFLAG_CORRUPT`）？
- 这个结构是否与系统的其他部分不一致（`XFS_SCRUB_OFLAG_XCORRUPT`）？
- 文件系统中是否有如此多的损坏以至于无法进行交叉引用（`XFS_SCRUB_OFLAG_XFAIL`）？
- 结构是否可以优化以提高性能或减少元数据大小（`XFS_SCRUB_OFLAG_PREEN`）？
- 结构中是否包含虽然不矛盾但需要系统管理员审查的数据（`XFS_SCRUB_OFLAG_WARNING`）？

以下各节描述了元数据扫描过程的工作原理。

### 元数据缓冲区验证
```````````````````````````

XFS 中最低层次的元数据保护是构建在缓冲区缓存中的元数据验证器。
这些函数执行廉价的块内部一致性检查，并回答以下问题：

- 这个块是否属于这个文件系统？
- 这个块是否属于请求读取的结构？这假设元数据块只有一个所有者，在 XFS 中始终为真。
- 存储在块中的数据类型是否在扫描期望的合理范围内？
- 块的物理位置是否与其读取的位置匹配？
- 块的校验和是否与数据匹配？

这里的保护范围非常有限——验证器只能确定文件系统代码基本没有明显的损坏错误，并且存储系统能够基本正确地检索数据。
运行时观察到的损坏问题会导致生成健康报告、失败的系统调用以及在极端情况下，如果损坏的元数据迫使取消脏事务，则关闭文件系统。
每个在线 fsck 扫描函数都应在检查过程中读取结构中的每一个磁盘上的元数据块。
检查过程中观察到的损坏问题会立即报告给用户空间作为损坏；在进行交叉引用时，会在完整检查完成后报告为交叉引用失败。
已由缓存中的缓冲区满足的读取（因此已经经过验证）会绕过这些检查。

### 内部一致性检查
```````````````````````````

在缓冲区缓存之后，下一层元数据保护是由文件系统内置的内部记录验证代码提供的。
这些检查由缓冲区验证器、使用缓冲区缓存的文件系统内部用户以及擦除代码本身分担，具体取决于所需的高级上下文量。检查的范围仍然限于块内部。这些高级检查功能回答了以下问题：

- 块中存储的数据类型是否与擦除操作所期望的一致？
- 这个块是否属于请求读取的操作所属的结构？
- 如果块包含记录，这些记录是否适合在该块内？
- 如果块跟踪内部空闲空间信息，这些信息是否与记录区域一致？
- 块内的记录是否存在明显的损坏？

这类记录检查更为严格且耗时更多。例如，会检查块指针和inode以确保它们指向动态分配部分内的分配组和文件系统。会检查名称中的无效字符，并检查标志以确保没有无效组合。还会检查其他记录属性以确保其值合理。对于B树记录，会检查其在B树键空间区间内的正确顺序和不可合并性（除了文件分支映射）。出于性能原因，常规代码可能会跳过某些检查，除非启用了调试模式或即将进行写入操作。当然，擦除函数必须检查所有可能的问题。

用户空间控制的记录属性验证

各种文件系统元数据直接由用户空间控制。
由于这一特性，验证工作无法比检查一个值是否在可能的范围内更加精确。这些字段包括：

- 受挂载选项控制的超级块字段
- 文件系统标签
- 文件时间戳
- 文件权限
- 文件大小
- 文件标志
- 目录项、扩展属性键和文件系统标签中存在的名称
- 扩展属性键命名空间
- 扩展属性值
- 文件数据块内容
- 配额限制
- 配额定时器过期（如果资源使用超过软限制）

跨引用空间元数据
```````````````````````````````

在内部块检查之后，更高一级的检查是跨引用不同元数据结构之间的记录。对于常规运行时代码来说，这类检查的成本被认为过于昂贵，但由于scrub专门用于消除不一致情况，因此必须探索所有调查途径。跨引用的具体集合高度依赖于正在检查的数据结构上下文。XFS btree代码中有用于在线fsck跨引用一个结构与另一个结构的键空间扫描函数。具体来说，scrub可以扫描索引的键空间以确定该键空间是否完全、稀疏或根本没有映射到记录。对于反向映射btree，可以在执行键空间扫描时屏蔽部分键，以便scrub可以决定rmap btree是否包含映射特定物理空间范围的记录，而不会受到其余rmap键空间稀疏性的影响。btree块在进行跨引用前会经历以下检查：

- 存储在块中的数据类型是否符合scrub的预期？
- 该块是否属于请求读取的拥有结构？
- 记录是否适合在这个块中？
- 块中的记录是否明显未被损坏？
- 名称哈希是否按正确顺序排列？
- btree内的节点指针是否指向对应类型的btree的有效块地址？
- 子指针是否指向叶节点？
- 兄弟指针是否指向同一级别的块？
- 对于每个节点块记录，记录键是否准确反映子块的内容？

空间分配记录的跨引用如下：

1. 任何由元数据结构提及的空间都按以下方式跨引用：

   - 反向映射索引是否只将适当的拥有者列为每个块的所有者？
   - 是否没有任何块被声明为自由空间？
   - 如果这些不是文件数据块，是否有任何块被声明为不同拥有者共享的空间？

2. btree块的跨引用如下：

   - 上述第1类中的所有内容
   - 如果存在父节点块，列出的键是否匹配该块的键空间？
   - 兄弟指针是否指向有效块？在同一级别上？
   - 子指针是否指向有效块？在下一级？

3. 自由空间btree记录的跨引用如下：

   - 上述第1和第2类中的所有内容
   - 反向映射索引是否没有列出该空间的所有者？
   - 该空间是否没有被inode索引所声明？
   - 它是否没有被引用计数索引提及？
   - 在其他自由空间btree中是否有对应的记录？

4. inode btree记录的跨引用如下：

   - 上述第1和第2类中的所有内容
### 问题翻译：

- 在空闲inode B树中是否存在匹配的记录？

- 孔洞掩码（holemask）中的清除位是否与inode簇对应？

- 自由掩码（freemask）中的设置位是否与链接计数为零的inode记录相对应？

### 第5部分：inode记录交叉引用如下：

- 所有第一类中的内容
  - 汇总文件fork信息的所有字段是否确实与那些fork相匹配？

  - 链接计数为零的每个inode是否对应于自由inode B树中的一个记录？

### 第6部分：文件fork空间映射记录交叉引用如下：

- 上述第一类和第二类中的所有内容
  - 这个空间是否没有被inode B树提及？

  - 如果这是一个CoW fork映射，它是否对应于引用计数B树中的一个CoW条目？

### 第7部分：引用计数记录交叉引用如下：

- 上述第一类和第二类中的所有内容
  - 在rmap B树的空间子键空间中（即所有映射到特定空间范围的记录，并忽略所有者信息），对于每个块是否有与引用计数记录声称的数量相同的反向映射记录？

### 提议的补丁集是为了找到以下记录中的缺口：
- `refcount B树 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=scrub-detect-refcount-gaps>`
- `inode B树 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=scrub-detect-inobt-gaps>`
- `rmap B树 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=scrub-detect-rmapbt-gaps>`
- 找到 `可合并的记录 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=scrub-detect-mergeable-records>`
- 并在开始修复之前改进与rmap的交叉引用 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=scrub-strengthen-rmap-checking>

### 检查扩展属性

扩展属性实现了一个键值存储，允许数据片段附加到任何文件。内核和用户空间都可以访问这些键和值，但受命名空间和权限限制。通常这些数据片段是关于文件的元数据——来源、安全上下文、用户提供的标签、索引信息等。名称可以长达255字节，并且可以存在于多个不同的命名空间中。值可以大到64KB。文件的扩展属性存储在attr fork映射的块中。
这些映射指向叶块、远程值块或 dabtree 块。属性分支中的块 0 总是结构的顶部，但除此之外，这三种类型的块可以在属性分支中的任何偏移位置出现。

叶块包含指向名称和值的属性键记录。名称始终存储在同一叶块中的其他位置。如果值小于文件系统块大小的 3/4，它们也存储在同一叶块中的其他位置。

如果值太大无法放入叶块中，则使用远程值块来存储该值。如果叶信息超过一个文件系统块的大小，则创建一个以块 0 为根的 dabtree，用于将属性名称的哈希映射到属性分支中的叶块。

由于缺少属性块与索引块之间的分离，检查扩展属性结构并不直接。擦除（scrub）必须读取属性分支映射的所有块并忽略非叶块：

1. 遍历属性分支中的 dabtree（如果存在），确保块或 dabtree 映射没有任何异常，并且所有映射都指向属性叶块。
2. 遍历属性分支中的块，寻找叶块。
对于叶子中的每个条目：

   a. 验证名称不包含无效字符
   b. 读取 attr 值
   这会执行一个命名查找，以确保 attr 名称的正确性，
   从而保证 dabtree 的正确性。
   如果值存储在远程块中，这也验证了远程值块的完整性。

目录的检查与交叉引用
``````````````````````````````````````````

文件系统目录树是一个有向无环图结构，其中文件构成节点，而目录项（dirent）构成边。
目录是一种特殊类型的文件，包含从一个 255 字节序列（名称）到 inumber 的一组映射。
这些称为目录项，简称 dirent。
每个目录文件必须恰好有一个指向该文件的目录。
根目录指向自身。
目录项可以指向任何类型的文件。
每个非目录文件可以有多个目录指向它。

在XFS中，目录被实现为一个包含最多三个32GB分区的文件。
- 第一个分区包含目录项数据块。
- 每个数据块包含可变长度的记录，这些记录将用户提供的名称与一个inode（索引节点）以及可选的文件类型关联起来。
- 如果目录项数据超过一个数据块，则第二个分区（作为EOF后的扩展存在）会被填充一个包含空闲空间信息和索引的数据块。该索引将目录项名称的哈希映射到第一个分区中的目录数据块。
- 这使得目录名称查找非常快。
- 如果第二个分区增长超过一个数据块，则第三个分区会被填充一个线性数组的空闲空间信息，以便更快地扩展。
- 如果空闲空间已经被分离，并且第二个分区再次增长超过一个数据块，则使用dabtree来将目录项名称的哈希映射到目录数据块。

检查目录相对直接：
1. 遍历第二个分区中的dabtree（如果存在），以确保没有数据块或dabtree映射中的不规则情况，且这些映射指向的是目录项数据块。
2. 遍历第一个分区中的数据块，查找目录项。
每个目录项（dirent）的检查如下：

a. 名称中是否不包含无效字符？

b. i-number 是否对应一个实际分配的inode？

c. 子inode是否有非零的链接计数？

d. 如果目录项包含文件类型，它是否与inode的类型匹配？

e. 如果子项是子目录，子项的dotdot指针是否指向父目录？

f. 如果目录有第二个分区，执行目录项名称的命名查找，以确保dabtree的正确性。

3. 遍历第三分区中的空闲空间列表（如果存在），以确保其描述的空闲空间确实未被使用。
涉及父目录（:ref:`parents <dirparent>`）和文件链接计数（:ref:`file link counts <nlinks>`）的检查操作将在后面的章节中详细讨论。

检查目录/属性B树
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

如前几节所述，目录/属性B树（dabtree）索引通过映射用户提供的名称来提高查找速度，避免线性扫描。内部，它将名称的32位哈希值映射到适当文件分叉中的块偏移量。dabtree的内部结构类似于记录固定大小元数据记录的B树——每个dabtree块包含一个魔术数字、校验和、兄弟指针、UUID、树层级和日志序列号。叶节点和节点记录的格式相同——每个条目都指向层次结构中的下一级，其中dabtree节点记录指向dabtree叶块，而dabtree叶记录则指向分叉中其他地方的非dabtree块。

检查和交叉引用dabtree与空间B树的操作非常相似：

- 块中存储的数据类型是否与scrub期望的一致？

- 块是否属于请求读取的拥有结构？

- 记录是否在块内合适的位置？

- 块内的记录是否存在明显的损坏？

- 名称哈希是否按正确的顺序排列？

- dabtree内的节点指针是否指向有效的dabtree块偏移量？

- dabtree内的叶指针是否指向有效的目录或attr叶块偏移量？

- 子指针是否指向叶节点？

- 兄弟指针是否在同一层级上指向？

- 对于每个dabtree节点记录，记录键是否准确反映子dabtree块的内容？

- 对于每个dabtree叶记录，记录键是否准确反映目录或attr块的内容？

交叉引用汇总计数器
```````````````````````

XFS维护三类汇总计数器：可用资源、配额资源使用情况和文件链接计数。理论上，可以通过遍历整个文件系统来找到可用资源（数据块、inode、实时范围）的数量。但这会导致报告非常缓慢，因此事务性文件系统可以在超级块中维护这些信息的汇总。
将这些值与文件系统元数据进行交叉验证，本质上是遍历每个 AG 中的空闲空间和 i 节点元数据以及实时位图，但存在一些复杂情况，我们将在后续章节 :ref:`详细讨论 <fscounters>`。
:ref:`配额使用 <quotacheck>` 和 :ref:`文件链接计数 <nlinks>` 的检查足够复杂，需要单独的章节来说明。

修复后的重新验证
```````````````````

在执行修复后，再次运行检查代码以验证新的结构，并将健康评估的结果记录在内部并返回给调用进程。此步骤对于系统管理员监控文件系统的状态及任何修复的进度至关重要。对于开发者而言，这是判断在线和离线检查工具中错误检测和纠正效果的一种有用方法。

最终一致性与在线 fsck
------------------------------------

复杂的操作可以通过一系列事务对多个每 AG 数据结构进行修改。这些事务链一旦提交到日志，在系统崩溃时会通过日志恢复重启。由于事务链中的 AG 头缓冲区在事务之间未锁定，因此在线检查必须与正在进行的事务链协调，以避免因待处理的事务链而误检测出不一致的情况。此外，在有操作待处理的情况下，不能运行在线修复，因为元数据暂时不一致，无法重建。只有在线 fsck 需要 AG 元数据的完全一致性，相比文件系统变更操作，这种情况应该相对较少见。
在线 fsck 与事务链的协调如下：

* 对于每个 AG（分配组），维护一个针对该 AG 的意图项计数
  每当新的项被添加到链中时，应增加该计数
  当文件系统锁定 AG 头缓冲区并完成工作后，应减少该计数
* 当在线 fsck 需要检查某个 AG 时，应锁定 AG 头缓冲区以使所有想要修改该 AG 的事务链静止
  如果计数为零，则继续进行检查操作
  如果计数非零，则循环缓冲锁以允许事务链向前进展
  这可能导致在线 fsck 完成时间较长，但常规文件系统更新优先于后台检查活动
  关于发现这种情况的详细信息将在 :ref:`下一节 <chain_coordination>` 中介绍，关于解决方案的详细信息将在 :ref:`之后的部分<intent_drains>` 中介绍
.. _chain_coordination:

问题的发现
```````````````````````

在开发在线检查的过程中，fsstress 测试揭示了在线 fsck 与由其他写入线程创建的复合事务链之间的不良交互，导致错误地报告元数据不一致。
这些报告的根本原因是由于引入反向映射和 reflink 时扩展延迟工作项和复合事务链所引入的最终一致性模型。
最初，XFS 中引入了事务链来避免在取消文件空间映射时出现死锁。

为了避免死锁，死锁预防规则要求仅按递增顺序锁定 AG（Allocation Group），这使得无法在一个事务中释放 AG 7 中的空间范围，然后再尝试释放 AG 3 中现在多余的块映射 B 树（block mapping btree, BMBT）块。为了解决这类死锁问题，XFS 创建了“Extent Freeing Intent (EFI)”日志项，在一个事务中提交释放某些空间的同时，将实际的元数据更新推迟到一个新的事务中进行。

事务序列如下：

1. 第一个事务包含对文件块映射结构的物理更新，以移除 B 树块的映射。然后，它将一个动作项附加到内存中的事务上，用于调度延迟释放空间。具体来说，每个事务维护一个 `struct xfs_defer_pending` 对象列表，每个对象又维护一个 `struct xfs_extent_free_item` 对象列表。回到上面的例子，该动作项会跟踪释放 AG 7 中未映射的空间和 AG 3 中的块映射 B 树块。通过这种方式记录的延迟释放会在日志中创建一个 EFI 日志项，并将该日志项附加到事务上。当日志被持久化到磁盘时，EFI 项会被写入到磁盘上的事务记录中。EFI 可以列出最多 16 个要释放的范围，所有这些范围都按照 AG 的顺序排序。
2. 第二笔事务包含对 AG 3 的空闲空间 B+树的物理更新，以释放之前的 BMBT 块，以及对 AG 7 的空闲空间 B+树的第二次物理更新，以释放未映射的文件空间。
   注意到在可能的情况下，这些物理更新会按照正确的顺序重新排序。
   该事务附带了一个扩展空闲完成（EFD）日志项。
   EFD 包含一个指向事务 #1 中记录的 EFI 的指针，以便日志恢复可以判断是否需要重放 EFI。
   如果系统在事务 #1 写回到文件系统之后但事务 #2 提交之前崩溃，扫描文件系统的元数据会显示不一致的文件系统元数据，因为看起来没有任何所有者拥有该未映射的空间。
   幸运的是，日志恢复会为我们纠正这种不一致性——当恢复发现意图日志项但没有找到相应的意图完成项时，它将重建意图项的核心状态并完成它。
   在上面的例子中，日志必须重放已恢复的 EFI 中描述的两个释放操作来完成恢复阶段。
   XFS 的事务链策略中有一些细节需要注意：

* 日志项必须按正确的顺序添加到事务中，以防止与事务未持有的主要对象发生冲突。
  换句话说，在释放扩展之前，所有针对未映射块的每个 AG 元数据更新都必须完成，并且不应在最后一个更新提交到日志之前重新分配扩展。
* 在链中的每个事务之间会释放 AG 头缓冲区。
这意味着其他线程可以观察到 AG（分配组）处于中间状态，但只要第一个细微之处处理得当，这不应该影响文件系统操作的正确性。
* 卸载文件系统会将所有待处理的工作写入磁盘，这意味着离线 fsck（文件系统检查和修复工具）永远不会看到由延迟工作项处理引起的临时不一致。
通过这种方式，XFS 使用了一种最终一致性的方式来避免死锁并增加并行性。
在反向映射和 reflink 特性的设计阶段，决定将单个文件系统更改的所有反向映射更新压缩到单个交易中是不切实际的，因为单个文件映射操作可能会分解为许多小的更新：

* 块映射更新本身
* 块映射更新的反向映射更新
* 修正空闲列表
* 空闲列表修正的反向映射更新

* 块映射 B 树的形状变化
* B 树更新的反向映射更新
* 再次修正空闲列表
* 空闲列表修正的反向映射更新

* 引用计数信息的更新
* 引用计数更新的反向映射更新
* 第三次修正空闲列表
* 空闲列表修正的反向映射更新

* 释放未映射且不属于任何其他文件的空间
* 第四次修正空闲列表
* 空闲列表修正的反向映射更新

* 释放块映射 B 树所使用的空间
* 第五次修正空闲列表
* 空闲列表修正的反向映射更新

通常每个 AG 每个交易链中的空闲列表修正不需要超过一次，但在空间非常紧张的情况下理论上是可能的。
对于写时复制更新来说，情况更糟，因为必须先执行一次以从暂存区移除空间，然后再将其映射到文件中！

为了冷静地应对这种爆炸性增长，XFS 扩展了其延迟工作项的使用范围，覆盖大多数反向映射更新和所有引用计数更新。通过将工作分解为一系列小更新，这种方法减少了最坏情况下事务预留的大小，并增加了系统的最终一致性。
再次强调，这通常不是问题，因为 XFS 仔细安排其延迟工作项以避免各线程之间意外出现资源重用冲突。
然而，在线 fsck 改变了规则——请记住，尽管每 AG 结构的物理更新通过锁定 AG 头缓冲区来协调，但事务之间的缓冲锁会被释放。
一旦 scrub 获取资源并为数据结构加锁后，它必须在不释放锁的情况下完成所有验证工作。
如果空间 B 树的主要锁是一个 AG 头缓冲锁，那么 scrub 可能会中断另一个正在进行链条结束工作的线程。
例如，如果执行写时复制（copy-on-write）的线程已经完成了反向映射更新但尚未完成相应的引用计数（refcount）更新，那么两个AG B树对scrub操作来说将显得不一致，并且会记录一次损坏观察。这种观察是不正确的。
如果在这种状态下尝试修复，结果将是灾难性的！

在发现这一缺陷后，我们评估并拒绝了其他几种解决方案：

1. 为分配组（allocation groups）添加一个更高层次的锁，并要求写入线程按照AG顺序获取该锁，然后再进行任何更改。
这在实际中非常难以实现，因为很难确定需要获取哪些锁以及获取的顺序，除非模拟整个操作。
对文件操作进行预演以发现必要的锁会使文件系统变得非常慢。

2. 让延迟工作协调器代码了解连续的目标相同的AG意图项，并在事务回滚期间锁定AG头缓冲区。
这会给协调器引入大量复杂性，因为它与实际的延迟工作项之间只有松散的耦合关系。
此外，这种方法也无法解决问题，因为延迟工作项可以生成新的子任务，但在开始处理新的兄弟任务之前，所有子任务必须完成。

3. 教在线fsck遍历所有等待保护正在scrub的数据结构的锁，以查找待处理的操作。
检查和修复操作必须将这些待处理的操作纳入其评估之中。
这个解决方案不可行，因为它对主文件系统的侵入性极强。
.. _intent_drains:

意图排水机制
`````````````

在线文件系统检查（fsck）使用原子意图项计数器和锁循环来与事务链协调。
意图排水机制有两个关键特性：
首先，当一个延迟工作项被*排队*到一个事务时，计数器会增加，并且在相关意图完成日志项被*提交*到另一个事务后减少。
第二个特性是，可以在不持有AG头锁的情况下将延迟工作添加到事务中，但是每个AG的工作项在没有锁定该AG头缓冲区以记录物理更新和意图完成日志项的情况下不能被标记为已完成。
第一个特性使得扫描（scrub）能够优先让位于运行中的事务链，这是对在线fsck的显式降级，以利于文件操作。
第二个特性对于正确协调扫描至关重要，因为扫描总是能够判断是否可能发生冲突。
对于常规文件系统代码，排水机制如下工作：

1. 调用适当的子系统函数将一个延迟工作项添加到一个事务中。
2. 该函数调用`xfs_defer_drain_bump`来增加计数器。
3. 当延迟项管理器希望完成延迟工作项时，它调用`->finish_item`来完成它。
4. `->finish_item`的实现记录一些更改并调用`xfs_defer_drain_drop`来减少计数器并唤醒任何等待排水机制的线程。
5. 子事务提交，这会解锁与意图项关联的资源。

对于清理（scrub），排水（drain）的工作方式如下：

1. 锁定正在清理的元数据所关联的资源。
例如，扫描 refcount b树会锁定 AGI 和 AGF 头缓冲区。
2. 如果计数器为零（`xfs_defer_drain_busy` 返回 false），则没有正在进行的链，并且可以继续执行操作。
3. 否则，释放步骤 1 中获取的资源。
4. 等待意图计数器变为零（`xfs_defer_drain_intents`），除非捕获了信号，否则返回步骤 1。

为了避免在步骤 4 中进行轮询，排水机制提供了一个等待队列，以便在意图计数降至零时唤醒清理线程。

提出的补丁集是 `scrub intent drain 系列 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=scrub-drain-intents>`_。

.. _jump_labels:

静态键（又称跳转标签补丁）
`````````````````````````````````````

XFS 在线 fsck 尽可能地将常规文件系统与检查和修复代码分开。然而，在在线 fsck 的某些部分（如意图排水机制和稍后的实时更新钩子）中，让在线 fsck 代码了解文件系统其余部分的情况是有用的。
由于并不期望在线 fsck 会持续在后台运行，因此当在线 fsck 被编译到内核中但并未积极为用户空间运行时，尽量减少这些钩子带来的运行时开销非常重要。
在一个写入线程的热点路径中获取锁以访问一个数据结构，却发现无需进一步操作，这种做法代价很高——在作者的计算机上，每次访问的开销大约为 40-50 纳秒。
幸运的是，内核支持动态代码修补，这使得 XFS 可以在不运行在线 fsck 时用“nop”滑块替换静态分支到钩子代码。
这个滑块的开销仅限于指令解码器跳过滑块所需的时间，这似乎在小于 1 纳秒的范围内，并且不会访问除指令获取之外的内存。
当在线 fsck 启用静态密钥时，滑块会被无条件分支替换，以调用钩子代码。
切换代价相当高（约 22000 纳秒），但完全由调用在线 fsck 的程序承担。如果多个线程同时进入在线 fsck 或同时检查多个文件系统，则可以分摊这一代价。
更改分支方向需要获取 CPU 热插拔锁，并且由于 CPU 初始化需要内存分配，因此在线 fsck 必须小心不要在持有任何可能在内存回收路径中被访问的锁或资源时更改静态密钥。
为了尽量减少对 CPU 热插拔锁的竞争，应避免不必要的启用或禁用静态密钥。
因为静态密钥旨在在 xfs_scrub 未运行时最小化常规文件系统操作中的钩子开销，预期使用模式如下：

- XFS 中被挂钩的部分应声明一个默认为 false 的静态作用域静态密钥。
`DEFINE_STATIC_KEY_FALSE` 宏负责处理这一点。
静态密钥本身应声明为一个“static”变量。
- 在决定调用仅由scrub使用的代码时，常规文件系统应调用“static_branch_unlikely”谓词，以在静态密钥未启用时避免仅限scrub的钩子代码。
- 常规文件系统应导出辅助函数，这些函数调用“static_branch_inc”来启用静态密钥，并调用“static_branch_dec”来禁用静态密钥。
包装函数使得内核分发者可以在构建时关闭在线fsck，从而轻松编译掉相关代码。
- 希望启用仅限scrub的XFS功能的scrub函数应在设置函数中调用“xchk_fsgates_enable”来启用特定的钩子。
这必须在获取任何用于内存回收的资源之前完成。
调用者最好确保他们确实需要静态密钥所控制的功能；这里的“TRY_HARDER”标志很有用。
在线scrub有资源获取助手（例如“xchk_perag_lock”），用于处理所有scrubber函数锁定AGI和AGF缓冲区的工作。
如果它检测到scrub与运行中的事务之间存在冲突，它将尝试等待意图完成。
如果调用助手的函数没有启用静态密钥，则助手将返回-EDEADLOCK，这应该导致使用“TRY_HARDER”标志重新启动scrub。
擦除设置功能应检测该标志，启用静态密钥，并再次尝试擦除操作。
擦除清理功能会禁用所有通过 `xchk_fsgates_enable` 获得的静态密钥。
更多信息，请参阅内核文档中的 `Documentation/staging/static-keys.rst`。

.. _xfile:

可分页内核内存
----------------------

一些在线检查功能通过扫描文件系统来在内存中构建磁盘元数据结构的影子副本，并将两者进行比较。
为了使在线修复能够重建元数据结构，它必须先计算出将要存储在新结构中的记录集，然后才能将新结构持久化到磁盘上。
理想情况下，修复操作通过单个原子提交完成，引入一个新的数据结构。
为了实现这些目标，内核需要在一个不需要文件系统正确运行的地方收集大量信息。
内核内存不适合这种情况，因为：

* 在内存中分配一个连续区域以创建 C 数组非常困难，特别是在 32 位系统上。
* 记录的链表会引入双重指针开销，这非常高，并且消除了索引查找的可能性。
* 内核内存是固定的，这可能导致系统进入内存不足（OOM）状态。
* 系统可能没有足够的内存来暂存所有信息。
在任何给定时间，联机文件系统检查（online fsck）不需要将整个记录集保留在内存中，这意味着如果必要的话，可以将单个记录分页出去。
联机文件系统检查的持续开发表明，执行索引数据存储的能力也将非常有用。
幸运的是，Linux 内核已经具备一种字节寻址和可分页存储设施：tmpfs。
内核中的图形驱动程序（最显著的是 i915）利用 tmpfs 文件来存储不需要一直驻留在内存中的中间数据，因此这种使用先例已经确立。
因此，“xfile”应运而生！

+--------------------------------------------------------------------------+
| **历史侧记**：                                                            |
+--------------------------------------------------------------------------+
| 在线修复的第一版将记录插入到新构建的 B 树中，这导致失败，因为文件系统     |
| 可能会在构建数据结构时关闭，而这会在恢复完成后仍然处于活动状态。          |
|                                                                          |
| 第二版通过将所有内容存储在内存中解决了半重建结构的问题，但经常导致系统   |
| 缺乏内存。                                                               |
|                                                                          |
| 第三版通过使用链表解决了内存不足问题，但链表指针的内存开销极为严重。     |
+--------------------------------------------------------------------------+

xfile 访问模型
```````````````````

对 xfile 预期用途的调查提出了以下用例：

1. 固定大小记录的数组（空间管理 B 树、目录和扩展属性条目）

2. 稀疏固定大小记录的数组（配额和链接计数）

3. 可变大小的大二进制对象（BLOB）（目录和扩展属性名称及值）

4. 在内存中暂存 B 树（反向映射 B 树）

5. 任意内容（实时空间管理）

为了支持前四种用例，高级数据结构封装了 xfile，以便在线文件系统检查功能之间共享功能。
本节其余部分讨论了 xfile 向这五种高级数据结构中的四种提供的接口。
第五种用例在实时总结 (:ref:`实时总结 <rtsummary>`) 案例研究中讨论。
XFS 是非常基于记录的，这意味着加载和存储完整记录的能力非常重要。
为了支持这些用例，提供了一对 `xfile_load` 和 `xfile_store` 函数来读取和持久化对象到 xfile，并将任何错误视为内存不足错误。对于在线修复，以这种方式压制错误条件是可以接受的行为，因为唯一的反应是将操作回退到用户空间。
然而，任何关于文件访问模式的讨论如果不回答“那么mmap呢？”这个问题都是不完整的。
直接使用指针访问存储是很方便的，就像用户空间代码对常规内存所做的那样。

在线fsck（文件系统检查）不应导致系统出现内存不足（OOM）的情况，这意味着xfiles必须响应内存回收。
tmpfs只有在页面缓存folio既未被固定也未被锁定时，才能将其推送到交换缓存中，这意味着xfile不应固定太多folio。

对xfile内容的短期直接访问是通过锁定页面缓存folio并将其映射到内核地址空间来完成的。对象的加载和存储使用这种机制。由于folio锁不应该长时间持有，因此对xfile内容的长期直接访问是通过增加folio引用计数、将其映射到内核地址空间并释放folio锁来实现的。

这些长期使用者必须通过挂接到缩减器基础设施来响应内存回收，以知道何时释放folio。
提供了`xfile_get_folio`和`xfile_put_folio`函数来获取支持xfile部分的（已锁定的）folio并释放它。
唯一使用这些folio租用函数的代码是xfarray的:ref:`排序<xfarray_sort>`算法和:ref:`内存中的B树<xfbtree>`。

xfile 访问协调
```````````````````````

出于安全原因，xfiles必须由内核私有拥有。
它们被标记为`S_PRIVATE`以防止安全系统的干扰，永远不应映射到进程文件描述符表中，并且其页面也永远不应映射到用户空间进程中。
为了避免与VFS相关的锁定递归问题，所有对shmfs文件的访问都是通过直接操作页面缓存来完成的。
xfile写入器调用xfile地址空间中的`->write_begin`和`->write_end`函数来获取可写页面，将调用者的缓冲区复制到页面中，并释放这些页面。
xfile读取器调用`shmem_read_mapping_page_gfp`直接获取页面，然后将内容复制到调用者的缓冲区。

换句话说，xfiles忽略了VFS的读写路径，以避免创建一个虚拟的`struct kiocb`结构体以及避免获取inode和冻结锁。
tmpfs不能被冻结，而xfiles也不能暴露给用户空间。
如果一个xfile在多个线程之间共享用于修复阶段，则调用者必须提供自己的锁来进行访问协调。
例如，如果一个清理函数将扫描结果存储在一个xfile中，并需要其他线程提供对扫描数据的更新，则该清理函数必须为所有线程提供一个共享的锁。

.. _xfarray：

固定大小记录数组
```````````````````````````

在XFS中，每种类型的索引空间元数据（空闲空间、inode、引用计数、文件分支空间和反向映射）都由一组固定大小的记录组成，并通过经典的B+树进行索引。
目录有一组固定大小的dirent记录，指向名称，扩展属性有一组固定大小的属性键记录，指向名称和值。
配额计数器和文件链接计数器则使用数字来索引记录。
在修复过程中，清理函数需要在收集步骤期间暂存新的记录，并在B+树构建步骤期间检索这些记录。
尽管可以通过直接调用 xfile 的读写方法来满足这一需求，但对于调用者来说，有一个更高层次的抽象来处理数组偏移量计算、提供迭代函数，并处理稀疏记录和排序会更简单。
`xfarray` 抽象在基于字节访问的 xfile 上提供了一个固定大小记录的线性数组。

.. _xfarray_access_patterns:

数组访问模式
^^^^^^^^^^^^^^^^^^^^^

在线 fsck 中的数组访问模式通常可以分为三类。假设所有情况下都需要对记录进行迭代，这将在下一节中详细讨论。
第一类调用者处理的是按位置索引的记录。记录之间可能存在空隙，并且在收集步骤期间，一个记录可能被多次更新。
换句话说，这些调用者需要一个稀疏的线性寻址表文件。典型的使用场景包括配额记录或文件链接计数记录。
通过 `xfarray_load` 和 `xfarray_store` 函数以编程方式访问数组元素，这些函数包装了同名的 xfile 函数，以便在任意数组索引处加载和存储数组元素。
空隙被定义为 null 记录，而 null 记录被定义为全零字节序列。
空记录是通过调用 ``xfarray_element_is_null`` 检测到的。
它们的创建方式有两种：一种是通过调用 ``xfarray_unset`` 将现有记录置为空；另一种是从未向数组索引存储任何内容。
第二种类型的调用者处理的记录不是按位置索引的，并且不需要对记录进行多次更新。
这里的典型用例是重建空间B树和键/值B树。
这些调用者可以通过 ``xfarray_append`` 函数将记录添加到数组末尾，而不关心数组索引。
对于需要以特定顺序呈现记录的调用者（例如重建B树数据），可以使用 ``xfarray_sort`` 函数来排列已排序的记录；这个函数将在后面详细讨论。
第三种类型的调用者是一个袋子，用于计数记录。
这里的典型用例是从反向映射信息构建空间扩展引用计数。
记录可以以任意顺序放入袋子中，可以随时从袋子中移除记录，并且记录的唯一性由调用者负责。
``xfarray_store_anywhere`` 函数用于在包中的任意空记录槽中插入一条记录；而 ``xfarray_unset`` 函数则从包中移除一条记录。

提议的补丁集是
`大内存数组 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=big-array>`_

迭代数组元素
^^^^^^^^^^^^^^^^^^^^^^^^

大多数 xfarray 的使用者需要能够迭代数组中存储的记录。
调用者可以通过以下方式探测所有可能的数组索引：

.. code-block:: c

    xfarray_idx_t i;
    foreach_xfarray_idx(array, i) {
        xfarray_load(array, i, &rec);

        /* 对 rec 进行某些操作 */
    }

所有使用此惯用法的用户都必须准备好处理空记录，或者已经知道没有空记录。
对于希望迭代稀疏数组的 xfarray 用户，``xfarray_iter`` 函数会忽略那些从未写入数据的索引。它通过调用 ``xfile_seek_data``（内部使用 ``SEEK_DATA``）来跳过数组中未填充内存页的部分。一旦找到一页，它将跳过该页中的零值区域。

.. code-block:: c

    xfarray_idx_t i = XFARRAY_CURSOR_INIT;
    while ((ret = xfarray_iter(array, &i, &rec)) == 1) {
        /* 对 rec 进行某些操作 */
    }

.. _xfarray_sort:

排序数组元素
^^^^^^^^^^^^^^^^^^^^^^

在第四次在线修复演示期间，一位社区评审员指出，出于性能原因，在线修复应该批量加载记录到 B 树记录块中，而不是一次插入一条记录到新的 B 树中。XFS 中的 B 树插入代码负责保持记录的正确顺序，因此自然地，xfarray 也必须支持在批量加载之前对记录集合进行排序。

案例研究：排序 xfarrays
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

xfarray 中使用的排序算法实际上是自适应快速排序和堆排序子算法的组合，灵感来自 `Sedgewick <https://algs4.cs.princeton.edu/23quicksort/>`_ 和 `pdqsort <https://github.com/orlp/pdqsort>`_，并针对 Linux 内核进行了定制。为了在合理的时间内对记录进行排序，``xfarray`` 利用了快速排序提供的二进制子分区，但同时也使用堆排序以防所选的快速排序基准点不佳导致性能下降。
两种算法（通常）都是O(n * lg(n))，但两个实现之间的性能差距很大。
Linux 内核已经包含了一个相当快速的堆排序实现。
它只对普通的C数组进行操作，这限制了其用途范围。
`xfarray` 在两个关键的地方使用了它：

* 对由单个 xfile 页面支持的任何记录子集进行排序
* 从 `xfarray` 的潜在不同部分加载少量记录到内存缓冲区，并对该缓冲区进行排序

换句话说，`xfarray` 使用堆排序来限制快速排序的嵌套递归，从而缓解快速排序最糟糕的运行时行为。
选择快速排序的基准值是一件棘手的事情。
一个好的基准值会将待排序的集合分成两半，从而实现分而治之的行为，这对于O(n * lg(n))的性能至关重要。
一个差的基准值几乎不会分割子集，导致O(n²)的运行时间。
`xfarray` 的排序例程试图通过将九个记录采样到内存缓冲区，并使用内核堆排序来识别这九个记录的中位数，以避免选择一个差的基准值。
大多数现代快速排序实现采用 Tukey 的“九分位法”从经典的 C 数组中选择一个基准值。
典型的九分位法实现会选取三个唯一的三元记录组，对每个三元组进行排序，然后对每个三元组的中间值进行排序以确定九分位值。
然而，如前所述，xfile 访问并非完全廉价。
事实证明，将这九个元素读入内存缓冲区，运行内核中的堆排序算法，并选择缓冲区中的第四个元素作为基准值更为高效。
Tukey 的九分位法在 J. W. Tukey 的论文《The ninther, a technique for low-effort robust (resistant) location in large samples》（《在大样本中低效但稳健（抗干扰）定位的技术》）中有描述，该论文收录于 H. David 编辑的 *Contributions to Survey Sampling and Applied Statistics* （《抽样调查与应用统计学的贡献》）（Academic Press, 1978），第 251-257 页。

快速排序的分区过程相当标准——围绕基准值重新排列记录子集，然后设置当前和下一个栈帧来分别对基准值较大的一半和较小的一半进行排序。
这样可以将栈空间需求保持在 log2(记录数) 的水平。
作为一种最终的性能优化手段，快速排序的高（hi）和低（lo）扫描阶段尽可能长时间地将已检查的 xfile 页面映射在内核中，以减少映射/取消映射周期。
令人惊讶的是，在直接在 xfile 页面上应用堆排序之后，这样做能够进一步减少整体排序时间近一半。
.. _xfblob:

Blob 存储
```````````

扩展属性和目录为暂存记录增加了额外的要求：有限长度的任意字节序列。
每个目录项记录需要存储项名，每个扩展属性需要存储属性名和值。
这些名称、键和值可能会占用大量内存，因此创建了 `xfblob` 抽象来简化在 xfile 基础上的这些数据块的管理。
Blob 数组提供了 `xfblob_load` 和 `xfblob_store` 函数来检索和持久化对象。
存储函数为每个持久化的对象返回一个魔法标识符（cookie）。
稍后，调用者可以使用此标识符通过 `xblob_load` 函数来召回对象。
`xfblob_free` 函数释放特定的数据块，而 `xfblob_truncate` 函数则释放所有数据块，因为不需要进行压缩。
关于修复目录和扩展属性的详细信息将在后续关于原子文件内容交换的部分中讨论。
然而，需要注意的是，这些修复函数仅使用 Blob 存储来缓存少量条目，然后再将它们添加到临时磁盘文件中，这就是为什么不需要压缩的原因。
提议的补丁集位于
`扩展属性修复 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=repair-xattrs>`_ 系列的开始。

.. _xfbtree:

内存中的 B+ 树
`````````````````

关于 :ref:`二级元数据<secondary_metadata>` 的章节提到，检查和修复二级元数据通常需要在文件系统的实时元数据扫描与更新该元数据的写入线程之间进行协调。
保持扫描数据的更新需要能够将文件系统中的元数据更新传播到扫描收集的数据中。这可以通过将并发更新追加到一个单独的日志文件并在写入新元数据之前应用这些更新来实现，但这会导致系统其他部分非常繁忙时内存消耗无界增长。

另一种选择是跳过旁路日志，并直接将文件系统的实时更新提交到扫描数据中，这种方式以更高的开销换取了更低的最大内存需求。

在这两种情况下，存储扫描结果的数据结构必须支持索引访问以保证性能。

鉴于索引查找扫描数据对于这两种策略都是必需的，在线 fsck 采用了第二种策略，即直接将实时更新提交到扫描数据中。

由于 xfarrays 不支持索引且不强制记录顺序，因此不适合用于此任务。

然而，XFS 有一个库可以创建和维护有序的反向映射记录：现有的 rmap btree 代码！

如果能够在内存中创建这样一个结构就好了。

回想一下，:ref:`xfile <xfile>` 抽象表示将内存页面作为常规文件，这意味着内核可以随意创建字节或块可寻址的虚拟地址空间。

XFS 缓冲缓存专门用于抽象块导向的地址空间 I/O，这意味着将缓冲缓存适配为与 xfiles 接口交互即可重用整个 btree 库。

基于 xfile 构建的 B树统称为“xfbtrees”。
以下几节描述了它们实际的工作原理。

所提议的补丁集是 `内存中的B树 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=in-memory-btrees>`_ 系列。

将 xfiles 用作缓冲区缓存目标
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

为了支持将 xfiles 用作缓冲区缓存目标，需要进行两项修改：
1. 首先，需要使 `struct xfs_buftarg` 结构能够承载 `struct xfs_buf` 的 rhashtable，因为通常这些结构是由每个 AG（Allocation Group）结构持有的。
2. 第二项修改是修改缓冲区的 `ioapply` 函数，使其能够从 xfile 中“读取”缓存页，并将缓存页“写回”到 xfile。

对单个缓冲区的多次访问由 `xfs_buf` 锁控制，因为 xfile 本身不提供任何锁定机制。有了这种适应后，使用 xfile 支持的缓冲区缓存的用户可以使用与使用磁盘支持的缓冲区缓存相同的 API。xfile 和缓冲区缓存之间的分离意味着更高的内存使用，因为它们不共享页面，但这一特性将来可能会使内存中的 B 树支持事务性更新成为可能。然而，目前它只是消除了对新代码的需求。

使用 xfbtree 进行空间管理
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

对于 xfile 的空间管理非常简单——每个 B 树块的大小为一个内存页。
这些块使用与磁盘上的 btree 相同的头部格式，但在内存中的块验证器会忽略校验和，假设 xfile 内存不会比普通的 DRAM 更容易出现损坏。
在这里重用现有代码比绝对的内存效率更为重要。
一个支持 xfbtree 的 xfile 的第一个块包含一个头部块。
该头部描述了所有者、高度以及根 xfbtree 块的块号。
要分配一个 btree 块，请使用 `xfile_seek_data` 查找文件中的空隙。
如果没有空隙，则通过扩展 xfile 的长度来创建一个空隙。
预先使用 `xfile_prealloc` 为块分配空间，并返回该位置。
要释放一个 xfbtree 块，请使用 `xfile_discard`（其内部使用 `FALLOC_FL_PUNCH_HOLE`）从 xfile 中移除该内存页。
填充 xfbtree
^^^^^^^^^^^^^^^^^^^^^

想要创建 xfbtree 的在线 fsck 函数应按照以下步骤进行：

1. 调用 `xfile_create` 创建一个 xfile。
2. 调用 `xfs_alloc_memory_buftarg` 创建一个指向 xfile 的缓冲区缓存目标结构。
3. 将缓冲区缓存目标、缓冲区操作及其他信息传递给 `xfbtree_init` 以初始化传入的 `struct xfbtree` 并向 xfile 写入初始根块。
每种 B 树类型应定义一个包装器，将必要的参数传递给创建函数。
例如，rmap B 树定义了 `xfs_rmapbt_mem_create` 来处理调用者所需的所有细节。
4. 将 xfbtree 对象传递给特定 B 树类型的 B 树游标创建函数。
沿用上述示例，`xfs_rmapbt_mem_cursor` 负责为调用者处理这一过程。
5. 将 B 树游标传递给常规 B 树函数以进行查询和更新内存中的 B 树。
例如，一个用于 rmap xfbtree 的 B 树游标可以像其他任何 B 树游标一样传递给 `xfs_rmap_*` 函数。
有关如何处理记录到事务中的 xfbtree 更新的信息，请参阅 :ref:`下一节 <xfbtree_commit>`。
6. 完成后，删除 B 树游标，销毁 xfbtree 对象，释放缓冲区目标，并销毁 xfile 以释放所有资源。

.. _xfbtree_commit:

记录的 xfbtree 缓冲区提交
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

尽管重用 rmap B 树代码来处理暂存结构是一个巧妙的方法，但内存中 B 树块存储的临时性带来了一些挑战。
XFS 事务管理器不应提交由 xfile 支持的缓冲区日志项，因为日志格式不理解数据设备之外的设备更新。

临时的 xfbtree 在 AIL 将日志事务回写到文件系统时可能已经不存在，并且在日志恢复期间肯定不会存在。

由于这些原因，任何在事务上下文中更新 xfbtree 的代码必须将缓冲区日志项从事务中移除，并在提交或取消事务之前将更新写入支持的 xfile 中。

``xfbtree_trans_commit`` 和 ``xfbtree_trans_cancel`` 函数按照以下步骤实现这一功能：

1. 找出每个其缓冲区目标为 xfile 的缓冲区日志项。
2. 记录该日志项的脏/已排序状态。
3. 将该日志项从缓冲区中分离。
4. 将缓冲区加入一个特殊的 delwri 列表。
5. 如果唯一脏的日志项是第 3 步中分离出来的那些，则清除事务脏标志。
6. 提交 delwri 列表以将更改提交到 xfile（如果正在进行更新）。

通过这种方式从事务中移除 xfile 日志缓冲区后，可以提交或取消事务。
批量加载磁盘上的 B+树
------------------------------

如前所述，早期的在线修复迭代通过创建一个新的 B 树并逐个添加观察记录来构建新的 B 树结构。逐条加载 B 树记录虽然不需要在提交前对内核中的记录进行排序，具有一定的优势，但非常慢，并且如果系统在修复过程中崩溃，会导致块泄露。逐条加载记录也意味着修复过程无法控制新 B 树中块的加载因子。

幸运的是，久经考验的 `xfs_repair` 工具提供了一种更高效的从记录集合重建 B 树索引的方法——批量 B 树加载。尽管从代码实现上来看，这种方式效率不高，因为 `xfs_repair` 为每种 B 树类型都有单独复制粘贴的实现。为了准备在线 fsck（文件系统检查和修复），研究了四种批量加载器，并做了详细的笔记，将这四个加载器重构为一个通用的 B 树批量加载机制。这些笔记经过更新后如下所示：

### 几何计算
```````````````````

批量加载的第一步是组装将在新 B 树中存储的所有记录，并对这些记录进行排序。接下来，调用 `xfs_btree_bload_compute_geometry` 来根据记录集、B 树类型以及任何负载因子偏好来计算 B 树的形状。这些信息对于资源预留是必需的。
首先，几何计算根据B树块的大小和块头的大小来计算能够放入叶块中的最小和最大记录数。大致来说，最大记录数为：

        maxrecs = (block_size - header_size) / record_size

XFS设计规定，在可能的情况下应合并B树块，这意味着最小记录数是maxrecs的一半：

        minrecs = maxrecs / 2

接下来需要确定的是期望的加载因子。这个值必须至少为minrecs且不超过maxrecs。选择minrecs是不理想的，因为它浪费了半个块；选择maxrecs也是不理想的，因为向每个新重建的叶块添加一条记录会导致树分裂，从而导致性能立即显著下降。因此，默认的加载因子被设定为maxrecs的75%，这样可以提供一个相对紧凑的结构而不会有任何立即的分裂惩罚：

        default_load_factor = (maxrecs + minrecs) / 2

如果空间紧张，加载因子将设置为maxrecs，以尽量避免空间不足：

        leaf_load_factor = 空间充足？default_load_factor : maxrecs

对于B树节点块，加载因子使用B树键和指针的组合大小作为记录大小进行计算：

        maxrecs = (block_size - header_size) / (key_size + ptr_size)
        minrecs = maxrecs / 2
        node_load_factor = 空间充足？default_load_factor : maxrecs

一旦完成这些计算，就可以计算存储记录集所需的叶块数量：

        leaf_blocks = ceil(record_count / leaf_load_factor)

计算指向树下一层所需节点块的数量：

        n_blocks = (n == 0 ? leaf_blocks : node_blocks[n])
        node_blocks[n + 1] = ceil(n_blocks / node_load_factor)

整个计算过程递归执行，直到当前层只需要一个块为止。最终的几何结构如下：

- 对于AG根B树，这一层是根层，因此新树的高度为``level + 1``，所需的空间是每一层块数量的总和。
- 对于inode根B树，如果顶级记录无法放入inode叉区，则高度为``level + 2``，所需的空间是每一层块数量的总和，并且inode叉区指向根块。
- 对于inode根B树，如果顶级记录可以存储在inode叉区，则根块可以直接存储在inode中，高度为``level + 1``，所需的空间比每一层块数量总和少一个。

这只有在非bmap B树获得在inode中生根的能力时才变得相关，这是未来的补丁集的一部分，这里仅为了完整性而包含。
.. _newbt:

预留新的 B+树 块
````````````````````

一旦修复程序知道了新 B+树所需块的数量，它就会使用空闲空间信息来分配这些块。
每个预留的范围（extent）都由 B+树构建器状态数据单独跟踪。
为了提高崩溃恢复能力，预留代码还在每次空间分配的同一事务中记录一个范围释放意图（EFI）项，并将内存中的 `struct xfs_extent_free_item` 对象附加到空间预留上。
如果系统崩溃，日志恢复会使用未完成的 EFI 项来释放未使用的空间，保持文件系统的不变。
每当 B+树构建器从预留的范围内为 B+树申明一个块时，它会更新内存中的预留信息以反映已申明的空间。
块预留尝试尽可能分配连续的空间，以减少活跃的 EFI 数量。
在修复程序写入这些新的 B+树块时，为预留空间创建的 EFI 项会固定住磁盘上的日志尾部。
其他部分系统可能会持续忙碌并推动日志头部向固定的尾部靠近。
为了避免文件系统活锁，EFI 项不能长时间固定日志尾部。
为了解决这个问题，这里重用了延迟操作机制中的动态重新日志功能，在日志头部提交一个事务，该事务包含旧 EFI 的 EFD 和新 EFI。
这使得日志能够释放旧的EFI以保持日志向前移动。
EFIs在提交和回收阶段中起着重要作用；请参阅下一节及关于 :ref:`回收<reaping>` 的部分以获取更多详细信息。
提议的补丁集包括 `位图重构 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=repair-bitmap-rework>`_ 和 `为批量加载B树做准备 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=repair-prep-for-bulk-loading>`_。

写入新树
````````````````````

这一部分相对简单——B树构建器（`xfs_btree_bulkload`）从预留列表中申请一个块，写入新的B树块头，填充剩余部分的数据记录，并将新的叶节点块添加到已写入块的列表中：

```
  ┌────┐
  │叶节点│
  │RRR │
  └────┘
```

每当向层级中添加一个新的块时，都会设置兄弟指针：

```
  ┌────┐ ┌────┐ ┌────┐ ┌────┐
  │叶节点│→│叶节点│→│叶节点│→│叶节点│
  │RRR │←│RRR │←│RRR │←│RRR │
  └────┘ └────┘ └────┘ └────┘
```

当它完成记录叶节点块的写入后，接着处理节点块。
为了填充一个节点块，它遍历树中下一层的每个块来计算相关键并将它们写入父节点：

```
      ┌────┐       ┌────┐
      │节点│──────→│节点│
      │PP  │←──────│PP  │
      └────┘       └────┘
      ↙   ↘         ↙   ↘
  ┌────┐ ┌────┐ ┌────┐ ┌────┐
  │叶节点│→│叶节点│→│叶节点│→│叶节点│
  │RRR │←│RRR │←│RRR │←│RRR │
  └────┘ └────┘ └────┘ └────┘
```

当到达根级别时，就可以提交新的B树了：

```
          ┌─────────┐
          │  根     │
          │   PP    │
          └─────────┘
          ↙         ↘
      ┌────┐       ┌────┐
      │节点│──────→│节点│
      │PP  │←──────│PP  │
      └────┘       └────┘
      ↙   ↘         ↙   ↘
  ┌────┐ ┌────┐ ┌────┐ ┌────┐
  │叶节点│→│叶节点│→│叶节点│→│叶节点│
  │RRR │←│RRR │←│RRR │←│RRR │
  └────┘ └────┘ └────┘ └────┘
```

提交新B树的第一步是同步地将B树块持久化到磁盘。这稍微有些复杂，因为新的B树块可能在过去不久被释放过，因此构建器必须使用 `xfs_buf_delwri_queue_here` 来从AIL列表中移除（陈旧的）缓冲区，然后才能将新的块写入磁盘。块通过delwri列表排队并使用 `xfs_buf_delwri_submit` 在一次大的批次中写入。一旦新的块被持久化到磁盘，控制权返回到调用批量加载器的个别修复函数。修复函数必须在事务中记录新根的位置、清理为新B树预留的空间，并回收旧的元数据块：

1. 提交新B树根的位置。
2. 对于每个内核预留空间：
   
   a. 记录所有由B树构建器消耗的空间的Extent Freeing Done（EFD）项。新的EFD必须指向附加在预留空间上的EFI，以防止日志恢复释放新的块。
   b. 对于未被使用的内核预留空间的部分，创建一个普通的延迟范围释放工作项，以便稍后在事务链中释放未使用的空间。
c. 在步骤2a和2b中记录的EFDs和EFIs不应超出提交事务的预留空间。
如果B树加载代码怀疑这种情况即将发生，必须调用`xrep_defer_finish`来清除延迟的工作并获取一个新的事务。

3. 第二次清除延迟的工作以完成提交并清理修复事务。
在步骤2c和3中的事务回滚代表了修复算法中的一个弱点，因为在收割步骤结束前的日志刷新和崩溃可能导致空间泄漏。
在线修复函数通过使用非常大的事务来最小化这种情况的发生，每个事务可以容纳数千个块释放指令。
修复继续进行到旧块的收割，这将在几个批量加载案例研究之后的一个后续章节中详细说明：:ref:`section<reaping>`。

重建inode索引B树的高级过程如下：

1. 遍历反向映射记录，从inode块信息生成`struct xfs_inobt_rec`记录，并生成旧inode B树块的位图。
2. 按inode顺序将记录附加到一个xfarray中。
3. 使用`xfs_btree_bload_compute_geometry`函数计算inode B树所需的块数。
如果启用了空闲空间inode B树，则再次调用该函数来估算finobt的几何结构。
4. 分配在前一步计算出的块数量
5. 使用 `xfs_btree_bload` 将 xfarray 记录写入 B 树块，并生成内部节点块。如果启用了空闲空间 inode B 树，则再次调用它以加载 finobt
6. 将新 B 树根块的位置提交到 AGI（分配组信息）
7. 使用在第一步中创建的位图回收旧的 B 树块

详细内容如下：
inode B 树将 inumber 映射到相关 inode 记录的磁盘位置，这意味着可以从反向映射信息重建 inode B 树。
具有 `XFS_RMAP_OWN_INOBT` 所有者的反向映射记录标记了旧 inode B 树块的位置。
每个具有 `XFS_RMAP_OWN_INODES` 所有者的反向映射记录至少标记了一个 inode 集群缓冲区的位置。
一个集群是最小数量的磁盘上的 inode，可以在单个交易中分配或释放；它永远不会小于 1 个文件系统块或 4 个 inode。
对于每个inode簇所代表的空间，确保在空闲空间B树中没有记录，在引用计数B树中也没有记录。如果有，则空间元数据不一致足以终止该操作。
否则，读取每个簇缓冲区以检查其内容是否为磁盘上的inode，并判断文件是已分配（`xfs_dinode.i_mode != 0`）还是未分配（`xfs_dinode.i_mode == 0`）。
累积连续的inode簇缓冲区读取结果，直到有足够的信息填充一个inode块记录，该记录包含在inode键空间中的64个连续数字。
如果块是稀疏的，块记录可能包含空洞。
一旦修复函数累积了一个块的数据，它会调用`xfarray_append`将inode B树记录添加到xfarray中。
在创建B树步骤期间，此xfarray会被遍历两次——一次用于填充所有inode块记录的inode B树，另一次用于填充包含未分配非稀疏inode的块记录的空闲inode B树。
inode B树的记录数量等于xfarray记录的数量，但空闲inode B树的记录数量需要在inode块记录存储到xfarray时进行计算。
提议的补丁集是`AG B树修复 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=repair-ag-btrees>`_ 系列。

案例研究：重建空间引用计数

反向映射记录用于重建引用计数信息。
参考计数对于共享文件数据的写时复制操作是必需的。
可以将反向映射条目想象成矩形，代表物理块的范围，并且这些矩形可以放置在一起，允许它们相互重叠。
从下面的图中可以看出，参考计数记录必须在堆栈高度发生变化的地方开始或结束。
换句话说，记录生成的触发条件是基于水平变化的：

                        █    ███
              ██      █████ ████   ███        ██████
        ██   ████     ███████████ ████     █████████
        ████████████████████████████████ ███████████
        ^ ^  ^^ ^^    ^ ^^ ^^^  ^^^^  ^ ^^ ^  ^     ^
        2 1  23 21    3 43 234  2123  1 01 2  3     0

磁盘上的参考计数B树不存储参考计数为0的情况，因为自由空间B树已经记录了哪些块是空闲的。
用于执行写时复制操作的范围应该是唯一具有参考计数为1的记录。
单所有者的文件块不会记录在自由空间B树或参考计数B树中。
重建参考计数B树的高级过程如下：

1. 遍历反向映射记录以生成 `struct xfs_refcount_irec` 记录，对于任何具有多个反向映射的空间，将其添加到xfarray中。
   所有由 `XFS_RMAP_OWN_COW` 拥有的记录也会被添加到xfarray中，因为这些是分配来执行写时复制操作的范围，并且在参考计数B树中进行跟踪。
   使用由 `XFS_RMAP_OWN_REFC` 拥有的记录创建旧参考计数B树块的位图。
2. 按物理范围顺序对记录进行排序，并将写时复制操作的范围放在xfarray的末尾。
这与 refcount B+树中记录的排序顺序相匹配。
3. 使用 `xfs_btree_bload_compute_geometry` 函数计算新树所需块的数量。
4. 分配上一步计算出的块数量。
5. 使用 `xfs_btree_bload` 将 xfarray 记录写入 B+树块并生成内部节点块。
6. 将新的 B+树根块位置提交到 AGF（分配组自由块位图）。
7. 使用第一步创建的位图回收旧的 B+树块。

详细步骤如下；`xfs_repair` 使用相同的算法从反向映射记录生成 refcount 信息：
- 直到反向映射 B+树中的记录用尽：

  - 从 B+树中获取下一条记录并放入一个袋子中。
- 收集所有具有相同起始块的记录，并将它们放入袋子中。
- 当袋子不为空时：

    - 在袋子中的映射中，计算参考计数发生变化的最低块号。
此位置将是下一个未处理的反向映射的起始块号，或者是包中最短映射之后的下一个块。
- 从包中移除所有结束于此位置的映射。
- 从B树中收集所有开始于此位置的反向映射，并将它们放入包中。
- 如果包的大小发生了变化且大于一，则创建一个新的引用计数记录，将我们刚刚遍历的块号范围与包的大小关联起来。

这里的包状结构是一个类型2的xfarray，如在“xfarray访问模式”(:ref:`xfarray_access_patterns`)部分所讨论的那样。
反向映射通过``xfarray_store_anywhere``添加到包中，并通过``xfarray_unset``移除。
通过``xfarray_iter``循环检查包成员。

提议的补丁集是
`AG B树修复
<https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=repair-ag-btrees>`_ 系列。

案例研究：重建文件分支映射索引
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

重建数据/属性分支映射B树的高层次过程如下：

1. 遍历反向映射记录，为该inode和分支生成 `struct xfs_bmbt_rec` 记录。
将这些记录追加到一个xfarray中。
计算旧的bmap btree块的位图，从``BMBT_BLOCK``记录中获取信息
2. 使用``xfs_btree_bload_compute_geometry``函数来计算新树所需块的数量
3. 按文件偏移顺序对记录进行排序
4. 如果范围记录可以放入inode叉的即时区域中，则将这些记录提交到该即时区域并跳到步骤8
5. 分配上一步计算所需的块数量
6. 使用``xfs_btree_bload``将xfarray记录写入btree块并生成内部节点块
7. 将新的btree根块提交到inode叉的即时区域
8. 使用第一步创建的位图回收旧的btree块

这里有一些复杂性：
首先，如果数据和属性叉不都是BMBT格式，可以通过调整叉偏移来调整即时区域的大小
其次，如果叉映射的数量足够少，可能可以使用EXTENTS格式而不是BMBT格式，这可能需要进行转换
第三，必须谨慎地重新加载内核范围（incore extent）映射，以避免干扰任何延迟分配的范围（extent）。

所提议的补丁集是 `文件映射修复 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=repair-file-mappings>`_ 系列。

.. _reaping:

回收旧元数据块
----------------

每当在线文件系统检查（online fsck）构建一个新的数据结构来替换有问题的数据结构时，如何找到并处理属于旧结构的块就成了一个问题。最懒惰的方法当然是完全不处理它们，但这会逐渐导致空间泄漏，从而引发服务质量下降。希望有人会安排重建空闲空间信息来堵住这些泄漏点。

离线修复在记录了决定不清除的文件和目录使用情况后，会重建所有空间元数据，因此可以在发现的空闲空间中构建新结构，并且避免了回收问题。作为修复的一部分，在线文件系统检查（online fsck）严重依赖反向映射记录（reverse mapping records），以找到由相应 rmap 所有者拥有但实际上为空闲的空间。交叉引用 rmap 记录与其他 rmap 记录是必要的，因为可能还有其他数据结构也认为它们拥有其中的一些块（例如，交叉链接树）。允许块分配器再次分配这些块不会使系统趋向一致性。

对于空间元数据，寻找要处理的范围的过程通常遵循以下格式：

1. 创建一个位图，表示必须保留的数据结构所使用的空间。
用于创建新元数据的空间预留可以在这里使用，前提是使用相同的rmap所有者代码来标识所有正在重建的对象。

1. 调查反向映射数据，以创建一个位图，该位图表示正在保留的元数据所拥有的空间，并且这些元数据具有相同的 `XFS_RMAP_OWN_*` 编号。
2. 使用位图的差集运算符从 (1) 中减去 (2)。
剩余的置位位代表了可能可以释放的候选区段。
处理过程进入下一步骤 4。

对于基于文件的元数据（如扩展属性、目录、符号链接、配额文件和实时位图）的修复，通过构建一个新的结构并将其附加到一个临时文件上，然后交换文件分支中的所有映射来完成。
之后，旧文件分支中的映射是待处置的候选块。
处置旧区段的过程如下：

4. 对于每个候选区段，统计该区段中第一个块的反向映射记录数量，这些记录不属于正在修复的数据结构的所有者。
- 如果为零，则该块只有一个所有者，可以被释放。
- 如果不为零，则该块属于一个交叉链接的结构，不能被释放。
5. 从extent中的下一个block开始，确定还有多少个block具有与第一个block相同的零/非零其他所有者状态。
6. 如果该区域是交叉链接的，则删除正在修复的结构的反向映射条目，并继续处理下一个区域。
7. 如果要释放该区域，则标记缓冲区缓存中相应的任何缓冲区为陈旧，以防止日志回写。
8. 释放该区域并继续。

然而，这一过程有一个复杂之处：
事务具有有限的大小，因此回收过程必须小心地提交事务以避免溢出。溢出有两个来源：

a. 代表不再占用的空间记录的日志项（EFIs）。

b. 缓冲区失效的日志项。

这也是一个在回收过程中发生崩溃时可能会导致block泄漏的时间窗口。如前所述，在线修复功能使用非常大的事务来尽量减少这种情况的发生。所提议的补丁集是为了准备批量加载b树的系列工作：`准备批量加载b树 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=repair-prep-for-bulk-loading>`_。

案例研究：常规B树修复后的回收
```````````````````````````````````````````````

旧引用计数和inode b树是最容易回收的，因为它们有带有特殊所有者代码的rmap记录：引用计数b树的代码是 `XFS_RMAP_OWN_REFC`，inode和空闲inode b树的代码是 `XFS_RMAP_OWN_INOBT`。
创建一个范围列表以回收旧 B 树块在概念上非常简单：

1. 锁定相关的 AGI/AGF 头缓冲区，以防止分配和释放。
2. 对于每个具有对应于正在重建的元数据结构的 rmap 所有者的反向映射记录，在位图中设置相应的范围。
3. 遍历当前具有相同 rmap 所有者的数据结构。对于访问到的每个块，在上述位图中清除该范围。
4. 位图中每个设置的位代表一个可能是来自旧数据结构的块，因此是回收的候选块。换句话说，``(rmap_records_owned_by & ~blocks_reachable_by_walk)`` 是可能可以释放的块。
如果可以在整个修复过程中保持 AGF 锁（这是常见情况），则步骤 2 可以与创建新 B 树记录的反向映射记录遍历同时进行。

案例研究：重建空闲空间索引

`````````````````````````````````````````````
重建空闲空间索引的高级过程如下：

1. 遍历反向映射记录，从反向映射 B 树中的间隙生成 ``struct xfs_alloc_rec_incore`` 记录。
2. 将这些记录附加到一个 xfarray 中。
3. 使用 ``xfs_btree_bload_compute_geometry`` 函数计算每个新树所需块的数量。
`````````````````````````````````````````````
4. 根据上一步计算出的块数，从收集到的空闲空间信息中分配这些块。
5. 使用 `xfs_btree_bload` 将 xfarray 记录写入 btree 块，并生成按长度索引的空闲空间内部节点块。
再次调用它以按块号索引的空闲空间进行处理。
6. 将新 btree 根块的位置提交到 AGF（分配组脚本表）。
7. 通过查找未被逆向映射 btree、新的空闲空间 btree 或 AGFL（分配组自由列表）记录的空间来回收旧的 btree 块。

修复空闲空间 btree 比常规 btree 修复有三个关键复杂点：

首先，空闲空间在逆向映射记录中并未显式跟踪。因此，必须从逆向映射 btree 的键空间的物理空间部分中的间隙推断出新的空闲空间记录。
其次，空闲空间修复不能使用通用的 btree 预留代码，因为新块是从空闲空间 btree 中预留的。
当修复空闲空间 btree 本身时这是不可能的。然而，在整个空闲空间索引重建过程中，修复操作持有 AGF 缓冲锁，因此可以使用收集到的空闲空间信息为新的空闲空间 btree 提供块。
没有必要为每个预留的extent都分配EFI，因为新的空闲空间btree是在磁盘文件系统认为未被占用的空间中构建的。
然而，如果从收集到的空闲空间信息中为新btree预留块改变了空闲空间记录的数量，则修复过程必须根据新的记录数重新估算新的空闲空间btree的几何结构，直到预留空间足够为止。
作为提交新btree的一部分，修复过程必须确保为预留的块创建反向映射，并将未使用的预留块插入到空闲空间btree中。
使用延迟反向映射（rmap）和释放操作来确保这一转换是原子性的，类似于其他btree修复功能。
第三，修复后找到要回收的块并不是特别直接。
空闲空间btree和反向映射btree所需的块由AGFL提供。
放入AGFL的块具有所有者为`XFS_RMAP_OWN_AG`的反向映射记录。
当这些块从AGFL移动到空闲空间btree或反向映射btree时，该所有权会被保留。
当修复过程遍历反向映射记录以合成空闲空间记录时，它会创建一个位图（`ag_owner_bitmap`），表示所有由`XFS_RMAP_OWN_AG`记录所声称的空间。
修复上下文还维护了一个第二位图，对应于rmap btree块和AGFL块（`rmap_agfl_bitmap`）。
当遍历完成后，位图分离操作 ``(ag_owner_bitmap & ~rmap_agfl_bitmap)`` 计算出旧的空闲空间 B+ 树所使用的范围。这些块可以使用上述方法进行回收。

提议的补丁集是 `AG B+ 树修复 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=repair-ag-btrees>`_ 系列。

.. _rmap_reap:

案例研究：修复后回收反向映射 B+ 树
`````````````````````````````````````````````````````````

修复后的旧反向映射 B+ 树更容易被回收。如前一节所述，AGFL 上的块、两个空闲空间 B+ 树块和反向映射 B+ 树块都有以 `XFS_RMAP_OWN_AG` 作为所有者的反向映射记录。收集反向映射记录并构建新的 B+ 树的完整过程在关于 :ref:`实时重建反向映射数据 <rmap_repair>` 的案例研究中有详细描述，但其中的一个关键点是新的反向映射 B+ 树将不会包含任何关于旧反向映射 B+ 树的记录，并且旧的 B+ 树块也不会在空闲空间 B+ 树中被跟踪。候选回收块列表通过设置新反向映射 B+ 树记录中的间隙对应位，然后清除空闲空间 B+ 树和当前 AGFL 块对应的位来计算得出。结果 `（new_rmapbt_gaps & ~(agfl | bnobt_records)）` 使用上述方法进行回收。反向映射 B+ 树重建过程的其余部分在另一个 :ref:`案例研究 <rmap_repair>` 中讨论。提议的补丁集是 `AG B+ 树修复 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=repair-ag-btrees>`_ 系列。
案例研究：重建 AGFL

分配组空闲块列表（AGFL）的修复步骤如下：

1. 为反向映射数据声称的所有空间创建一个位图，这些空间属于 `XFS_RMAP_OWN_AG`
2. 减去两个空闲空间 B+树 和 rmap B+树 所占用的空间
3. 减去反向映射数据声称属于任何其他所有者的空间，以避免将交叉链接的块重新添加到 AGFL 中
4. 当 AGFL 被填满后，回收剩余的块
5. 下一步修复空闲列表的操作将会调整列表的大小

更多详细信息，请参阅 `fs/xfs/scrub/agheader_repair.c <https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/fs/xfs/scrub/agheader_repair.c>`_

节点记录修复
------------

节点记录必须谨慎处理，因为它们既有磁盘上的记录（“dinodes”），也有内存中的（“缓存”）表示形式。如果在线检查文件系统（online fsck）不注意仅在磁盘元数据严重损坏时访问磁盘元数据，那么很容易出现缓存一致性问题。

当在线检查文件系统想要打开一个损坏的文件进行扫描时，它必须使用专门的资源获取函数，这些函数会返回内存中的表示形式 *或* 对所需对象的锁定，以防止对磁盘位置的任何更新。

对磁盘上的节点缓冲区所做的唯一修复应该是确保能够加载内核结构所必需的部分。
这意味着修复由inode簇缓冲区和inode fork验证器捕获的所有问题，并重试`iget`操作。
如果第二次`iget`失败，则修复失败。
一旦内存中的表示加载完成，修复程序可以锁定inode，并对其进行全面检查、修复和优化。
大多数inode属性很容易检查和约束，或者是由用户控制的任意位模式；这些都很容易修复。
处理数据和attr fork的扩展计数以及文件块计数更为复杂，因为计算正确的值需要遍历fork，如果遍历失败，则需要保留字段无效并等待fork fsck函数运行。
提议的补丁集是
[inode](https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=repair-inodes)
修复系列。

### 配额记录修复

与inode类似，配额记录（"dquot"）也有磁盘上的记录和内存中的表示，因此同样存在缓存一致性问题。
在XFS代码库中，两者都称为dquot，这可能会造成一些混淆。
对磁盘上的配额记录缓冲区进行的唯一修复应该是使内核结构能够加载。
一旦内存中的表示加载完成，唯一需要检查的属性显然是有问题的限制和定时器值。
### 配额使用计数器的检查、修复和讨论

配额使用计数器的检查、修复和讨论在关于 :ref:`实时配额检查 <quotacheck>` 的章节中有详细介绍。

提议的补丁集是 `quota <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=repair-quota>`_ 修复系列。

.. _fscounters:

### 冻结以修复汇总计数器

文件系统汇总计数器跟踪文件系统的资源可用性，如空闲块、空闲inode和已分配的inode。这些信息可以通过遍历空闲空间和inode索引来生成，但这是一个缓慢的过程。因此，XFS在磁盘上的超级块中维护了一个副本，该副本应该反映磁盘上的元数据，至少在文件系统被干净卸载时是这样。为了性能考虑，XFS还维护了内核中的计数器副本，这些副本对于实现活动事务的资源预留至关重要。写入线程从内核计数器中预留最坏情况下的资源量，并在提交时归还未使用的部分。因此，在将超级块写入磁盘时只需对超级块进行序列化处理即可。

XFS v5引入的懒加载超级块计数器功能进一步改进了这一点，通过训练日志恢复从AG头重新计算汇总计数器，从而消除了大多数事务需要触碰超级块的需求。XFS仅在文件系统卸载时才会提交汇总计数器。为了进一步减少争用，内核计数器实现了每个CPU计数器（percpu counter），这意味着每个CPU都会从全局内核计数器中分配一批块，并且可以从本地批次中满足小规模的分配。
高性能的汇总计数器使得在线 fsck 很难检查它们，因为在系统运行时无法使每个 CPU 的计数器静止。
虽然在线 fsck 可以读取文件系统的元数据来计算汇总计数器的正确值，但没有办法使每个 CPU 计数器的值保持稳定，因此在遍历完成后，这些计数器很有可能已经过时。
早期版本的在线扫描会返回用户空间，并带有未完成扫描标志，但这对于系统管理员来说并不是一个令人满意的解决方案。
为了进行修复，在遍历文件系统元数据时必须使内存中的计数器稳定下来，以便获取准确的读数并将其安装到每个 CPU 的计数器中。
为了满足这一要求，线上 fsck 必须阻止系统中的其他程序向文件系统发起新的写入操作，必须禁用后台垃圾回收线程，并且必须等待现有的写入程序退出内核。
一旦建立了这种状态，扫描就可以遍历 AG 空闲空间索引、inode B 树和实时位图来计算所有四个汇总计数器的正确值。
这与文件系统冻结非常相似，尽管并非所有组件都是必需的：

- 最终冻结状态设置为比 `SB_FREEZE_COMPLETE` 高一级，以防止其他线程解冻文件系统或其它扫描线程启动另一个 fscounters 冻结。
- 它不会使日志静止。

有了这段代码后，现在可以暂停文件系统足够长的时间来检查和纠正汇总计数器。

+--------------------------------------------------------------------------+
| **历史注释**：                                                           |
+--------------------------------------------------------------------------+
| 初始实现使用了实际的 VFS 文件系统冻结机制来静止文件系统活动。          |
| 在文件系统被冻结的情况下，可以精确解决计数器值，但直接调用 VFS 方法   |
| 存在许多问题：                                                           |
|                                                                          |
| - 其他程序可以在我们不知情的情况下解冻文件系统。                        |
|   这会导致不正确的扫描结果和不正确的修复。                               |
|                                                                          |
| - 添加一个额外的锁来防止其他人解冻文件系统需要添加一个 `->freeze_super`|
|   函数来包装 `freeze_fs()`。                                             |
|   这反过来又导致了其他微妙的问题，因为事实证明 VFS 的 `freeze_super`  |
|   和 `thaw_super` 函数可能会丢弃对 VFS 超级块的最后一个引用，任何后续  |
|   访问都会变成一个使用已释放内存（UAF）的错误！                         |
|   如果在底层块设备冻结文件系统时卸载文件系统，则会发生这种情况。       |
|   通过抓取超级块的额外引用可以解决这个问题，但鉴于这种方法的其他不足 |
|   之处，这感觉不太理想。                                                 |
|                                                                          |
| - 检查汇总计数器时不需要使日志静止，但 VFS 冻结会自动执行此操作。       |
|   这增加了实时 fscounter fsck 操作的运行时间。                           |
|                                                                          |
| - 使日志静止意味着 XFS 会在清理日志时将（可能不正确的）计数器刷新到磁  |
|   盘。                                                                   |
|                                                                          |
| - VFS 中的一个 bug 导致即使 `sync_filesystem` 未能刷新文件系统并返回错  |
|   误时，冻结仍可完成。这个 bug 在 Linux 5.17 版本中得到修复。           |
+--------------------------------------------------------------------------+

提议的补丁集是
`汇总计数器清理 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=repair-fscounters>`_ 系列。
完整的文件系统扫描
---------------------

某些类型的元数据只能通过遍历整个文件系统中的所有文件来记录观察结果，并将这些观察结果与磁盘上记录的信息进行比较。
和其他类型的在线修复一样，修复过程是通过将这些观察结果写入替换结构并原子性地提交来完成的。
然而，由于停机时间过长，关闭整个文件系统来检查数百亿个文件并不实际。
因此，在线 fsck 必须构建基础设施以管理对文件系统中所有文件的实时扫描。
要解决两个问题才能执行实时遍历：

- scrub 如何在收集数据时管理扫描？

- 扫描如何跟上其他线程对系统的更改？

.. _iscan:

协调的节点扫描
```````````````````````

在 20 世纪 70 年代最初的 Unix 文件系统中，每个目录条目包含一个索引号（*inumber*），该索引号用作磁盘上的固定大小记录（*inode*）数组（*itable*）的索引，描述文件的属性及其数据块映射。
这个系统由 J. Lions 在《Lions' Commentary on UNIX, 6th Edition》（澳大利亚新南威尔士大学计算机科学系，1977年11月），第18-2页中的 "inode (5659)" 描述；以及后来由 D. Ritchie 和 K. Thompson 在《The UNIX Time-Sharing System》（《贝尔系统技术杂志》，1978年7月），第1913-4页中的 "Implementation of the File System" 描述。
XFS 保留了这种设计的大部分内容，只是现在 inumber 是数据部分文件系统中的搜索键。
它们形成一个可以表示为 64 位整数的连续键空间，尽管 inode 本身在这个键空间内稀疏分布。
扫描以线性方式跨 inumber 键空间进行，从 `0x0` 开始，结束于 `0xFFFFFFFFFFFFFFFF`。
自然地，扫描一个键空间需要一个扫描游标对象来跟踪扫描进度。因为这个键空间是稀疏的，所以这个游标包含两个部分。

扫描游标的第一个部分跟踪下一个将要检查的inode；我们称之为检查游标。

稍微不那么明显的是，扫描游标对象还必须跟踪哪些部分的键空间已经被访问过，这对于决定并发文件系统更新是否需要合并到扫描数据中至关重要。我们称这部分为已访问inode游标。

推进扫描游标是一个多步骤的过程，封装在`xchk_iscan_iter`函数中：

1. 锁定包含由已访问inode游标指向的inode的AG的AGI缓冲区。这保证了在这个AG中的inode在推进游标期间不会被分配或释放。
2. 使用每个AG的inode B树查找刚刚访问过的inode之后的下一个inode编号，因为它可能不是键空间相邻的。
3. 如果当前AG中没有更多的inode：

   a. 将检查游标移动到对应于下一个AG起始点的inode编号键空间位置。
   
   b. 调整已访问inode游标，以表明它已经“访问”了当前AG的inode键空间中的最后一个可能inode。
XFS 的inode编号是分段的，因此需要标记光标为已访问到下一个AG的inode键空间开始之前的整个键空间。

- 如果文件系统中还有未检查的AG，则解锁AGI并返回到步骤1。
- 如果没有更多的AG需要检查，则将两个光标设置到inode键空间的末尾。扫描现在完成。

4. 否则，在这个AG中至少还有一个inode需要扫描：

   - 将检查光标向前移动到由inode B树标记为分配的下一个inode。
   - 调整已访问inode光标，使其指向检查光标当前位置之前的inode。
   - 由于扫描器持有AGI缓冲锁，所以在已访问inode光标刚刚前进的部分inode键空间中不可能创建新的inode。

5. 获取检查光标所指向的inode编号对应的内核inode。
   通过保持AGI缓冲锁直到这一点，扫描器知道在整个键空间中前进检查光标是安全的，并且它已经稳定了下一个inode，使其在扫描释放内核inode之前不会从文件系统中消失。

6. 释放AGI锁并将内核inode返回给调用者。
在线文件系统检查（fsck）功能按如下步骤扫描文件系统中的所有文件：

1. 通过调用`xchk_iscan_start`开始扫描。
2. 移动扫描游标（`xchk_iscan_iter`）以获取下一个inode。如果提供了inode：

   a. 锁定inode，以防止在扫描过程中进行更新。
   b. 扫描inode。
   c. 在仍然持有inode锁的情况下，调整已访问inode游标（`xchk_iscan_mark_visited`），使其指向当前inode。
   d. 解锁并释放inode。
3. 调用`xchk_iscan_teardown`完成扫描。

inode缓存中存在一些细微之处，这使得为调用者获取内核态inode变得复杂。
显然，inode元数据必须足够一致以便加载到inode缓存中。
其次，如果内核态inode处于某种中间状态，扫描协调器必须释放AGI并将主文件系统推回，使inode恢复到可加载状态。
提议的补丁是 `inode扫描器 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=scrub-iscan>`_ 系列。

新功能的第一个用户是 `在线配额检查 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=repair-quotacheck>`_ 系列。

### Inode管理

在常规文件系统代码中，已分配的XFS内核inode引用始终是在事务上下文之外获取的（通过 `xfs_iget`），因为现有文件的内核上下文创建不需要元数据更新。
然而，需要注意的是，在文件创建过程中获取的内核inode引用必须在事务上下文中进行，因为文件系统必须确保磁盘上的inode B树索引更新和实际磁盘inode初始化的原子性。
内核inode引用始终是在事务上下文之外释放的（通过 `xfs_irele`），因为有一些活动可能需要磁盘上的更新：

- VFS可能会决定作为 `DONTCACHE` inode释放的一部分启动写回操作。
- 预测性预分配需要取消预留。
- 一个未链接的文件可能失去了最后一个引用，在这种情况下，整个文件必须被去激活，这涉及到释放其在磁盘元数据中的所有资源并释放inode。

这些活动统称为inode去激活。去激活有两个部分——VFS部分，它会触发所有脏页面的写回操作；XFS部分，清理XFS特定的信息并在inode被解除链接时释放inode。
如果inode被解除链接（或在文件句柄操作后未连接），内核会立即将inode送入去激活机制。
在正常运行期间，为了防止死锁，资源获取顺序如下：

1. 索引节点引用（`iget`）
2. 如果正在进行修复，则获取文件系统冻结保护（`mnt_want_write_file`）
3. 索引节点 `IOLOCK`（VFS `i_rwsem`）锁，用于控制文件 I/O 操作
4. 索引节点 `MMAPLOCK`（页缓存 `invalidate_lock`）锁，用于可以更新页缓存映射的操作
5. 日志功能启用
6. 事务日志空间授权
7. 事务所需的数据和实时设备上的空间
8. 如果正在修复文件，则获取内核 dquot 引用
   （注意：这些并不是锁定，只是获取）
9. 索引节点 `ILOCK`，用于文件元数据更新
10. AG header buffer 锁 / 实时元数据inode ILOCK
11. 如果适用，实时元数据buffer锁
12. 如果适用，范围映射B树块
资源通常以相反的顺序释放，但这不是必须的。
然而，在线fsck（文件系统检查）与常规XFS操作不同，因为它可能会检查一个通常在锁定顺序后期获取的对象，并决定将该对象与锁定顺序早期获取的对象进行交叉引用。
接下来的几节详细说明了在线fsck如何避免死锁的具体方法。

### 在擦除操作期间的iget和irele
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

在代表擦除操作执行的inode扫描运行在事务上下文中，可能已经锁定了资源并将其绑定到事务中。
对于`iget`来说，这并不是很大的问题，因为它可以在现有事务的上下文中操作，只要所有绑定的资源都在inode引用之前被获取即可。
当VFS的`iput`函数接收到一个没有其他引用的链接inode时，它通常会将inode放入LRU列表中，希望如果另一个进程在系统内存耗尽并释放inode之前重新打开文件，可以节省时间。
文件系统调用者可以通过在inode上设置一个`DONTCACHE`标志来绕过LRU过程，从而让内核尝试立即将inode放入失效机制中。
在过去，文件节点的失效总是由丢弃该文件节点的过程来完成，这对于 scrub 来说是一个问题，因为 scrub 可能已经持有了一个事务，而 XFS 不支持事务嵌套。
另一方面，如果没有 scrub 事务，则希望立即丢弃未被使用的文件节点以避免污染缓存。
为了捕捉这些细微差别，在线 fsck 代码有一个单独的 `xchk_irele` 函数来设置或清除 `DONTCACHE` 标志，以获得所需的释放行为。
提议的补丁集包括修复 `scrub iget 使用` 和 `dir iget 使用`。

.. _ilocking:

文件节点锁定
^^^^^^^^^^^^^^

在常规文件系统代码中，VFS 和 XFS 将按照已知顺序获取多个 IOLOCK 锁：更新目录树时，父节点 → 子节点；其他情况下则按其 `struct inode` 对象地址的数值顺序。
对于常规文件，在获取 IOLOCK 后可以获取 MMAPLOCK 以阻止页面错误。
如果必须获取两个 MMAPLOCK，则按照其 `struct address_space` 对象地址的数值顺序获取。
由于现有文件系统代码的结构，必须在分配事务之前获取 IOLOCK 和 MMAPLOCK。
如果必须获取两个 ILOCK，则按照 i-number 顺序获取。
在协调文件节点扫描期间，必须谨慎地进行文件节点锁的获取。
在线文件系统检查（online fsck）无法遵守这些约定，因为对于目录树扫描器来说，擦除过程会持有正在扫描的文件的IO锁，并且需要获取目录链接另一端文件的IO锁。如果目录树因为包含循环而损坏，`xfs_scrub` 无法使用常规的inode锁定功能，从而避免陷入 ABBA 死锁。

解决这两个问题的方法是直接的——任何时候在线 fsck 需要获取第二个相同类型的锁时，它都会使用 trylock 来避免 ABBA 死锁。如果 trylock 失败，擦除过程将释放所有 inode 锁，并使用 trylock 循环重新获取所有必要的资源。trylock 循环使擦除过程能够检查是否有待处理的致命信号，这是擦除过程避免文件系统死锁或成为无响应进程的方式。

然而，trylock 循环意味着在线 fsck 必须准备好在锁周期之前和之后测量被擦除的资源，以检测变化并相应地作出反应。

.. _dirparent:

案例研究：查找目录父项
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

以修复目录父指针的代码为例。在线 fsck 必须验证一个目录的 `..` 目录条目是否指向其父目录，并且父目录中恰好有一个目录条目指向子目录。完全验证这种关系（并在可能的情况下进行修复）需要在持有子目录锁定的同时遍历文件系统中的每个目录，并且在此过程中对目录树进行更新。协调的 inode 扫描提供了一种方法，可以在不遗漏任何 inode 的情况下遍历文件系统。
子目录保持锁定状态以防止对 `dotdot` 目录条目的更新，但如果扫描器未能锁定父目录，则可以释放并重新锁定子目录和预期的父目录。

如果在目录未锁定期间 `dotdot` 条目发生变化，则说明移动或重命名操作更改了子目录的父级关系，此时扫描可以提前退出。

所提议的补丁集是
`目录修复 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=repair-dirs>`_ 系列。

.. _fshooks:

文件系统钩子
````````````````

在线文件系统检查（fsck）功能在进行完整的文件系统扫描时需要的第二个支持是在动态环境中获取其他线程对文件系统所做的更新的信息，因为在动态环境中与过去的状态进行比较是没有意义的。Linux 内核基础设施中有两个组件使得在线 fsck 能够监控常规的文件系统操作：文件系统钩子和 :ref:`静态键 <jump_labels>`。

文件系统钩子将正在进行的文件系统操作信息传递给下游消费者。在这种情况下，下游消费者始终是一个在线 fsck 功能。由于多个 fsck 功能可以并行运行，因此在线 fsck 使用 Linux 通知调用链机制来分发更新到任何数量感兴趣的 fsck 进程。调用链是一个动态列表，这意味着它们可以在运行时配置。

因为这些钩子是 XFS 模块私有的，所以传递的信息正好满足检查功能更新其观察所需的内容。
当前XFS钩子的实现使用SRCU通知链来减少对高线程工作负载的影响。
常规的阻塞通知链使用读写信号量（rwsem），对于单线程应用程序来说开销似乎要小得多。
然而，可能会发现阻塞链和静态键的组合是一种更高效的方案；这里需要进一步的研究。
要在一个文件系统的特定点插入钩子，需要以下组件：

- 必须在某个方便的地方（如一个知名的内存中文件系统对象）嵌入一个`struct xfs_hooks`对象。
- 每个钩子必须定义一个操作码以及包含更多上下文信息的结构体。
- 钩子提供者应围绕`xfs_hooks`和`xfs_hook`对象提供适当的包装函数和结构体，以利用类型检查确保正确使用。
- 在常规文件系统代码中选择一个调用点来调用`xfs_hooks_call`，并传入操作码和数据结构。
该调用点应紧邻（且不早于）文件系统更新提交到事务的位置。
通常情况下，当文件系统调用一个钩子链时，应该能够处理睡眠，并且不应容易受到内存回收或锁递归的影响。
然而，具体的要求非常依赖于钩子调用者和被调用者的上下文。
在线 fsck 功能应定义一个结构来保存扫描数据，一个锁以协调对扫描数据的访问，以及一个 `struct xfs_hook` 对象。

扫描器函数和常规文件系统代码必须按照相同的顺序获取资源；具体细节请参见下一节。

在线 fsck 代码必须包含一个 C 函数来捕获钩子动作代码和数据结构。
如果正在更新的对象已经被扫描访问过，则必须将钩子信息应用于扫描数据。

在解锁inode开始扫描之前，在线 fsck 必须调用 `xfs_hooks_setup` 来初始化 `struct xfs_hook`，并调用 `xfs_hooks_add` 来启用钩子。

一旦扫描完成，在线 fsck 必须调用 `xfs_hooks_del` 来禁用钩子。
应尽量减少钩子的数量以降低复杂度。

静态键用于在在线 fsck 不运行时将文件系统钩子的开销降至几乎为零。

.. _liveupdate:

扫描期间的实时更新
```````````````````

在线 fsck 扫描代码和 :ref:`钩子<fshooks>` 文件系统代码的执行路径如下所示：

```
            其他程序
                  ↓
            inode 锁 ←────────────────────┐
                  ↓                         │
            AG 头部锁                       │
                  ↓                         │
            文件系统函数                     │
                  ↓                         │
            通知调用链                      │    相同
                  ↓                         ├─── inode
            清理钩子函数                     │    锁
                  ↓                         │
            扫描数据互斥锁 ←──┐    相同      │
                  ↓            ├─── 扫描     │
            更新扫描数据     │    锁        │
                  ↑            │             │
            扫描数据互斥锁 ←──┘             │
                  ↑                         │
            inode 锁 ←────────────────────┘
                  ↑
            清理函数
                  ↑
            inode 扫描器
                  ↑
            xfs_scrub
```

为了确保检查代码与进行文件系统更新的代码之间正确交互，必须遵循以下规则：

- 在调用通知调用链之前，被钩子挂载的文件系统函数必须获取与清理扫描函数扫描 inode 时相同的锁。
- 扫描函数和清理钩子函数必须通过获取扫描数据上的锁来协调对扫描数据的访问。
- 清理钩子函数在扫描观察中添加实时更新信息时，必须确保被更新的inode已经完成扫描。
  扫描协调器提供了一个辅助谓词（``xchk_iscan_want_live_update``）来实现这一点。

- 清理钩子函数不得更改调用者的状态，包括其正在运行的事务。
  它们不得获取任何可能与被挂钩的文件系统功能冲突的资源。

- 钩子函数可以在必要时终止inode扫描以避免违反其他规则。
  inode扫描API相对简单：

- ``xchk_iscan_start`` 开始一次扫描。

- ``xchk_iscan_iter`` 获取扫描中下一个inode的引用，如果没有更多的inode可扫描，则返回零。

- ``xchk_iscan_want_live_update`` 决定一个inode是否已经在扫描中被访问过。
  这对于钩子函数决定是否需要更新内存中的扫描信息至关重要。

- ``xchk_iscan_mark_visited`` 标记一个inode为已访问。

- ``xchk_iscan_teardown`` 结束扫描。

这些功能也是inode扫描系列的一部分：
`inode扫描 <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=scrub-iscan>`_

.. _quotacheck:

案例研究：配额计数检查
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

将挂载时间的配额检查代码与在线修复时的配额检查代码进行比较是有益的。
挂载时间的配额检查不需要处理并发操作，因此它执行以下步骤：

1. 确保磁盘上的dquot处于足够良好的状态，使得所有内存中的dquot都能够加载，并将磁盘缓冲区中的资源使用计数器清零。
2. 遍历文件系统中的每个inode
将每个文件的资源使用情况添加到内核dquot中

3. 遍历每个内核dquot
如果内核dquot没有被刷新，则将支持内核dquot的磁盘缓冲区添加到延迟写（delwri）列表中

4. 将缓冲区列表写入磁盘
像大多数在线fsck功能一样，在线配额检查在新收集的元数据反映所有文件系统状态之前，不能写入常规文件系统对象
因此，在线配额检查将文件资源使用记录到一个由稀疏的“xfarray”实现的影子dquot索引中，并且只有在扫描完成后才会写入实际的dquot
处理事务更新比较棘手，因为配额资源使用更新是分阶段处理以尽量减少对dquot的竞争：

1. 参与的inode被加入并锁定到一个事务中
2. 对于每个附加到文件的dquot：

   a. 锁定dquot
   b. 向dquot的资源使用情况添加配额预留
预订记录在事务中：
c. 解锁 dquot
3. 实际配额使用的变化在事务中进行跟踪
4. 在事务提交时，每个 dquot 再次被检查：

   a. 再次锁定 dquot
   b. 记录配额使用变化，并将未使用的预订归还给 dquot
   c. 解锁 dquot
对于在线配额检查，在步骤 2 和步骤 4 中设置了钩子
步骤 2 的钩子创建了一个事务 dquot 上下文（`dqtrx`）的影子版本，其工作方式类似于常规代码
步骤 4 的钩子将影子 `dqtrx` 的更改提交到影子 dquot
请注意，这两个钩子都是在索引节点锁定的情况下被调用的，这是实时更新与索引节点扫描器协调的方式
quotacheck 扫描看起来是这样的：

1. 设置一个协调的inode扫描
2. 对inode扫描迭代器返回的每个inode执行以下操作：

   a. 获取并锁定inode
   b. 确定该inode的资源使用情况（数据块、inode数量、实时块），并将这些信息添加到与inode关联的用户ID、组ID和项目ID的影子dquot中
   c. 解锁并释放inode
3. 对系统中的每个dquot执行以下操作：

   a. 获取并锁定dquot
   b. 将dquot与扫描过程中创建并通过实时钩子更新的影子dquot进行对比

实时更新对于能够遍历所有配额记录而不必长时间持有任何锁至关重要。
如果需要修复，真实的dquot和影子dquot会被锁定，并将其资源计数设置为影子dquot中的值。

提议的补丁集是 `online quotacheck <https://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/log/?h=repair-quotacheck>`_ 系列。

.. _nlinks:

案例研究：文件链接计数检查
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

文件链接计数检查也使用实时更新钩子。
协调的inode扫描器用于遍历文件系统中的所有目录，并且每个文件的链接计数记录存储在一个稀疏的“xfarray”中，该数组按inode编号索引。

在扫描阶段，目录中的每个条目会生成如下观察数据：

1. 如果条目是根目录的“..”条目，则该目录的父链接计数会增加，因为根目录的“..”条目是自引用的。
2. 如果条目是子目录的“..”条目，则其父目录的反向引用计数会增加。
3. 如果条目既不是“.”也不是“..”，则目标文件的父计数会增加。
4. 如果目标是一个子目录，则其父目录的子链接计数会增加。

理解链接计数inode扫描器如何与实时更新钩子交互的一个关键点是，扫描游标跟踪哪些*父*目录已被扫描。换句话说，即使B已经被扫描，如果A没有被扫描，实时更新也会忽略任何关于“A → B”的更新。此外，一个具有指向B的“..”条目的子目录A，在A的影