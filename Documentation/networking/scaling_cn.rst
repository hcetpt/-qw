SPDX 许可证标识符: GPL-2.0

=====================================
Linux 网络堆栈中的扩展性
=====================================

简介
============

本文档描述了 Linux 网络堆栈中一组互补的技术，以增加并行性和提高多处理器系统的性能。以下技术将被介绍：

- RSS: 接收端扩展
- RPS: 接收包调度
- RFS: 接收流调度
- 加速接收流调度
- XPS: 发送包调度

RSS: 接收端扩展
=========================

现代的网卡支持多个接收和发送描述符队列（多队列）。在接收时，网卡可以将不同的数据包发送到不同的队列中，以便在各个 CPU 上分配处理任务。网卡通过应用过滤器来分配每个数据包到少数逻辑流之一，从而分配数据包。每个流的数据包会被导向一个单独的接收队列，进而可以由不同的 CPU 处理。这种机制通常被称为“接收端扩展”（RSS）。RSS 及其他扩展技术的目标是均匀地提高性能。多队列分布也可以用于流量优先级划分，但这不是这些技术的重点。RSS 中使用的过滤器通常是基于网络层和/或传输层头部的哈希函数——例如，对数据包的 IP 地址和 TCP 端口进行四元组哈希。最常见的硬件 RSS 实现使用了一个 128 项的间接表，其中每一项存储一个队列编号。数据包的接收队列是通过屏蔽计算出的哈希值的低七位，并将其作为键访问间接表来确定的（通常为 Toeplitz 哈希）。

一些网卡支持对称 RSS 哈希，即如果 IP（源地址、目标地址）和 TCP/UDP（源端口、目标端口）元组互换，计算出的哈希值相同。这对某些监控 TCP/IP 流的应用（如 IDS、防火墙等）非常有益，因为需要双向流落在同一个接收队列（和 CPU）上。“对称-XOR”是一种 RSS 算法，通过异或输入的源和目标字段来实现这种哈希对称性。然而，这会减少输入熵，并可能被利用。具体算法如下：

```
# (SRC_IP ^ DST_IP, SRC_IP ^ DST_IP, SRC_PORT ^ DST_PORT, SRC_PORT ^ DST_PORT)
```

结果然后传递给底层的 RSS 算法。

一些先进的网卡允许根据可编程过滤器来引导数据包到队列。例如，可以将指向 Web 服务器的 TCP 80 端口的数据包定向到它们自己的接收队列。这样的“n-元组”过滤器可以通过 ethtool （--config-ntuple）进行配置。

RSS 配置
-----------------

对于支持多队列的网卡，其驱动程序通常提供一个内核模块参数来指定要配置的硬件队列数量。例如，在 bnx2x 驱动程序中，这个参数称为 num_queues。典型的 RSS 配置是在设备支持足够队列的情况下为每个 CPU 分配一个接收队列，否则至少为每个内存域分配一个队列，其中内存域是一组共享特定内存级别的 CPU（L1、L2、NUMA 节点等）。

RSS 设备的间接表，通过屏蔽哈希来解析队列，通常在初始化时由驱动程序编程。默认映射是在表中均匀分布队列，但间接表可以在运行时使用 ethtool 命令（--show-rxfh-indir 和 --set-rxfh-indir）获取和修改。修改间接表可以使不同的队列具有不同的相对权重。

RSS IRQ 配置
~~~~~~~~~~~~~~~~~~~~~

每个接收队列都有一个独立的 IRQ 与之关联。当新数据包到达指定队列时，网卡会触发此中断来通知 CPU。PCIe 设备的信号路径使用消息中断（MSI-X），可以将每个中断路由到特定的 CPU。当前队列到 IRQ 的映射可以从 /proc/interrupts 文件中确定。默认情况下，IRQ 可能在任何 CPU 上处理。由于相当一部分的数据包处理发生在接收中断处理中，因此分散接收中断到不同的 CPU 是有利的。要手动调整每个中断的 IRQ 亲和性，请参阅 `Documentation/core-api/irq/irq-affinity.rst`。一些系统会运行 irqbalance 守护进程，动态优化 IRQ 分配，因此可能会覆盖任何手动设置。

建议配置
~~~~~~~~~~~~~~~~~~~~~

当延迟是一个问题或接收中断处理形成瓶颈时，应启用 RSS。将负载分散到 CPU 之间可以减少队列长度。对于低延迟网络，最佳设置是为系统中的每个 CPU 分配尽可能多的队列（或 NIC 最大值，如果较低）。最高效的高吞吐量配置可能是队列数量最少的情况，即没有接收队列因饱和的 CPU 导致溢出，因为在默认模式下启用了中断合并，总的中断数量（以及工作量）会随着每个额外队列而增长。
每个CPU的负载可以通过`mpstat`工具进行观察，但请注意，在具有超线程（HT）功能的处理器上，每个超线程都会被表示为一个独立的CPU。对于中断处理而言，初步测试表明超线程没有带来任何好处，因此需要将队列数量限制为系统中CPU核心的数量。

### 专用RSS上下文
~~~~~~~~~~~~~~~
现代网卡支持创建多个共存的RSS配置，这些配置是基于明确的匹配规则选择的。当应用程序希望将接收流量的队列集限制在特定的目的端口或IP地址时，这非常有用。下面的例子展示了如何将所有TCP端口22的流量导向队列0和1。

要创建一个额外的RSS上下文，请使用以下命令：
```
# ethtool -X eth0 hfunc toeplitz context new
New RSS context is 1
```
内核会报告分配的上下文ID（默认的、始终存在的RSS上下文ID为0）。可以使用与默认上下文相同的API查询和修改新的上下文：
```
# ethtool -x eth0 context 1
RX flow hash indirection table for eth0 with 13 RX ring(s):
    0:      0     1     2     3     4     5     6     7
    8:      8     9    10    11    12     0     1     2
[...]
# ethtool -X eth0 equal 2 context 1
# ethtool -x eth0 context 1
RX flow hash indirection table for eth0 with 13 RX ring(s):
    0:      0     1     0     1     0     1     0     1
    8:      0     1     0     1     0     1     0     1
[...]
```
为了利用新的上下文，可以使用n元组过滤器将流量定向到该上下文：
```
# ethtool -N eth0 flow-type tcp6 dst-port 22 context 1
Added rule with ID 1023
```
完成之后，移除上下文和规则：
```
# ethtool -N eth0 delete 1023
# ethtool -X eth0 context 1 delete
```

### RPS：接收包转向
======================
接收包转向（RPS）逻辑上是RSS的软件实现。由于它是在软件中实现的，因此必然在数据路径中的较晚阶段被调用。RSS选择硬件中断处理程序运行的队列和CPU，而RPS则选择在中断处理程序之上执行协议处理的CPU。这是通过将包放置在目标CPU的待处理队列上并唤醒该CPU来实现的。RPS具有一些相对于RSS的优势：

1. 它可以用于任何网卡。
2. 可以轻松地添加软件过滤器以对新协议进行哈希计算。
3. 它不会增加硬件设备的中断率（尽管它确实引入了跨处理器中断（IPI））。

RPS在接收中断处理程序的下半部分被调用，当驱动程序通过`netif_rx()`或`netif_receive_skb()`发送包时。这些函数会调用`get_rps_cpu()`函数，该函数选择应该处理包的队列。
确定RPS目标CPU的第一步是对包的地址或端口（根据协议不同为二元组或四元组哈希）进行流哈希计算。这作为包相关流的一致哈希值。哈希值由硬件提供或在网络栈中计算得出。具备相应能力的硬件可以在接收描述符中传递哈希值；这通常与RSS使用的哈希相同（例如计算的Toeplitz哈希）。哈希值保存在`skb->hash`中，并且可以在网络栈的其他地方作为包流的哈希值使用。
每个接收硬件队列都有一个关联的CPU列表，RPS可以将包入队到这些CPU进行处理。对于每个接收到的包，从流哈希值模数列表大小计算出列表中的索引。索引所指向的CPU是处理包的目标，包会被排队到该CPU的待处理队列尾部。在下半部分例程结束时，向任何其待处理队列中有包的CPU发送IPI。IPI会在远程CPU上唤醒待处理任务，并且任何已排队的包随后会被向上处理到网络栈。
#### RPS配置
-----------------
RPS要求内核编译时包含CONFIG_RPS kconfig符号（默认情况下SMP下启用）。即使编译进去，RPS仍然处于禁用状态，直到显式配置。可以使用sysfs文件条目为每个接收队列配置RPS可能转发流量的CPU列表：
```
/sys/class/net/<dev>/queues/rx-<n>/rps_cpus
```
此文件实现了CPU位图。当其为零（默认值）时，RPS被禁用，此时包将在中断CPU上处理。《Documentation/core-api/irq/irq-affinity.rst》解释了如何将CPU分配到位图中。
#### 建议配置
~~~~~~~~~~~~~~~~~~~~~~~
对于单队列设备，典型的RPS配置是将`rps_cpus`设置为中断CPU所在内存域中的CPU。如果NUMA局部性不是问题，也可以将其设置为系统中的所有CPU。在高中断率下，可能明智的做法是将中断CPU从映射中排除，因为该CPU已经承担了很多工作。
对于一个多队列系统，如果RSS（接收侧扩展）配置为每个硬件接收队列映射到一个CPU，则RPS（接收包调度）可能变得多余且不必要。如果硬件队列的数量少于CPU的数量，并且每个队列的rps_cpus与该队列的中断CPU共享相同的内存域，则RPS可能会带来好处。

### RPS流限制

RPS在多个CPU之间扩展内核接收处理而不引入重排序。将来自同一流的所有数据包发送到同一个CPU的代价是在流量变化时可能导致CPU负载不平衡。在极端情况下，单一流量可能占据大部分流量。特别是在具有许多并发连接的常见服务器工作负载中，这种行为表明存在问题，例如配置错误或伪造源地址的拒绝服务攻击。

**流限制**是RPS的一个可选功能，在CPU争用期间通过稍微优先丢弃大流量的数据包来优先处理小流量。当某个RPS或RFS目标CPU接近饱和时，流限制才会生效。一旦CPU的输入数据包队列长度超过最大队列长度的一半（由sysctl `net.core.netdev_max_backlog`设置），内核开始统计过去256个数据包中的每一流量。如果新到达的数据包在一个流中所占比例超过设定的比例（默认为一半），则该数据包会被丢弃。其他流的数据包只有在输入数据包队列达到`netdev_max_backlog`时才会被丢弃。

当输入数据包队列长度低于阈值时，不会丢弃任何数据包，因此流限制不会直接切断连接：即使是大流量也保持连通性。

### 接口

流限制默认编译进内核（CONFIG_NET_FLOW_LIMIT），但未启用。它是为每个CPU独立实现的（以避免锁和缓存争用），并通过设置sysctl `net.core.flow_limit_cpu_bitmap`中的相关位来在每个CPU上切换。它提供了与rps_cpus相同的CPU位图接口（见上文）：

```
/proc/sys/net/core/flow_limit_cpu_bitmap
```

每个流的速率通过将每个数据包哈希到哈希表桶并递增桶计数器来计算。哈希函数与RPS中选择CPU的哈希函数相同，但由于桶的数量可以远大于CPU数量，因此流限制对大流量有更细粒度的识别和更少的误报。默认哈希表有4096个桶。这个值可以通过sysctl修改：

```
net.core.flow_limit_table_len
```

这个值仅在分配新表时使用。修改它不会更新活动表。

### 建议配置

流限制在具有许多并发连接的系统中非常有用，其中单个连接占用50%的CPU资源表示存在问题。

在这样的环境中，应启用所有处理网络接收中断的CPU上的此功能（如在`/proc/irq/N/smp_affinity`中设置）。

此功能依赖于输入数据包队列长度超过流限制阈值（50%）加上流历史长度（256）。

实验表明，将`net.core.netdev_max_backlog`设置为1000或10000表现良好。
RFS：接收流调度
==========================

尽管RPS（Receive Packet Steering）仅基于哈希值对数据包进行调度，从而通常能够提供良好的负载分布，但它并未考虑应用程序的局部性。这一问题通过接收流调度（RFS）得以解决。RFS的目标是通过将数据包的内核处理调度到正在运行该数据包的应用程序线程所在的CPU上，来提高数据缓存命中率。RFS依赖于相同的RPS机制将数据包入队到另一个CPU的待处理队列，并唤醒该CPU。

在RFS中，数据包不会直接根据其哈希值转发，而是使用哈希值作为索引来查找流表。该表将流映射到处理这些流的CPU。流哈希（见上文RPS部分）用于计算该表中的索引。每个条目中记录的CPU是上次处理该流的CPU。如果一个条目中没有有效的CPU，则映射到该条目的数据包会使用普通的RPS进行调度。多个表项可能指向同一个CPU。实际上，在有许多流和少量CPU的情况下，很可能单个应用程序线程处理具有许多不同流哈希值的流。

`rps_sock_flow_table`是一个全局流表，包含流的*期望*CPU：当前在用户空间处理该流的CPU。每个表项的值是一个CPU索引，在调用`recvmsg`和`sendmsg`时更新（具体来说是在`inet_recvmsg()`、`inet_sendmsg()`和`tcp_splice_read()`中）。

当调度器将一个线程移动到新CPU上，而它在旧CPU上有未处理的接收数据包时，可能会出现数据包乱序。为了避免这种情况，RFS使用第二个流表来跟踪每个流的未完成数据包：`rps_dev_flow_table`是针对每个设备的每个硬件接收队列的具体表。每个表项存储一个CPU索引和一个计数器。CPU索引表示该流的数据包进一步内核处理所入队的*当前*CPU。理想情况下，内核和用户空间处理在同一CPU上进行，因此两个表中的CPU索引相同。如果调度器最近迁移了一个用户空间线程，而内核仍在旧CPU上排队处理数据包，则此假设可能不成立。

`rps_dev_flow_table`中的计数器记录了最后一次将该流的数据包入队到当前指定CPU时该CPU的待处理队列长度。每个待处理队列都有一个头计数器，在出队时递增。尾计数器计算为头计数器+队列长度。换句话说，`rps_dev_flow[i]`中的计数器记录了流i中最后被入队到当前指定CPU上的元素（当然，条目i实际上是通过哈希选择的，且多个流可能会哈希到相同的条目i）。

避免乱序数据包的诀窍在于：选择数据包处理的CPU（从`get_rps_cpu()`获取）时，会比较`rps_sock_flow`表和数据包接收到的队列的`rps_dev_flow`表。如果该流的期望CPU（在`rps_sock_flow`表中找到）与当前CPU（在`rps_dev_flow`表中找到）匹配，则将数据包入队到该CPU的待处理队列。如果不匹配，则在满足以下条件之一时，将当前CPU更新为期望的CPU：

  - 当前CPU的队列头计数器 >= `rps_dev_flow[i]`中记录的尾计数器值
  - 当前CPU未设置（>= `nr_cpu_ids`）
  - 当前CPU离线

经过此检查后，数据包会被发送到（可能已更新的）当前CPU。这些规则旨在确保只有在旧CPU上没有未处理的数据包时，流才会迁移到新的CPU，因为未处理的数据包可能比即将在新CPU上处理的数据包晚到达。

RFS配置
--------

RFS仅在启用kconfig符号`CONFIG_RPS`时可用（默认情况下对于SMP系统是启用的）。功能在明确配置之前保持禁用状态。全局流表的条目数量通过以下方式设置：

```
/proc/sys/net/core/rps_sock_flow_entries
```

每个队列的流表条目数量通过以下方式设置：

```
/sys/class/net/<dev>/queues/rx-<n>/rps_flow_cnt
```

建议配置
~~~~~~~~~~

在为接收队列启用RFS之前，需要先设置这两个参数。
两个值都向上取整到最接近的二的幂。建议的流数量取决于预期的任意时刻活动连接数，这可能显著少于打开的连接数。我们发现，在中等负载的服务器上，将 `rps_sock_flow_entries` 设置为 32768 的值表现得相当好。
对于单队列设备，单个队列的 `rps_flow_cnt` 值通常配置为与 `rps_sock_flow_entries` 相同的值。
对于多队列设备，每个队列的 `rps_flow_cnt` 可能配置为 `rps_sock_flow_entries / N`，其中 N 是队列的数量。例如，如果 `rps_sock_flow_entries` 设置为 32768 并且有 16 个配置的接收队列，则每个队列的 `rps_flow_cnt` 可能配置为 2048。

加速 RFS
========

加速 RFS 对 RFS 就像 RSS 对 RPS：这是一种使用软状态来根据消耗每个流数据的应用线程所在位置来引导流量的硬件加速负载均衡机制。加速 RFS 应该比 RFS 表现更好，因为数据包直接发送到处理应用数据的本地 CPU。目标 CPU 要么是应用程序运行所在的 CPU，要么至少是在缓存层次结构中与应用线程的 CPU 局部的 CPU。
为了启用加速 RFS，网络堆栈调用 `ndo_rx_flow_steer` 驱动程序函数来指定匹配特定流的数据包所需的硬件队列。每当 `rps_dev_flow_table` 中的流条目更新时，网络堆栈会自动调用此函数。驱动程序随后使用特定于设备的方法来编程 NIC 以引导数据包。
一个流的硬件队列是从记录在 `rps_dev_flow_table` 中的 CPU 衍生出来的。堆栈会参考由 NIC 驱动程序维护的 CPU 到硬件队列映射。这个映射是 `/proc/interrupts` 显示的 IRQ 亲和表的自动生成的反向映射。驱动程序可以使用内核库 `cpu_rmap`（“CPU 亲和反向映射”）中的函数来填充映射。对于每个 CPU，映射中对应的队列设置为与其处理 CPU 在缓存局部性上最近的队列。

加速 RFS 配置
--------------

加速 RFS 仅在内核编译时包含 `CONFIG_RFS_ACCEL` 且 NIC 设备和驱动程序提供支持的情况下可用。
它还要求通过 ethtool 启用 ntuple 过滤。CPU 到队列的映射会根据驱动程序为每个接收队列配置的 IRQ 亲和性自动推断出来，因此不需要额外的配置。

建议的配置
~~~~~~~~~~~

每当想要使用 RFS 并且 NIC 支持硬件加速时，应启用此技术。
XPS：传输包调度
=============================

传输包调度是一种智能选择在多队列设备上发送数据包时使用的传输队列的机制。这可以通过记录两种映射来实现，一种是CPU到硬件队列的映射，另一种是接收队列到硬件传输队列的映射。

1. 使用CPU映射的XPS

此映射的主要目标通常是将队列专门分配给CPU子集，在这些队列上的传输完成由该集合内的CPU处理。这种选择提供了两个好处。首先，由于较少的CPU争夺相同的队列，因此设备队列锁上的争用显著减少（如果每个CPU都有自己的传输队列，则可以完全消除争用）。其次，传输完成时的缓存未命中率降低，特别是对于持有sk_buff结构的数据缓存行。

2. 使用接收队列映射的XPS

此映射用于根据管理员设置的接收队列映射配置来选择传输队列。一组接收队列可以映射到一组传输队列（多对多），尽管常见的使用情况是一对一映射。这将使发送数据包时使用与接收相同的队列关联成为可能。这对于繁忙轮询多线程工作负载非常有用，在这种情况下，很难将特定CPU与特定应用程序线程相关联。应用程序线程没有绑定到CPU，并且每个线程处理从单个队列收到的数据包。连接的套接字中缓存了接收队列号。在这种模型中，通过同一传输队列发送与关联接收队列相对应的数据包有助于保持CPU开销低。传输完成的工作锁定在应用程序正在轮询的相同队列关联上。这避免了触发其他CPU中断的开销。当应用程序在繁忙轮询期间清理数据包时，可以在同一线程上下文中处理传输完成，从而减少延迟。

XPS是通过设置可使用该队列传输的CPU/接收队列位图来为每个传输队列进行配置的。从CPU到传输队列或从接收队列到传输队列的逆向映射被计算并维护在每个网络设备上。在传输流中的第一个数据包时，会调用函数get_xps_queue()来选择一个队列。此函数使用套接字连接的接收队列ID来匹配接收队列到传输队列查找表。或者，此函数也可以使用运行CPU的ID作为CPU到队列查找表的键。如果ID匹配单个队列，则使用该队列进行传输。如果有多个队列匹配，则使用流哈希来计算一组中的索引来选择一个队列。在基于接收队列映射选择传输队列时，不会验证传输设备与接收设备，因为这需要在数据路径中执行昂贵的查找操作。

为特定流选择的传输队列保存在对应于该流的套接字结构中（例如TCP连接）。这个传输队列用于后续发送的流数据包以防止乱序（OOO）数据包。此选择还分摊了在整个流中的所有数据包上调用get_xps_queues()的成本。为了避免乱序数据包，只有在skb->ooo_okay标志为流中的数据包设置时才能更改流的队列。此标志表示流中没有待处理的数据包，因此可以更改传输队列而不会产生乱序数据包的风险。传输层负责适当地设置ooo_okay。例如，TCP在连接的所有数据都被确认后设置该标志。

XPS配置
-----------------

只有当kconfig符号CONFIG_XPS启用时（默认为SMP启用），XPS才可用。如果编译到内核中，XPS是否以及如何在设备初始化时配置取决于驱动程序。使用sysfs可以检查和配置CPU/接收队列到传输队列的映射：

基于CPU映射的选择：

```
/sys/class/net/<dev>/queues/tx-<n>/xps_cpus
```

基于接收队列映射的选择：

```
/sys/class/net/<dev>/queues/tx-<n>/xps_rxqs
```

建议配置
~~~~~~~~~~~~~~~~~~~~~~~

对于只有一个传输队列的网络设备，XPS配置没有任何效果，因为在这种情况下没有选择。在多队列系统中，最好配置XPS，使每个CPU映射到一个队列。如果系统中的队列数与CPU数相等，则每个队列也可以映射到一个CPU，形成专享配对，不会发生争用。如果队列数少于CPU数，则共享给定队列的最佳CPU可能是那些与处理该队列传输完成（传输中断）的CPU共享缓存的CPU。

基于接收队列选择传输队列时，必须显式配置接收队列到传输队列的映射。如果用户配置的接收队列映射不适用，则基于CPU映射选择传输队列。

每个TX队列速率限制
============================

这些是由硬件实现的速率限制机制，目前支持设置Mbps值的最大速率属性：

```
/sys/class/net/<dev>/queues/tx-<n>/tx_maxrate
```

零值表示禁用，这是默认值。
进一步信息
===================
RPS 和 RFS 在内核 2.6.35 中引入。XPS 在 2.6.38 中被纳入。最初的补丁由 Tom Herbert（therbert@google.com）提交。

加速的 RFS 在 2.6.35 中引入。最初的补丁由 Ben Hutchings（bwh@kernel.org）提交。

作者：

- Tom Herbert（therbert@google.com）
- Willem de Bruijn（willemb@google.com）
