### SPDX 许可证标识符: GPL-2.0

=====================================
Linux 网络堆栈中的扩展性
=====================================

#### 引言
==================

本文档描述了一组互补的技术，这些技术在 Linux 网络堆栈中用于增加并行性和提高多处理器系统的性能。以下是描述的主要技术：

- RSS: 接收端扩展（Receive Side Scaling）
- RPS: 接收包导向（Receive Packet Steering）
- RFS: 接收流导向（Receive Flow Steering）
- 加速接收流导向
- XPS: 发送包导向（Transmit Packet Steering）

#### RSS: 接收端扩展
========================

现代网卡支持多个接收和发送描述符队列（多队列）。在接收时，网卡可以将不同的数据包发送到不同的队列以在多个 CPU 之间分发处理任务。网卡通过应用过滤器给每个数据包来分配其到少量逻辑流中的一个，该过滤器决定了数据包的归属。属于每个流的数据包会被导向至独立的接收队列，而这些队列又可以被不同的 CPU 处理。这种机制通常被称为“接收端扩展”（RSS）。RSS 和其他扩展技术的目标是均匀地提高性能。

多队列分布也可以用于流量优先级划分，但这并不是这些技术的重点。
RSS 中使用的过滤器通常是基于网络层和/或传输层头部的哈希函数——例如，基于数据包的 IP 地址和 TCP 端口的四元组哈希。最常用的硬件实现方式使用了一个包含 128 个条目的间接表，其中每个条目存储一个队列编号。对于一个数据包而言，其接收队列由以下步骤确定：计算数据包的哈希值（通常是一个 Toeplitz 哈希），然后取哈希值的低七位作为键值索引间接表，读取对应的值。

一些网卡支持对称 RSS 哈希，即如果交换 IP（源地址、目标地址）和 TCP/UDP（源端口、目标端口）元组，计算出的哈希值保持不变。这对某些监控 TCP/IP 流量的应用（如 IDS、防火墙等）是有益的，因为需要确保双向流量落在同一个接收队列（和 CPU）上。“对称-XOR”是一种 RSS 算法，通过 XOR 操作输入的源和目标字段（IP 和/或第 4 层协议）来实现哈希对称性。然而，这会减少输入熵，并可能被利用。具体来说，算法按如下方式进行 XOR 操作：

```plaintext
# (SRC_IP ^ DST_IP, SRC_IP ^ DST_IP, SRC_PORT ^ DST_PORT, SRC_PORT ^ DST_PORT)
```

然后将结果传递给底层 RSS 算法进行处理。

一些高级网卡允许根据可编程过滤器将数据包导向队列。例如，可以将指向 Web 服务器的 TCP 端口 80 的数据包定向到自己的接收队列。这类“n 元组”过滤器可以通过 `ethtool` (`--config-ntuple`) 进行配置。

##### RSS 配置
-----------------

支持多队列的网卡驱动程序通常提供一个内核模块参数来指定要配置的硬件队列数量。例如，在 `bnx2x` 驱动程序中，这个参数称为 `num_queues`。典型的 RSS 配置是在设备支持足够队列的情况下为每个 CPU 分配一个接收队列，或者至少为每个内存域分配一个队列，其中内存域是指共享特定内存级别的 CPU 集合（L1、L2、NUMA 节点等）。

RSS 设备的间接表，它通过掩码后的哈希值解析队列，通常由驱动程序在初始化时编程。默认映射是将队列均匀分布在表中，但间接表可以通过 `ethtool` 命令（`--show-rxfh-indir` 和 `--set-rxfh-indir`）检索和修改。修改间接表可以为不同的队列分配不同的相对权重。

##### RSS 中断配置
~~~~~~~~~~~~~~~~~~~~~

每个接收队列都有一个与之关联的中断请求（IRQ）。当新数据包到达指定队列时，网卡会触发此中断以通知 CPU。PCIe 设备的信号路径使用消息信号中断（MSI-X），可以将每个中断路由到特定的 CPU。可以通过 `/proc/interrupts` 确定队列与 IRQ 的当前映射关系。默认情况下，一个 IRQ 可能在任何 CPU 上处理。由于数据包处理的一部分发生在接收中断处理中，因此将接收中断分散到多个 CPU 上是有利的。要手动调整每个中断的 IRQ 关联，请参阅 `Documentation/core-api/irq/irq-affinity.rst`。一些系统运行着 `irqbalance` 守护进程，该守护进程动态优化 IRQ 分配，可能会覆盖任何手动设置。

##### 建议配置
~~~~~~~~~~~~~~~~~~~~~

当延迟是一个问题或接收中断处理形成瓶颈时，应启用 RSS。在多个 CPU 之间分散负载可以减少队列长度。对于低延迟网络，最佳设置是为系统中的每个 CPU 分配尽可能多的队列（或者网卡的最大值，如果较低）。最高效的高吞吐量配置可能是队列数量最少且没有接收队列因 CPU 饱和而溢出的情况，因为在默认模式下启用中断合并后，总中断数（以及工作量）会随着队列数量的增加而增加。
每核负载可以通过 `mpstat` 工具进行观察，但请注意，在支持超线程（HT）的处理器上，每个超线程都被视为一个独立的CPU。对于中断处理来说，初步测试表明超线程并未带来任何好处，因此应将队列的数量限制为系统中的物理CPU核心数量。

专用RSS上下文
~~~~~~~~~~~~~~

现代网络接口卡（NIC）支持创建多个共存的RSS配置，这些配置基于明确的匹配规则进行选择。当应用程序希望将接收流量的队列集约束到特定的目标端口或IP地址时，这会非常有用。下面的例子展示了如何将所有TCP端口22的流量导向队列0和1。

要创建一个额外的RSS上下文，请使用如下命令：

```
# ethtool -X eth0 hfunc toeplitz context new
New RSS context is 1
```

内核会报告分配的上下文ID（默认的、始终存在的RSS上下文ID为0）。可以使用与默认上下文相同的API查询和修改新上下文：

```
# ethtool -x eth0 context 1
RX flow hash indirection table for eth0 with 13 RX ring(s):
    0:      0     1     2     3     4     5     6     7
    8:      8     9    10    11    12     0     1     2
[...]
# ethtool -X eth0 equal 2 context 1
# ethtool -x eth0 context 1
RX flow hash indirection table for eth0 with 13 RX ring(s):
    0:      0     1     0     1     0     1     0     1
    8:      0     1     0     1     0     1     0     1
[...]
```

为了利用新的上下文，可以使用n-tuple过滤器将流量定向至该上下文：

```
# ethtool -N eth0 flow-type tcp6 dst-port 22 context 1
Added rule with ID 1023
```

完成之后，删除上下文及其规则：

```
# ethtool -N eth0 delete 1023
# ethtool -X eth0 context 1 delete
```

RPS：接收包转向
==================

接收包转向（RPS）从逻辑上讲是RSS的软件实现。由于它是在软件中实现的，所以它必然在数据路径的较后阶段被调用。RSS通过选择队列以及运行硬件中断处理程序的CPU来工作，而RPS则选择高于中断处理程序之上进行协议处理的CPU。这是通过将数据包放置到所需CPU的待处理队列中并唤醒该CPU来进行处理来实现的。
RPS相比RSS有一些优势：

1. 它可以用于任何网卡
2. 可以轻松地添加软件过滤器以对新协议进行哈希运算
3. 它不会增加硬件设备的中断率（尽管它确实引入了进程间中断（IPIs））

RPS在接收中断处理程序的下半部被调用，当驱动程序通过 `netif_rx()` 或 `netif_receive_skb()` 将数据包发送到网络栈时。这些函数会调用 `get_rps_cpu()` 函数，该函数选择处理数据包的队列。
确定RPS目标CPU的第一步是根据数据包的地址或端口（取决于协议，可以是二元组或四元组哈希）计算流哈希。这作为数据包相关流的一致哈希。哈希值由硬件提供或者在网络栈中计算得出。有能力的硬件可以在数据包的接收描述符中传递哈希值；这通常与RSS使用的哈希相同（例如，计算得到的Toeplitz哈希）。哈希值保存在 `skb->hash` 中，并且可以在栈的其他地方用作数据包流的哈希值。
每个接收硬件队列都关联了一个CPU列表，RPS可将数据包排队到这些CPU上进行处理。对于每个接收到的数据包，根据流哈希模以列表大小计算出索引。索引所指向的CPU是处理数据包的目标，数据包会被排队到该CPU的待处理队列尾部。在下半部例程结束时，会向那些有数据包被排队到其待处理队列上的CPU发送IPI。IPI会唤醒远程CPU上的待处理队列处理过程，然后任何排队的数据包都会被进一步处理并通过网络栈。

RPS配置
--------

RPS需要一个编译时包含CONFIG_RPS kconfig符号的内核（默认情况下对于SMP系统已经启用）。即使编译了，RPS仍然保持禁用状态，直到显式配置。可以使用sysfs文件条目为每个接收队列配置RPS可以转发流量的CPU列表：

```
/sys/class/net/<dev>/queues/rx-<n>/rps_cpus
```

此文件实现了CPU位图。当其为零（默认情况）时RPS被禁用，此时数据包会在中断CPU上被处理。关于如何将CPU分配到位图的说明，请参阅 `Documentation/core-api/irq/irq-affinity.rst`。
建议配置
~~~~~~~~~~~~~~

对于单队列设备，典型的RPS配置是将 `rps_cpus` 设置为与中断CPU处于同一内存域的CPU。如果不存在NUMA局部性问题，则也可以设置为系统中的所有CPU。在高中断率的情况下，明智的做法可能是将中断CPU从映射中排除，因为该CPU已经承担了大量的工作。
### 多队列系统的RSS与RPS

对于多队列系统，如果RSS（接收侧缩放）配置使得每个硬件接收队列都映射到一个CPU上，那么RPS（接收方包处理缩放）可能就是多余的和不必要的。如果硬件队列的数量少于CPU的数量，则在每队列的rps_cpus与该队列中断CPU共享同一内存域的情况下，RPS可能会带来益处。

### RPS流限速

#### 概述

RPS可以在不引入重新排序的情况下跨CPU扩展内核接收处理。将来自同一流的所有数据包发送到同一个CPU的代价是在数据包速率变化时导致CPU负载不平衡。在极端情况下，单一的数据流可能会主导整个流量。特别是在具有大量并发连接的常见服务器工作负载中，这种行为可能表明存在问题，例如配置错误或源地址欺骗拒绝服务攻击。

**流限速**是RPS的一个可选特性，在CPU争用期间通过优先处理小流量来略微提前丢弃大流量的数据包。它仅在RPS或RFS的目的CPU接近饱和时激活。一旦CPU的输入数据包队列长度超过最大队列长度（由sysctl `net.core.netdev_max_backlog`设置）的一半，内核就开始对最近256个数据包进行逐流计数。如果某个流在新数据包到达时超过了设定比例（默认为一半）的数据包数量，则该新数据包将被丢弃。只有当输入数据包队列达到`netdev_max_backlog`时，其他流的数据包才会被丢弃。

当输入数据包队列长度低于阈值时，不会有任何数据包被丢弃，因此流限速并不会直接切断连接：即使对于大流量，也保持了连通性。

#### 接口

流限速默认被编译进内核（CONFIG_NET_FLOW_LIMIT），但没有启用。它是为每个CPU独立实现的（以避免锁和缓存争用），并可以通过设置sysctl `net.core.flow_limit_cpu_bitmap`中的相关位来为每个CPU开启。它暴露了与rps_cpus相同的CPU位图接口（参见上文），当从procfs调用时：

```
/proc/sys/net/core/flow_limit_cpu_bitmap
```

每个数据包的逐流速率是通过将每个数据包哈希到哈希表桶中并递增每个桶的计数器来计算的。哈希函数与RPS中选择CPU所使用的相同，但由于桶的数量可以远大于CPU的数量，流限速能够更精细地识别大流量，并且减少了误报。默认的表有4096个桶。这个值可以通过sysctl修改：

```
net.core.flow_limit_table_len
```

该值仅在分配新表时被参考。修改它不会更新当前活动的表。

### 建议配置

流限速对于具有大量并发连接的系统是有用的，其中单一连接占用50%的CPU可能表示存在问题。

在这样的环境中，应在所有处理网络接收中断的CPU上启用此功能（如在`/proc/irq/N/smp_affinity`中设置的）。

该特性依赖于输入数据包队列长度超过流限速阈值（50%）加上流历史长度（256）。

实验中设置`net.core.netdev_max_backlog`为1000或10000表现良好。
### RFS：接收流导向

虽然RPS仅根据哈希值对数据包进行导向，因此通常能提供良好的负载分布，但它没有考虑到应用程序的局部性。这正是通过接收流导向（RFS）来实现的。RFS的目标是通过将数据包处理导向到运行消费该数据包的应用程序线程所在的CPU上，以提高数据缓存命中率。RFS依赖于相同的RPS机制来将数据包加入另一个CPU的待处理队列，并唤醒那个CPU。

在RFS中，数据包不是直接根据其哈希值转发，而是使用哈希值作为索引进入一个流查找表。此表将流映射到处理这些流的CPU。流的哈希值（参见上面的RPS部分）用于计算这个表中的索引。
表中每个条目记录的是上次处理该流的CPU。如果某个条目不包含有效的CPU，则被映射到该条目的数据包将使用普通的RPS进行导向。多个表条目可能指向同一个CPU。实际上，对于大量流和少量CPU的情况，单个应用程序线程很可能处理具有多种不同流哈希值的流。

`rps_sock_flow_table`是一个全局流表，其中包含了流的*期望*CPU：即当前在用户空间处理该流的CPU。表中的每个值都是一个CPU索引，在调用`recvmsg`和`sendmsg`时（具体来说，是在`inet_recvmsg()`、`inet_sendmsg()`和`tcp_splice_read()`中）进行更新。

当调度器将一个线程移动到新的CPU上，而该线程在旧CPU上有未完成的接收数据包时，可能会出现数据包顺序错误。为了避免这种情况，RFS使用第二个流表来跟踪每个流的未完成数据包：`rps_dev_flow_table`是针对每个设备的每个硬件接收队列的特定表。表中的每个值存储了一个CPU索引和一个计数器。CPU索引表示该流的数据包为了进一步的内核处理而被加入队列的*当前*CPU。理想情况下，内核和用户空间的处理发生在同一个CPU上，因此两个表中的CPU索引相同。如果调度器最近迁移了一个用户空间线程，而内核仍然有数据包在旧CPU上排队等待内核处理，那么这种情况很可能是假的。

`rps_dev_flow_table`中的计数器记录了当前CPU队列长度，当该流中的数据包最后一次被加入队列时。每个队列都有一个头计数器，在出队时递增。尾计数器计算为头计数器加上队列长度。换句话说，`rps_dev_flow[i]`中的计数器记录了流i的最后一个元素，该元素已被加入流i当前指定的CPU进行处理（当然，条目i实际上是通过哈希选择的，多个流可能会哈希到同一个条目i）。

现在介绍避免乱序数据包的技巧：当从`get_rps_cpu()`选择数据包处理的CPU时，会比较`rps_sock_flow`表和数据包接收队列的`rps_dev_flow`表。如果流的期望CPU（在`rps_sock_flow`表中找到）与当前CPU（在`rps_dev_flow`表中找到）匹配，则数据包将被加入到该CPU的待处理队列。如果不匹配，则在以下任一情况发生时更新当前CPU以匹配期望CPU：

- 当前CPU的队列头计数器 >= `rps_dev_flow[i]`中记录的尾计数器值
- 当前CPU未设置（>= `nr_cpu_ids`）
- 当前CPU处于离线状态

经过此检查后，数据包将被发送到（可能已更新的）当前CPU。这些规则旨在确保只有在旧CPU上没有未完成的数据包时，流才会转移到新CPU，因为旧CPU上的未完成数据包可能晚于新CPU上即将处理的数据包到达。

### RFS配置

RFS仅在启用kconfig符号`CONFIG_RPS`时可用（默认情况下对于SMP是开启的）。功能保持禁用状态，直到明确配置。全局流表的条目数量通过以下设置：

```
/proc/sys/net/core/rps_sock_flow_entries
```

每个队列的流表条目数量通过以下设置：

```
/sys/class/net/<dev>/queues/rx-<n>/rps_flow_cnt
```

### 建议配置

这两个值都需要在启动RFS之前为接收队列设置。
两个值均向上取整到最接近的二的幂。建议的流计数取决于在任意给定时间预期活动连接的数量，这可能远小于打开的连接数。我们发现对于中等负载的服务器，将 `rps_sock_flow_entries` 设置为 32768 的值表现得相当不错。
对于单队列设备，单个队列的 `rps_flow_cnt` 值通常配置为与 `rps_sock_flow_entries` 相同的值。
对于多队列设备，每个队列的 `rps_flow_cnt` 可能配置为 `rps_sock_flow_entries` / N，其中 N 是队列的数量。因此，例如，如果 `rps_sock_flow_entries` 设置为 32768 并且有 16 个配置的接收队列，则每个队列的 `rps_flow_cnt` 可能配置为 2048。

加速 RFS
========

加速 RFS 对 RFS 就如同 RSS 对 RPS：它是一种使用软状态来根据消费每个流的数据包的应用程序线程运行位置引导流的硬件加速负载平衡机制。
加速 RFS 应该比 RFS 表现更好，因为数据包直接发送到消费数据的应用程序线程所在的本地 CPU。目标 CPU 要么是应用程序运行的相同 CPU，要么至少是在缓存层次结构中与应用程序线程的 CPU 本地的 CPU。
为了启用加速 RFS，网络堆栈调用 `ndo_rx_flow_steer` 驱动程序函数来通信匹配特定流的数据包所需的硬件队列。每当 `rps_dev_flow_table` 中的流条目更新时，网络堆栈会自动调用此函数。驱动程序接着使用设备特定的方法来编程 NIC 以引导数据包。
流的硬件队列是从记录在 `rps_dev_flow_table` 中的 CPU 派生出来的。堆栈参考由 NIC 驱动程序维护的 CPU 到硬件队列映射。这是从 `/proc/interrupts` 显示的 IRQ 亲和性表自动生成的逆向映射。驱动程序可以使用内核库 `cpu_rmap`（“CPU 亲和性反向映射”）中的函数填充映射。对于每个 CPU，映射中对应的队列设置为处理 CPU 在缓存局部性上最接近的一个。

加速 RFS 配置
--------------

加速 RFS 仅在内核编译了 `CONFIG_RFS_ACCEL` 且 NIC 设备和驱动程序提供了支持时可用。
它还需要通过 ethtool 启用 ntuple 过滤。CPU 到队列的映射会根据驱动程序为每个接收队列配置的 IRQ 亲和性自动推断，因此不需要额外的配置。

建议配置
~~~~~~~~~

当希望使用 RFS 并且 NIC 支持硬件加速时，应启用此技术。
XPS：传输包导向
=============================

传输包导向（XPS）是一种智能选择在多队列设备上发送数据包时使用的传输队列的机制。这可以通过记录两种映射来实现，一种是CPU到硬件队列的映射，另一种是接收队列到硬件传输队列的映射。

1. 使用CPU映射的XPS

该映射的目标通常是为子集中的CPU分配队列，这些队列的传输完成处理将在该集合内的CPU上进行。这种选择提供了两个好处。首先，由于较少的CPU争夺相同的队列，因此设备队列锁的竞争显著减少（如果每个CPU都有自己的传输队列，则可以完全消除竞争）。其次，传输完成时的缓存未命中率降低，特别是对于保存`sk_buff`结构的数据缓存行。
2. 使用接收队列映射的XPS

此映射用于根据管理员设置的接收队列映射配置来选择传输队列。一组接收队列可以映射到一组传输队列（多对多），尽管常见的使用情况是一对一的映射。这样可以在传输和接收时使用相同的队列关联。这对于繁忙轮询的多线程工作负载非常有用，在这些工作负载中，很难将给定的CPU与特定的应用程序线程相关联。应用程序线程不固定到CPU，并且每个线程处理从单个队列收到的数据包。连接的套接字中会缓存接收队列号。在这种模型中，通过与相关联的接收队列相同的传输队列发送数据包有助于保持CPU开销较低。传输完成的工作被锁定在应用程序正在轮询的相同队列关联上。这样可以避免在另一个CPU上触发中断的开销。当应用程序在繁忙轮询期间清理数据包时，传输完成可能会在同一线程上下文中处理，从而减少延迟。

XPS是通过设置可用于该队列传输的CPU/接收队列的位图来为每个传输队列配置的。相反的映射，即从CPU到传输队列或从接收队列到传输队列的映射，对于每个网络设备都会计算并维护。在传输流的第一个数据包时，会调用`get_xps_queue()`函数来选择一个队列。该函数使用套接字连接的接收队列ID作为查找表中接收队列到传输队列映射的匹配项。或者，该函数也可以使用运行CPU的ID作为CPU到队列查找表的键。如果ID匹配单个队列，则使用该队列进行传输。如果多个队列匹配，则使用流哈希计算索引来选择其中一个队列。当基于接收队列映射选择传输队列时，不会验证传输设备与接收设备是否一致，因为这需要在数据路径中执行昂贵的查找操作。

为特定流选择的传输队列会被保存在对应于该流的套接字结构中（例如TCP连接）。此传输队列用于后续在该流上发送的数据包以防止乱序数据包。这个选择也摊销了调用`get_xps_queues()`函数在整个流中所有数据包上的成本。为了避免乱序数据包，只有当流中的数据包设置了`skb->ooo_okay`标志时，才能更改流的队列。该标志表示流中没有未决数据包，因此可以更改传输队列而不用担心产生乱序数据包的风险。传输层负责正确设置`ooo_okay`标志。例如，TCP会在连接的所有数据都被确认后设置该标志。

XPS配置
-----------------

只有当kconfig符号`CONFIG_XPS`被启用时（默认情况下对于SMP系统是启用的），XPS才可用。如果编译进内核，XPS是否以及如何在设备初始化时配置取决于驱动程序。CPU/接收队列到传输队列的映射可以通过sysfs进行检查和配置：

对于基于CPU映射的选择：
```
/sys/class/net/<dev>/queues/tx-<n>/xps_cpus
```

对于基于接收队列映射的选择：
```
/sys/class/net/<dev>/queues/tx-<n>/xps_rxqs
```

建议配置
~~~~~~~~~~~~~~~~~~~~~~~

对于只有一个传输队列的网络设备，XPS配置没有任何影响，因为在这种情况下没有选择余地。在多队列系统中，最好配置XPS，使得每个CPU映射到一个队列。
如果系统的队列数与CPU数相同，则每个队列也可以映射到一个CPU，形成独占配对，不会出现竞争。如果队列数量少于CPU数量，则最合适的共享同一队列的CPU可能是那些与处理该队列传输完成（传输中断）的CPU共享缓存的CPU。
对于基于接收队列的传输队列选择，必须明确配置接收队列到传输队列的映射。如果用户配置的接收队列映射不适用，则根据CPU映射选择传输队列。

每个TX队列的速率限制
============================

这些是由硬件实现的速率限制机制，目前支持的最大速率属性是通过设置Mbps值来实现的：
```
/sys/class/net/<dev>/queues/tx-<n>/tx_maxrate
```

0值意味着禁用，这是默认设置。
进一步信息
===================
RPS 和 RFS 在内核 2.6.35 中被引入。XPS 被合并到了 2.6.38 版本中。最初的补丁由 Tom Herbert（therbert@google.com）提交。

加速的 RFS 在 2.6.35 中被引入。最初的补丁由 Ben Hutchings（bwh@kernel.org）提交。

作者：

- Tom Herbert（therbert@google.com）
- Willem de Bruijn（willemb@google.com）
